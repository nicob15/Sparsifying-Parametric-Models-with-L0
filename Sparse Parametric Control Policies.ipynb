{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sparse Parametric Control Policies\n",
    "\n",
    "In this tutorial, we utilize twin-delayed deep deterministic policy gradient (TD3) for learning a control policy for the pendulum. We compare the standard TD3 algorithm, utlizing neural networks for approximating value function and policy, with the sparse polynomial TD3 we developed. The sparse polynomial TD3 relies on dictionary learning and L$_0$ regularization to learn a sparse and interpreable polynomial policy. "
   ],
   "id": "1cdc098481f6a247"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8a64b677c6b11aca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T13:23:32.988758Z",
     "start_time": "2024-06-19T13:23:32.984542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from utils.replay_buffer import ExperienceReplayBuffer\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.agents.td3 import TD3\n",
    "from utils.agents.polyL0_td3 import TD3 as polyTD3\n",
    "\n",
    "# Set seeds\n",
    "seed = 23524\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ],
   "id": "d7c19066c77075c8",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "render = False\n",
    "\n",
    "if render:\n",
    "    env = gym.make('Pendulum-v1', g=9.81, render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make('Pendulum-v1', g=9.81)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set hyperparameters of the experiments\n",
    "max_episodes = 250\n",
    "max_steps = 200\n",
    "warmup = 25\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "max_act = 2\n",
    "buf_dim = int(max_episodes*max_steps)\n",
    "\n",
    "training_buffer = ExperienceReplayBuffer(state_dim=obs_dim, action_dim=act_dim, max_size=int(1e6))\n",
    "\n",
    "h_dim = 256\n",
    "tau = 0.005\n",
    "eval_interval = 20\n",
    "batch_size = 256\n",
    "training_start = 50\n",
    "training_iteration = 100\n",
    "\n",
    "# TD3 agent\n",
    "agent = TD3(state_dim=obs_dim, action_dim=act_dim, max_action=max_act, h_dim=h_dim, tau=tau, device=device)\n",
    "\n",
    "degree = 3\n",
    "scale = 1.0\n",
    "reg_coeff = 0.01\n",
    "droprate = 0.2\n",
    "# polynomial TD3 agent\n",
    "agent_poly = polyTD3(state_dim=obs_dim, action_dim=act_dim, max_action=max_act, degree_pi=degree, feature_scale=scale, reg_coeff=reg_coeff, droprate=droprate, tau=tau,device=device)"
   ],
   "id": "5913913f8986bee0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define train and evaluation functions.",
   "id": "b3627c11ab666d27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T13:23:33.037707Z",
     "start_time": "2024-06-19T13:23:33.031470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_agent(agent, env, agent_type='td3'):\n",
    "    training_rewards = []\n",
    "    eval_rewards = []\n",
    "    ep_reward = 0\n",
    "    for episode in range(max_episodes):\n",
    "        observation, info = env.reset()\n",
    "        for steps in range(max_steps+1):\n",
    "            if episode < warmup:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = (agent.select_action(observation) + np.random.normal(0, max_act * 0.1, size=act_dim)).clip(-max_act, max_act)\n",
    "\n",
    "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            training_buffer.add(observation, action, next_observation, reward, done)\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(\"Training episode {} is done with reward {}\".format(episode, ep_reward))\n",
    "                training_rewards.append(ep_reward)\n",
    "                ep_reward = 0.\n",
    "                done = False\n",
    "                if episode > training_start:\n",
    "                    agent.train(training_buffer, iterations=training_iteration, batch_size=batch_size)\n",
    "\n",
    "                if episode % eval_interval == 0 and episode > 0:\n",
    "                    eval_rew = eval_agent(agent, agent_type=agent_type, ep=episode)\n",
    "                    eval_rewards.append(eval_rew)\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    eval_rew = eval_agent(agent, agent_type=agent_type, ep=max_episodes+1)\n",
    "    eval_rewards.append(eval_rew)\n",
    "\n",
    "    return training_rewards, eval_rewards\n",
    "\n",
    "def eval_agent(agent, eval_episodes=10,agent_type='td3', ep=1):\n",
    "\n",
    "    eval_env = gym.make('Pendulum-v1', g=9.81)\n",
    "    avg_reward = 0.\n",
    "    ep_reward = 0.\n",
    "    for episode in range(eval_episodes):\n",
    "\n",
    "        observation, info = eval_env.reset()\n",
    "\n",
    "        for steps in range(max_steps + 1):\n",
    "\n",
    "            action = agent.select_action(observation)\n",
    "\n",
    "            next_observation, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(\"Evaluation episode {} is done with reward {}\".format(episode, ep_reward))\n",
    "                avg_reward += ep_reward\n",
    "                ep_reward = 0\n",
    "                done = False\n",
    "                break\n",
    "\n",
    "    eval_env.close()\n",
    "\n",
    "    avg_reward = avg_reward / eval_episodes\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    if agent_type == 'polytd3':\n",
    "        agent.print_equations(ep=ep)\n",
    "\n",
    "    return avg_reward"
   ],
   "id": "3b17c71ade31cf79",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train the two agents.",
   "id": "ccfa49af6aa95397"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_rewards_td3, eval_rewards_td3 = train_agent(agent, env, agent_type='td3')\n",
    "\n",
    "training_rewards_polytd3, eval_rewards_polytd3 = train_agent(agent_poly, env, agent_type='polytd3')"
   ],
   "id": "ced605ccf33358eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plot the rewards over training.",
   "id": "f5c481000c95957b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.suptitle('Training and Evaluation Rewards')\n",
    "axs[0].plot(training_rewards_td3, label='td3')\n",
    "axs[1].plot(eval_rewards_td3, label='td3')\n",
    "axs[0].plot(training_rewards_polytd3, label='polytd3')\n",
    "axs[1].plot(eval_rewards_polytd3, label='polytd3')\n",
    "axs[0].legend(loc=\"lower right\")\n",
    "axs[1].legend(loc=\"lower right\")\n",
    "\n",
    "plt.savefig('figures/rewards.svg')"
   ],
   "id": "87270ce4e1eecbce",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
