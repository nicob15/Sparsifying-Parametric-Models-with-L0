{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1576e777c47532ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Learning Sparse Reward Models\n",
    "\n",
    "We consider the case of learning the reward function of the pendulum. In particular, we aim to learn the reward function $R:\\mathcal{S}\\times \\mathcal{A}\\rightarrow \\mathbb{R}$, predicting the reward $\\hat{r}_t$ from the current state $\\mathbf{s}_t$ and action $\\mathbf{a}_t$: $$\\hat{r}_t=R(\\mathbf{s}_t, \\mathbf{a}_t).$$ We will approximate $R(\\mathbf{s}_t, \\mathbf{a}_t)$ using:\n",
    "* a fully-connected neural network (fcnn_model),\n",
    "* a fully-connected neural network sparsified by the L$_0$ regularization (sparsefcnn_model), and\n",
    "* a SINDy-like architecture again sparsified by the L$_0$ regularization (l0sindy_model)."
   ],
   "id": "203b0ba5353dd2c7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-19T12:23:33.924092Z",
     "start_time": "2024-06-19T12:23:33.921203Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds\n",
    "seed = 23524\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We first create a training set composed of 1000 episodes of 200 steps each. The actions are sampled from a random policy.",
   "id": "285aa8507a1b279d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T12:23:44.368221Z",
     "start_time": "2024-06-19T12:23:33.925182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "render = False\n",
    "if render:\n",
    "    env = gym.make('Pendulum-v1', g=9.81, render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make('Pendulum-v1', g=9.81)\n",
    "max_episodes = 1000\n",
    "max_steps = 200\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "buf_dim = int(max_episodes*max_steps)\n",
    "\n",
    "# create training set\n",
    "training_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=buf_dim)\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    for steps in range(max_steps+1):\n",
    "        action = env.action_space.sample() \n",
    "        next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        training_buffer.store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "print(\"Finished creating the training set\")"
   ],
   "id": "f30d49f053f6de6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/Documents/Sparsifying-Parametric-Models-with-L0/venv/lib/python3.10/site-packages/gymnasium/envs/classic_control/pendulum.py:173: UserWarning: \u001B[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\u001B[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating the training set\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Secondly, we create the test set that we will use to evaluate the models.",
   "id": "2e77e79ba07b6ed3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T12:23:45.394304Z",
     "start_time": "2024-06-19T12:23:44.369057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create test set\n",
    "max_episodes_test = 100\n",
    "buf_dim = int(max_episodes*max_steps)\n",
    "\n",
    "testing_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=buf_dim)\n",
    "\n",
    "for episode in range(max_episodes_test):\n",
    "    observation, _ = env.reset()\n",
    "    for steps in range(max_steps + 1):\n",
    "        action = env.action_space.sample()  \n",
    "        next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        testing_buffer.store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "print(\"Finished creating the test set\")"
   ],
   "id": "5c45b1ee713bfd84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating the test set\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After creating train and test set, we are now ready to train the three models. We utilize the same learning rate, batch size, and number of traning epochs.\n",
   "id": "88bc1c0d153e505e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T12:23:45.623667Z",
     "start_time": "2024-06-19T12:23:45.395490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# learning the reward function of the pendulum\n",
    "from utils.models import FCNN, SparseFCNN, L0SINDy_reward\n",
    "from utils.trainer import train_eval_reward_model\n",
    "import torch\n",
    "\n",
    "# number of hidden units used by the fcnn_model and the sparsefcnn_model\n",
    "h_dim = 128\n",
    "\n",
    "# shared hyperparameter of the experiment\n",
    "lr = 3e-4\n",
    "batch_size = 256\n",
    "num_epochs = 500"
   ],
   "id": "e6093ecf746ff4b5",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train and evaluate the fcnn_model.",
   "id": "5d92a884252b8594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T12:26:45.327054Z",
     "start_time": "2024-06-19T12:23:45.624508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fcnn_model = FCNN(input_dim=obs_dim+act_dim, output_dim=1, h_dim=h_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    fcnn_model = fcnn_model.cuda()\n",
    "\n",
    "optimizer_fcnn = torch.optim.Adam([\n",
    "    {'params': fcnn_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_fcnn = train_eval_reward_model(fcnn_model, optimizer_fcnn, training_buffer, testing_buffer, batch_size, num_epochs)\n",
    "print(\"Best testing error FCNN is {} and it was found at epoch {}\".format(metrics_fcnn[2], metrics_fcnn[3]))"
   ],
   "id": "f406d37cdd49c6ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average eval loss: 0.2242373675\n",
      "====> Epoch: 1 Average eval loss: 0.1555906683\n",
      "====> Epoch: 2 Average eval loss: 0.0955389962\n",
      "====> Epoch: 3 Average eval loss: 0.0506766774\n",
      "====> Epoch: 4 Average eval loss: 0.0323671512\n",
      "====> Epoch: 5 Average eval loss: 0.0183821898\n",
      "====> Epoch: 6 Average eval loss: 0.0121691236\n",
      "====> Epoch: 7 Average eval loss: 0.0085523184\n",
      "====> Epoch: 8 Average eval loss: 0.0061701760\n",
      "====> Epoch: 9 Average eval loss: 0.0050227093\n",
      "====> Epoch: 10 Average eval loss: 0.0075902031\n",
      "====> Epoch: 11 Average eval loss: 0.0044573457\n",
      "====> Epoch: 12 Average eval loss: 0.0041646799\n",
      "====> Epoch: 13 Average eval loss: 0.0036856132\n",
      "====> Epoch: 14 Average eval loss: 0.0027860457\n",
      "====> Epoch: 15 Average eval loss: 0.0037177089\n",
      "====> Epoch: 16 Average eval loss: 0.0020456859\n",
      "====> Epoch: 17 Average eval loss: 0.0060423431\n",
      "====> Epoch: 18 Average eval loss: 0.0029456445\n",
      "====> Epoch: 19 Average eval loss: 0.0060066800\n",
      "====> Epoch: 20 Average eval loss: 0.0038782619\n",
      "====> Epoch: 21 Average eval loss: 0.0077592246\n",
      "====> Epoch: 22 Average eval loss: 0.0038698779\n",
      "====> Epoch: 23 Average eval loss: 0.0018816909\n",
      "====> Epoch: 24 Average eval loss: 0.0010517989\n",
      "====> Epoch: 25 Average eval loss: 0.0015164994\n",
      "====> Epoch: 26 Average eval loss: 0.0011312158\n",
      "====> Epoch: 27 Average eval loss: 0.0015563174\n",
      "====> Epoch: 28 Average eval loss: 0.0010900984\n",
      "====> Epoch: 29 Average eval loss: 0.0025561855\n",
      "====> Epoch: 30 Average eval loss: 0.0015562609\n",
      "====> Epoch: 31 Average eval loss: 0.0014462984\n",
      "====> Epoch: 32 Average eval loss: 0.0018484611\n",
      "====> Epoch: 33 Average eval loss: 0.0008878423\n",
      "====> Epoch: 34 Average eval loss: 0.0009980825\n",
      "====> Epoch: 35 Average eval loss: 0.0009766265\n",
      "====> Epoch: 36 Average eval loss: 0.0010769935\n",
      "====> Epoch: 37 Average eval loss: 0.0022257559\n",
      "====> Epoch: 38 Average eval loss: 0.0007334737\n",
      "====> Epoch: 39 Average eval loss: 0.0008077199\n",
      "====> Epoch: 40 Average eval loss: 0.0012356424\n",
      "====> Epoch: 41 Average eval loss: 0.0012293976\n",
      "====> Epoch: 42 Average eval loss: 0.0006219384\n",
      "====> Epoch: 43 Average eval loss: 0.0052751885\n",
      "====> Epoch: 44 Average eval loss: 0.0008724786\n",
      "====> Epoch: 45 Average eval loss: 0.0013687357\n",
      "====> Epoch: 46 Average eval loss: 0.0005849187\n",
      "====> Epoch: 47 Average eval loss: 0.0019584736\n",
      "====> Epoch: 48 Average eval loss: 0.0011157984\n",
      "====> Epoch: 49 Average eval loss: 0.0005424026\n",
      "====> Epoch: 50 Average eval loss: 0.0004204264\n",
      "====> Epoch: 51 Average eval loss: 0.0010506629\n",
      "====> Epoch: 52 Average eval loss: 0.0011047221\n",
      "====> Epoch: 53 Average eval loss: 0.0006402967\n",
      "====> Epoch: 54 Average eval loss: 0.0018134960\n",
      "====> Epoch: 55 Average eval loss: 0.0008740926\n",
      "====> Epoch: 56 Average eval loss: 0.0053077042\n",
      "====> Epoch: 57 Average eval loss: 0.0006661847\n",
      "====> Epoch: 58 Average eval loss: 0.0004503898\n",
      "====> Epoch: 59 Average eval loss: 0.0006878892\n",
      "====> Epoch: 60 Average eval loss: 0.0016629871\n",
      "====> Epoch: 61 Average eval loss: 0.0033041625\n",
      "====> Epoch: 62 Average eval loss: 0.0003674482\n",
      "====> Epoch: 63 Average eval loss: 0.0009703682\n",
      "====> Epoch: 64 Average eval loss: 0.0008332829\n",
      "====> Epoch: 65 Average eval loss: 0.0022916121\n",
      "====> Epoch: 66 Average eval loss: 0.0010607751\n",
      "====> Epoch: 67 Average eval loss: 0.0038160244\n",
      "====> Epoch: 68 Average eval loss: 0.0003355416\n",
      "====> Epoch: 69 Average eval loss: 0.0003313651\n",
      "====> Epoch: 70 Average eval loss: 0.0011002880\n",
      "====> Epoch: 71 Average eval loss: 0.0003654507\n",
      "====> Epoch: 72 Average eval loss: 0.0003719268\n",
      "====> Epoch: 73 Average eval loss: 0.0009170754\n",
      "====> Epoch: 74 Average eval loss: 0.0009341421\n",
      "====> Epoch: 75 Average eval loss: 0.0004927107\n",
      "====> Epoch: 76 Average eval loss: 0.0005228889\n",
      "====> Epoch: 77 Average eval loss: 0.0005994986\n",
      "====> Epoch: 78 Average eval loss: 0.0009375832\n",
      "====> Epoch: 79 Average eval loss: 0.0008588551\n",
      "====> Epoch: 80 Average eval loss: 0.0008985571\n",
      "====> Epoch: 81 Average eval loss: 0.0014447229\n",
      "====> Epoch: 82 Average eval loss: 0.0040805596\n",
      "====> Epoch: 83 Average eval loss: 0.0005741993\n",
      "====> Epoch: 84 Average eval loss: 0.0002348131\n",
      "====> Epoch: 85 Average eval loss: 0.0003006717\n",
      "====> Epoch: 86 Average eval loss: 0.0010148506\n",
      "====> Epoch: 87 Average eval loss: 0.0004723014\n",
      "====> Epoch: 88 Average eval loss: 0.0013508892\n",
      "====> Epoch: 89 Average eval loss: 0.0020795525\n",
      "====> Epoch: 90 Average eval loss: 0.0002143924\n",
      "====> Epoch: 91 Average eval loss: 0.0007330317\n",
      "====> Epoch: 92 Average eval loss: 0.0023402267\n",
      "====> Epoch: 93 Average eval loss: 0.0043054763\n",
      "====> Epoch: 94 Average eval loss: 0.0046801087\n",
      "====> Epoch: 95 Average eval loss: 0.0002505548\n",
      "====> Epoch: 96 Average eval loss: 0.0008663564\n",
      "====> Epoch: 97 Average eval loss: 0.0004009344\n",
      "====> Epoch: 98 Average eval loss: 0.0004398813\n",
      "====> Epoch: 99 Average eval loss: 0.0008701414\n",
      "====> Epoch: 100 Average eval loss: 0.0002099978\n",
      "====> Epoch: 101 Average eval loss: 0.0007591236\n",
      "====> Epoch: 102 Average eval loss: 0.0013942929\n",
      "====> Epoch: 103 Average eval loss: 0.0007404242\n",
      "====> Epoch: 104 Average eval loss: 0.0012211420\n",
      "====> Epoch: 105 Average eval loss: 0.0003686727\n",
      "====> Epoch: 106 Average eval loss: 0.0012301193\n",
      "====> Epoch: 107 Average eval loss: 0.0004314101\n",
      "====> Epoch: 108 Average eval loss: 0.0002119315\n",
      "====> Epoch: 109 Average eval loss: 0.0025133989\n",
      "====> Epoch: 110 Average eval loss: 0.0003844307\n",
      "====> Epoch: 111 Average eval loss: 0.0024373371\n",
      "====> Epoch: 112 Average eval loss: 0.0004436269\n",
      "====> Epoch: 113 Average eval loss: 0.0002511244\n",
      "====> Epoch: 114 Average eval loss: 0.0001940821\n",
      "====> Epoch: 115 Average eval loss: 0.0003147845\n",
      "====> Epoch: 116 Average eval loss: 0.0010926944\n",
      "====> Epoch: 117 Average eval loss: 0.0010285683\n",
      "====> Epoch: 118 Average eval loss: 0.0005073358\n",
      "====> Epoch: 119 Average eval loss: 0.0008703247\n",
      "====> Epoch: 120 Average eval loss: 0.0034113452\n",
      "====> Epoch: 121 Average eval loss: 0.0014511285\n",
      "====> Epoch: 122 Average eval loss: 0.0009883005\n",
      "====> Epoch: 123 Average eval loss: 0.0004002354\n",
      "====> Epoch: 124 Average eval loss: 0.0006695068\n",
      "====> Epoch: 125 Average eval loss: 0.0010385693\n",
      "====> Epoch: 126 Average eval loss: 0.0007423690\n",
      "====> Epoch: 127 Average eval loss: 0.0027755050\n",
      "====> Epoch: 128 Average eval loss: 0.0006773376\n",
      "====> Epoch: 129 Average eval loss: 0.0009391009\n",
      "====> Epoch: 130 Average eval loss: 0.0007268983\n",
      "====> Epoch: 131 Average eval loss: 0.0005311671\n",
      "====> Epoch: 132 Average eval loss: 0.0001629661\n",
      "====> Epoch: 133 Average eval loss: 0.0009171772\n",
      "====> Epoch: 134 Average eval loss: 0.0010089068\n",
      "====> Epoch: 135 Average eval loss: 0.0005758714\n",
      "====> Epoch: 136 Average eval loss: 0.0006531456\n",
      "====> Epoch: 137 Average eval loss: 0.0013880994\n",
      "====> Epoch: 138 Average eval loss: 0.0002648494\n",
      "====> Epoch: 139 Average eval loss: 0.0003567188\n",
      "====> Epoch: 140 Average eval loss: 0.0001455184\n",
      "====> Epoch: 141 Average eval loss: 0.0006146752\n",
      "====> Epoch: 142 Average eval loss: 0.0002782228\n",
      "====> Epoch: 143 Average eval loss: 0.0012563522\n",
      "====> Epoch: 144 Average eval loss: 0.0008067335\n",
      "====> Epoch: 145 Average eval loss: 0.0010270304\n",
      "====> Epoch: 146 Average eval loss: 0.0004070852\n",
      "====> Epoch: 147 Average eval loss: 0.0006629860\n",
      "====> Epoch: 148 Average eval loss: 0.0005652960\n",
      "====> Epoch: 149 Average eval loss: 0.0009561767\n",
      "====> Epoch: 150 Average eval loss: 0.0001581994\n",
      "====> Epoch: 151 Average eval loss: 0.0005489123\n",
      "====> Epoch: 152 Average eval loss: 0.0004263773\n",
      "====> Epoch: 153 Average eval loss: 0.0005870536\n",
      "====> Epoch: 154 Average eval loss: 0.0009335558\n",
      "====> Epoch: 155 Average eval loss: 0.0002154834\n",
      "====> Epoch: 156 Average eval loss: 0.0007143115\n",
      "====> Epoch: 157 Average eval loss: 0.0061221020\n",
      "====> Epoch: 158 Average eval loss: 0.0007137314\n",
      "====> Epoch: 159 Average eval loss: 0.0011285231\n",
      "====> Epoch: 160 Average eval loss: 0.0105093224\n",
      "====> Epoch: 161 Average eval loss: 0.0015836512\n",
      "====> Epoch: 162 Average eval loss: 0.0009983993\n",
      "====> Epoch: 163 Average eval loss: 0.0004464286\n",
      "====> Epoch: 164 Average eval loss: 0.0002277490\n",
      "====> Epoch: 165 Average eval loss: 0.0009741165\n",
      "====> Epoch: 166 Average eval loss: 0.0020416237\n",
      "====> Epoch: 167 Average eval loss: 0.0009589193\n",
      "====> Epoch: 168 Average eval loss: 0.0003586138\n",
      "====> Epoch: 169 Average eval loss: 0.0001626415\n",
      "====> Epoch: 170 Average eval loss: 0.0020352858\n",
      "====> Epoch: 171 Average eval loss: 0.0008354029\n",
      "====> Epoch: 172 Average eval loss: 0.0003997184\n",
      "====> Epoch: 173 Average eval loss: 0.0002790460\n",
      "====> Epoch: 174 Average eval loss: 0.0002280253\n",
      "====> Epoch: 175 Average eval loss: 0.0009475332\n",
      "====> Epoch: 176 Average eval loss: 0.0008674478\n",
      "====> Epoch: 177 Average eval loss: 0.0002332746\n",
      "====> Epoch: 178 Average eval loss: 0.0005319811\n",
      "====> Epoch: 179 Average eval loss: 0.0002318702\n",
      "====> Epoch: 180 Average eval loss: 0.0029430061\n",
      "====> Epoch: 181 Average eval loss: 0.0249738656\n",
      "====> Epoch: 182 Average eval loss: 0.0002203417\n",
      "====> Epoch: 183 Average eval loss: 0.0022971511\n",
      "====> Epoch: 184 Average eval loss: 0.0001289975\n",
      "====> Epoch: 185 Average eval loss: 0.0007476881\n",
      "====> Epoch: 186 Average eval loss: 0.0006922589\n",
      "====> Epoch: 187 Average eval loss: 0.0002500338\n",
      "====> Epoch: 188 Average eval loss: 0.0018648570\n",
      "====> Epoch: 189 Average eval loss: 0.0001370039\n",
      "====> Epoch: 190 Average eval loss: 0.0004635976\n",
      "====> Epoch: 191 Average eval loss: 0.0012014513\n",
      "====> Epoch: 192 Average eval loss: 0.0001054416\n",
      "====> Epoch: 193 Average eval loss: 0.0015839782\n",
      "====> Epoch: 194 Average eval loss: 0.0013833257\n",
      "====> Epoch: 195 Average eval loss: 0.0031362819\n",
      "====> Epoch: 196 Average eval loss: 0.0032132918\n",
      "====> Epoch: 197 Average eval loss: 0.0005226487\n",
      "====> Epoch: 198 Average eval loss: 0.0008479675\n",
      "====> Epoch: 199 Average eval loss: 0.0002194013\n",
      "====> Epoch: 200 Average eval loss: 0.0001882622\n",
      "====> Epoch: 201 Average eval loss: 0.0001719032\n",
      "====> Epoch: 202 Average eval loss: 0.0004489246\n",
      "====> Epoch: 203 Average eval loss: 0.0009155822\n",
      "====> Epoch: 204 Average eval loss: 0.0007685106\n",
      "====> Epoch: 205 Average eval loss: 0.0002597473\n",
      "====> Epoch: 206 Average eval loss: 0.0002597788\n",
      "====> Epoch: 207 Average eval loss: 0.0007830930\n",
      "====> Epoch: 208 Average eval loss: 0.0002006643\n",
      "====> Epoch: 209 Average eval loss: 0.0105745904\n",
      "====> Epoch: 210 Average eval loss: 0.0004009337\n",
      "====> Epoch: 211 Average eval loss: 0.0011950699\n",
      "====> Epoch: 212 Average eval loss: 0.0015795680\n",
      "====> Epoch: 213 Average eval loss: 0.0005085912\n",
      "====> Epoch: 214 Average eval loss: 0.0001483614\n",
      "====> Epoch: 215 Average eval loss: 0.0003239686\n",
      "====> Epoch: 216 Average eval loss: 0.0009256046\n",
      "====> Epoch: 217 Average eval loss: 0.0002107736\n",
      "====> Epoch: 218 Average eval loss: 0.0002028452\n",
      "====> Epoch: 219 Average eval loss: 0.0001485964\n",
      "====> Epoch: 220 Average eval loss: 0.0007001262\n",
      "====> Epoch: 221 Average eval loss: 0.0000995725\n",
      "====> Epoch: 222 Average eval loss: 0.0002364188\n",
      "====> Epoch: 223 Average eval loss: 0.0006899892\n",
      "====> Epoch: 224 Average eval loss: 0.0009510918\n",
      "====> Epoch: 225 Average eval loss: 0.0001227851\n",
      "====> Epoch: 226 Average eval loss: 0.0004293905\n",
      "====> Epoch: 227 Average eval loss: 0.0004525112\n",
      "====> Epoch: 228 Average eval loss: 0.0003422046\n",
      "====> Epoch: 229 Average eval loss: 0.0001464653\n",
      "====> Epoch: 230 Average eval loss: 0.0001779723\n",
      "====> Epoch: 231 Average eval loss: 0.0004511588\n",
      "====> Epoch: 232 Average eval loss: 0.0003983834\n",
      "====> Epoch: 233 Average eval loss: 0.0001249626\n",
      "====> Epoch: 234 Average eval loss: 0.0001980279\n",
      "====> Epoch: 235 Average eval loss: 0.0001635196\n",
      "====> Epoch: 236 Average eval loss: 0.0020037456\n",
      "====> Epoch: 237 Average eval loss: 0.0034183592\n",
      "====> Epoch: 238 Average eval loss: 0.0009762951\n",
      "====> Epoch: 239 Average eval loss: 0.0001347754\n",
      "====> Epoch: 240 Average eval loss: 0.0065044835\n",
      "====> Epoch: 241 Average eval loss: 0.0000846708\n",
      "====> Epoch: 242 Average eval loss: 0.0001335744\n",
      "====> Epoch: 243 Average eval loss: 0.0003059178\n",
      "====> Epoch: 244 Average eval loss: 0.0001623014\n",
      "====> Epoch: 245 Average eval loss: 0.0001265606\n",
      "====> Epoch: 246 Average eval loss: 0.0003960673\n",
      "====> Epoch: 247 Average eval loss: 0.0002995353\n",
      "====> Epoch: 248 Average eval loss: 0.0016462104\n",
      "====> Epoch: 249 Average eval loss: 0.0001944413\n",
      "Best testing error FCNN is 8.46708207973279e-05 and it was found at epoch 241\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train and evaluate the sparsefcnn_model.",
   "id": "3fb8680d19f9c182"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T12:32:53.533967Z",
     "start_time": "2024-06-19T12:26:45.327933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reg_coefficient = 0.0005\n",
    "sparsefcnn_model = SparseFCNN(input_dim=obs_dim+act_dim, output_dim=1, h_dim=h_dim, lambda_coeff=reg_coefficient)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    sparsefcnn_model = sparsefcnn_model.cuda()\n",
    "\n",
    "optimizer_sparsefcnn = torch.optim.Adam([\n",
    "    {'params': sparsefcnn_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_sparsefcnn = train_eval_reward_model(sparsefcnn_model, optimizer_sparsefcnn, training_buffer, testing_buffer,\n",
    "                                               batch_size, num_epochs, l0=True)\n",
    "print(\"Best testing error sparse FCNN is {} and it was found at epoch {}\".format(metrics_sparsefcnn[2], metrics_sparsefcnn[3]))\n"
   ],
   "id": "90c9ca5b819b0c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0Dense(4 -> 128, droprate_init=0.5, lamba=0.0005, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False)\n",
      "L0Dense(128 -> 128, droprate_init=0.5, lamba=0.0005, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False)\n",
      "L0Dense(128 -> 1, droprate_init=0.5, lamba=0.0005, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/Documents/Sparsifying-Parametric-Models-with-L0/utils/l0_layer.py:51: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.weights, mode='fan_out')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average eval loss: 2.6028380394\n",
      "====> Epoch: 1 Average eval loss: 2.6091704369\n",
      "====> Epoch: 2 Average eval loss: 2.6171121597\n",
      "====> Epoch: 3 Average eval loss: 2.3126416206\n",
      "====> Epoch: 4 Average eval loss: 2.2280967236\n",
      "====> Epoch: 5 Average eval loss: 2.0264084339\n",
      "====> Epoch: 6 Average eval loss: 1.8241566420\n",
      "====> Epoch: 7 Average eval loss: 1.7464517355\n",
      "====> Epoch: 8 Average eval loss: 1.5395720005\n",
      "====> Epoch: 9 Average eval loss: 1.5218701363\n",
      "====> Epoch: 10 Average eval loss: 1.3187063932\n",
      "====> Epoch: 11 Average eval loss: 1.2107037306\n",
      "====> Epoch: 12 Average eval loss: 1.7120019197\n",
      "====> Epoch: 13 Average eval loss: 1.1185638905\n",
      "====> Epoch: 14 Average eval loss: 1.1201074123\n",
      "====> Epoch: 15 Average eval loss: 1.1884884834\n",
      "====> Epoch: 16 Average eval loss: 1.1545605659\n",
      "====> Epoch: 17 Average eval loss: 0.9281610847\n",
      "====> Epoch: 18 Average eval loss: 0.9276879430\n",
      "====> Epoch: 19 Average eval loss: 0.8349093795\n",
      "====> Epoch: 20 Average eval loss: 0.8748819232\n",
      "====> Epoch: 21 Average eval loss: 0.8342376947\n",
      "====> Epoch: 22 Average eval loss: 0.7435123324\n",
      "====> Epoch: 23 Average eval loss: 0.8097754717\n",
      "====> Epoch: 24 Average eval loss: 0.8202175498\n",
      "====> Epoch: 25 Average eval loss: 0.5330846310\n",
      "====> Epoch: 26 Average eval loss: 0.7426034212\n",
      "====> Epoch: 27 Average eval loss: 0.7752187252\n",
      "====> Epoch: 28 Average eval loss: 0.7597874999\n",
      "====> Epoch: 29 Average eval loss: 0.6139518619\n",
      "====> Epoch: 30 Average eval loss: 0.4274363220\n",
      "====> Epoch: 31 Average eval loss: 0.6331352592\n",
      "====> Epoch: 32 Average eval loss: 0.9115644097\n",
      "====> Epoch: 33 Average eval loss: 0.9443843365\n",
      "====> Epoch: 34 Average eval loss: 0.5021524429\n",
      "====> Epoch: 35 Average eval loss: 0.4911561906\n",
      "====> Epoch: 36 Average eval loss: 0.2964822352\n",
      "====> Epoch: 37 Average eval loss: 0.7499756813\n",
      "====> Epoch: 38 Average eval loss: 0.5640932322\n",
      "====> Epoch: 39 Average eval loss: 0.6287770271\n",
      "====> Epoch: 40 Average eval loss: 0.5415888429\n",
      "====> Epoch: 41 Average eval loss: 0.6789566278\n",
      "====> Epoch: 42 Average eval loss: 0.5451107621\n",
      "====> Epoch: 43 Average eval loss: 0.9310784936\n",
      "====> Epoch: 44 Average eval loss: 0.5353005528\n",
      "====> Epoch: 45 Average eval loss: 0.3336218297\n",
      "====> Epoch: 46 Average eval loss: 0.5309146643\n",
      "====> Epoch: 47 Average eval loss: 0.4570277333\n",
      "====> Epoch: 48 Average eval loss: 0.8306928277\n",
      "====> Epoch: 49 Average eval loss: 0.8065612316\n",
      "====> Epoch: 50 Average eval loss: 0.4297331870\n",
      "====> Epoch: 51 Average eval loss: 0.5046207905\n",
      "====> Epoch: 52 Average eval loss: 0.4320837855\n",
      "====> Epoch: 53 Average eval loss: 0.4479651153\n",
      "====> Epoch: 54 Average eval loss: 0.6075965166\n",
      "====> Epoch: 55 Average eval loss: 0.5793254972\n",
      "====> Epoch: 56 Average eval loss: 0.5675786138\n",
      "====> Epoch: 57 Average eval loss: 0.4697343707\n",
      "====> Epoch: 58 Average eval loss: 0.5549578667\n",
      "====> Epoch: 59 Average eval loss: 0.4529039860\n",
      "====> Epoch: 60 Average eval loss: 1.1886894703\n",
      "====> Epoch: 61 Average eval loss: 0.6952828169\n",
      "====> Epoch: 62 Average eval loss: 0.8117958903\n",
      "====> Epoch: 63 Average eval loss: 0.6402536035\n",
      "====> Epoch: 64 Average eval loss: 0.9233268499\n",
      "====> Epoch: 65 Average eval loss: 0.6242530942\n",
      "====> Epoch: 66 Average eval loss: 0.6819343567\n",
      "====> Epoch: 67 Average eval loss: 0.7802080512\n",
      "====> Epoch: 68 Average eval loss: 1.0098663568\n",
      "====> Epoch: 69 Average eval loss: 0.3539941311\n",
      "====> Epoch: 70 Average eval loss: 0.4754576087\n",
      "====> Epoch: 71 Average eval loss: 0.8260563016\n",
      "====> Epoch: 72 Average eval loss: 1.0747114420\n",
      "====> Epoch: 73 Average eval loss: 0.7470747828\n",
      "====> Epoch: 74 Average eval loss: 0.4775394797\n",
      "====> Epoch: 75 Average eval loss: 0.8075904846\n",
      "====> Epoch: 76 Average eval loss: 1.0744050741\n",
      "====> Epoch: 77 Average eval loss: 0.8212823868\n",
      "====> Epoch: 78 Average eval loss: 0.5345067382\n",
      "====> Epoch: 79 Average eval loss: 0.8563283682\n",
      "====> Epoch: 80 Average eval loss: 0.7039064169\n",
      "====> Epoch: 81 Average eval loss: 0.6919892430\n",
      "====> Epoch: 82 Average eval loss: 0.7163400054\n",
      "====> Epoch: 83 Average eval loss: 1.2184128761\n",
      "====> Epoch: 84 Average eval loss: 0.8651211858\n",
      "====> Epoch: 85 Average eval loss: 0.5865484476\n",
      "====> Epoch: 86 Average eval loss: 0.5453801751\n",
      "====> Epoch: 87 Average eval loss: 1.1681157351\n",
      "====> Epoch: 88 Average eval loss: 0.5670096874\n",
      "====> Epoch: 89 Average eval loss: 1.1894708872\n",
      "====> Epoch: 90 Average eval loss: 0.7440718412\n",
      "====> Epoch: 91 Average eval loss: 0.6532628536\n",
      "====> Epoch: 92 Average eval loss: 1.1766649485\n",
      "====> Epoch: 93 Average eval loss: 1.0052480698\n",
      "====> Epoch: 94 Average eval loss: 0.9224078059\n",
      "====> Epoch: 95 Average eval loss: 0.7846981287\n",
      "====> Epoch: 96 Average eval loss: 1.0187427998\n",
      "====> Epoch: 97 Average eval loss: 0.6633233428\n",
      "====> Epoch: 98 Average eval loss: 1.0714037418\n",
      "====> Epoch: 99 Average eval loss: 0.7515047789\n",
      "====> Epoch: 100 Average eval loss: 0.9086512327\n",
      "====> Epoch: 101 Average eval loss: 0.9151765108\n",
      "====> Epoch: 102 Average eval loss: 1.0589743853\n",
      "====> Epoch: 103 Average eval loss: 0.8589078188\n",
      "====> Epoch: 104 Average eval loss: 1.1014742851\n",
      "====> Epoch: 105 Average eval loss: 1.1957478523\n",
      "====> Epoch: 106 Average eval loss: 0.6192488074\n",
      "====> Epoch: 107 Average eval loss: 1.1113588810\n",
      "====> Epoch: 108 Average eval loss: 0.7964525223\n",
      "====> Epoch: 109 Average eval loss: 0.9425588846\n",
      "====> Epoch: 110 Average eval loss: 0.9266187549\n",
      "====> Epoch: 111 Average eval loss: 0.8242211938\n",
      "====> Epoch: 112 Average eval loss: 1.0567500591\n",
      "====> Epoch: 113 Average eval loss: 1.0634000301\n",
      "====> Epoch: 114 Average eval loss: 0.8399254680\n",
      "====> Epoch: 115 Average eval loss: 0.6252009869\n",
      "====> Epoch: 116 Average eval loss: 0.9598065019\n",
      "====> Epoch: 117 Average eval loss: 1.2454148531\n",
      "====> Epoch: 118 Average eval loss: 1.4329804182\n",
      "====> Epoch: 119 Average eval loss: 0.6686088443\n",
      "====> Epoch: 120 Average eval loss: 0.8561772108\n",
      "====> Epoch: 121 Average eval loss: 0.9470468760\n",
      "====> Epoch: 122 Average eval loss: 0.9864118099\n",
      "====> Epoch: 123 Average eval loss: 1.1255962849\n",
      "====> Epoch: 124 Average eval loss: 1.0387674570\n",
      "====> Epoch: 125 Average eval loss: 0.7289781570\n",
      "====> Epoch: 126 Average eval loss: 0.7120072246\n",
      "====> Epoch: 127 Average eval loss: 1.3433635235\n",
      "====> Epoch: 128 Average eval loss: 1.2616215944\n",
      "====> Epoch: 129 Average eval loss: 1.0480676889\n",
      "====> Epoch: 130 Average eval loss: 0.8690317273\n",
      "====> Epoch: 131 Average eval loss: 1.0698806047\n",
      "====> Epoch: 132 Average eval loss: 0.6739480495\n",
      "====> Epoch: 133 Average eval loss: 0.6569560170\n",
      "====> Epoch: 134 Average eval loss: 0.9749678373\n",
      "====> Epoch: 135 Average eval loss: 1.0033154488\n",
      "====> Epoch: 136 Average eval loss: 0.8787093759\n",
      "====> Epoch: 137 Average eval loss: 0.9057401419\n",
      "====> Epoch: 138 Average eval loss: 1.0683919191\n",
      "====> Epoch: 139 Average eval loss: 0.9000368714\n",
      "====> Epoch: 140 Average eval loss: 0.6356126070\n",
      "====> Epoch: 141 Average eval loss: 1.0073642731\n",
      "====> Epoch: 142 Average eval loss: 0.9828628898\n",
      "====> Epoch: 143 Average eval loss: 0.7050578594\n",
      "====> Epoch: 144 Average eval loss: 0.7491279244\n",
      "====> Epoch: 145 Average eval loss: 0.7333806753\n",
      "====> Epoch: 146 Average eval loss: 1.2080521584\n",
      "====> Epoch: 147 Average eval loss: 0.9493505359\n",
      "====> Epoch: 148 Average eval loss: 0.8414101601\n",
      "====> Epoch: 149 Average eval loss: 0.9752043486\n",
      "====> Epoch: 150 Average eval loss: 0.7838211656\n",
      "====> Epoch: 151 Average eval loss: 0.5788998604\n",
      "====> Epoch: 152 Average eval loss: 0.6717244983\n",
      "====> Epoch: 153 Average eval loss: 1.1322804689\n",
      "====> Epoch: 154 Average eval loss: 0.6346809268\n",
      "====> Epoch: 155 Average eval loss: 1.0445181131\n",
      "====> Epoch: 156 Average eval loss: 0.7626528740\n",
      "====> Epoch: 157 Average eval loss: 0.8751196265\n",
      "====> Epoch: 158 Average eval loss: 1.0790917873\n",
      "====> Epoch: 159 Average eval loss: 0.8631599545\n",
      "====> Epoch: 160 Average eval loss: 0.8622141480\n",
      "====> Epoch: 161 Average eval loss: 0.7616755366\n",
      "====> Epoch: 162 Average eval loss: 0.6262047291\n",
      "====> Epoch: 163 Average eval loss: 0.9158095717\n",
      "====> Epoch: 164 Average eval loss: 0.7994587421\n",
      "====> Epoch: 165 Average eval loss: 0.8571441174\n",
      "====> Epoch: 166 Average eval loss: 0.8313775063\n",
      "====> Epoch: 167 Average eval loss: 0.7989613414\n",
      "====> Epoch: 168 Average eval loss: 0.6572685242\n",
      "====> Epoch: 169 Average eval loss: 0.6541377902\n",
      "====> Epoch: 170 Average eval loss: 0.5880492926\n",
      "====> Epoch: 171 Average eval loss: 0.6334068775\n",
      "====> Epoch: 172 Average eval loss: 0.8309208751\n",
      "====> Epoch: 173 Average eval loss: 0.6995358467\n",
      "====> Epoch: 174 Average eval loss: 0.8306548595\n",
      "====> Epoch: 175 Average eval loss: 0.8863247633\n",
      "====> Epoch: 176 Average eval loss: 0.4186806679\n",
      "====> Epoch: 177 Average eval loss: 0.5592941046\n",
      "====> Epoch: 178 Average eval loss: 0.7186305523\n",
      "====> Epoch: 179 Average eval loss: 0.6681105494\n",
      "====> Epoch: 180 Average eval loss: 0.5032215714\n",
      "====> Epoch: 181 Average eval loss: 0.7040813565\n",
      "====> Epoch: 182 Average eval loss: 0.5297136307\n",
      "====> Epoch: 183 Average eval loss: 0.6463854313\n",
      "====> Epoch: 184 Average eval loss: 0.3876977563\n",
      "====> Epoch: 185 Average eval loss: 0.7500063181\n",
      "====> Epoch: 186 Average eval loss: 0.3848729432\n",
      "====> Epoch: 187 Average eval loss: 0.3661346734\n",
      "====> Epoch: 188 Average eval loss: 0.6858505607\n",
      "====> Epoch: 189 Average eval loss: 0.4602010548\n",
      "====> Epoch: 190 Average eval loss: 0.7546567321\n",
      "====> Epoch: 191 Average eval loss: 0.5183392167\n",
      "====> Epoch: 192 Average eval loss: 0.3861135542\n",
      "====> Epoch: 193 Average eval loss: 0.5179227591\n",
      "====> Epoch: 194 Average eval loss: 0.4966651201\n",
      "====> Epoch: 195 Average eval loss: 0.3772945106\n",
      "====> Epoch: 196 Average eval loss: 0.4793193340\n",
      "====> Epoch: 197 Average eval loss: 0.4391255677\n",
      "====> Epoch: 198 Average eval loss: 0.2792893350\n",
      "====> Epoch: 199 Average eval loss: 0.4551368654\n",
      "====> Epoch: 200 Average eval loss: 0.3087107837\n",
      "====> Epoch: 201 Average eval loss: 0.3863699138\n",
      "====> Epoch: 202 Average eval loss: 0.4871402085\n",
      "====> Epoch: 203 Average eval loss: 0.4121694267\n",
      "====> Epoch: 204 Average eval loss: 0.3766387701\n",
      "====> Epoch: 205 Average eval loss: 0.3329676688\n",
      "====> Epoch: 206 Average eval loss: 0.2812526822\n",
      "====> Epoch: 207 Average eval loss: 0.4143092632\n",
      "====> Epoch: 208 Average eval loss: 0.4739097059\n",
      "====> Epoch: 209 Average eval loss: 0.3677465618\n",
      "====> Epoch: 210 Average eval loss: 0.5308319330\n",
      "====> Epoch: 211 Average eval loss: 0.4297881722\n",
      "====> Epoch: 212 Average eval loss: 0.2833970189\n",
      "====> Epoch: 213 Average eval loss: 0.4075388014\n",
      "====> Epoch: 214 Average eval loss: 0.3228971064\n",
      "====> Epoch: 215 Average eval loss: 0.3299795389\n",
      "====> Epoch: 216 Average eval loss: 0.4007233977\n",
      "====> Epoch: 217 Average eval loss: 0.2455948740\n",
      "====> Epoch: 218 Average eval loss: 0.3153778315\n",
      "====> Epoch: 219 Average eval loss: 0.2471504062\n",
      "====> Epoch: 220 Average eval loss: 0.2865739167\n",
      "====> Epoch: 221 Average eval loss: 0.2523955703\n",
      "====> Epoch: 222 Average eval loss: 0.2584716678\n",
      "====> Epoch: 223 Average eval loss: 0.1638310701\n",
      "====> Epoch: 224 Average eval loss: 0.1960325986\n",
      "====> Epoch: 225 Average eval loss: 0.2568132579\n",
      "====> Epoch: 226 Average eval loss: 0.1535155624\n",
      "====> Epoch: 227 Average eval loss: 0.2624233365\n",
      "====> Epoch: 228 Average eval loss: 0.1496384293\n",
      "====> Epoch: 229 Average eval loss: 0.3703334332\n",
      "====> Epoch: 230 Average eval loss: 0.1763559580\n",
      "====> Epoch: 231 Average eval loss: 0.3223526776\n",
      "====> Epoch: 232 Average eval loss: 0.2587923408\n",
      "====> Epoch: 233 Average eval loss: 0.1243338734\n",
      "====> Epoch: 234 Average eval loss: 0.1430474520\n",
      "====> Epoch: 235 Average eval loss: 0.2460783124\n",
      "====> Epoch: 236 Average eval loss: 0.1835963130\n",
      "====> Epoch: 237 Average eval loss: 0.2230752110\n",
      "====> Epoch: 238 Average eval loss: 0.2196791917\n",
      "====> Epoch: 239 Average eval loss: 0.1491046846\n",
      "====> Epoch: 240 Average eval loss: 0.1797940582\n",
      "====> Epoch: 241 Average eval loss: 0.1273790300\n",
      "====> Epoch: 242 Average eval loss: 0.1462146640\n",
      "====> Epoch: 243 Average eval loss: 0.1370131522\n",
      "====> Epoch: 244 Average eval loss: 0.1378962100\n",
      "====> Epoch: 245 Average eval loss: 0.2012658864\n",
      "====> Epoch: 246 Average eval loss: 0.2081554830\n",
      "====> Epoch: 247 Average eval loss: 0.1373284608\n",
      "====> Epoch: 248 Average eval loss: 0.1289176196\n",
      "====> Epoch: 249 Average eval loss: 0.1544796377\n",
      "Best testing error sparse FCNN is 0.12433387339115143 and it was found at epoch 233\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train and evaluate the l0sindy_model.",
   "id": "f0b75e1648d9470d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T12:36:56.989854Z",
     "start_time": "2024-06-19T12:32:53.534828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# degree of the polynomial features\n",
    "degree = 3\n",
    "\n",
    "reg_coefficient = 0.0005\n",
    "l0sindy_model = L0SINDy_reward(input_dim=obs_dim+act_dim, output_dim=1, degree=degree, lambda_coeff=reg_coefficient)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    l0sindy_model = l0sindy_model.cuda()\n",
    "\n",
    "optimizer_fcnn = torch.optim.Adam([\n",
    "    {'params': l0sindy_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_l0sindy = train_eval_reward_model(l0sindy_model, optimizer_fcnn, training_buffer, testing_buffer, batch_size, num_epochs, l0=True)\n",
    "print(\"Best testing error L0 SINDy is {} and it was found at epoch {}\".format(metrics_l0sindy[2], metrics_l0sindy[3]))\n",
    "\n",
    "l0sindy_model.print_equations()"
   ],
   "id": "d8761a32de0032cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy polynomial of order  3\n",
      "with 35 coefficients\n",
      "['1' 'x0' 'x1' 'x2' 'x3' 'x0^2' 'x0 x1' 'x0 x2' 'x0 x3' 'x1^2' 'x1 x2'\n",
      " 'x1 x3' 'x2^2' 'x2 x3' 'x3^2' 'x0^3' 'x0^2 x1' 'x0^2 x2' 'x0^2 x3'\n",
      " 'x0 x1^2' 'x0 x1 x2' 'x0 x1 x3' 'x0 x2^2' 'x0 x2 x3' 'x0 x3^2' 'x1^3'\n",
      " 'x1^2 x2' 'x1^2 x3' 'x1 x2^2' 'x1 x2 x3' 'x1 x3^2' 'x2^3' 'x2^2 x3'\n",
      " 'x2 x3^2' 'x3^3']\n",
      "L0Dense(35 -> 1, droprate_init=0.5, lamba=0.0003, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False, bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/Documents/Sparsifying-Parametric-Models-with-L0/utils/l0_layer.py:51: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.weights, mode='fan_out')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average eval loss: 56.9089736938\n",
      "====> Epoch: 1 Average eval loss: 30.5081157684\n",
      "====> Epoch: 2 Average eval loss: 19.4083766937\n",
      "====> Epoch: 3 Average eval loss: 14.3223829269\n",
      "====> Epoch: 4 Average eval loss: 10.9057102203\n",
      "====> Epoch: 5 Average eval loss: 8.5602312088\n",
      "====> Epoch: 6 Average eval loss: 6.8299779892\n",
      "====> Epoch: 7 Average eval loss: 5.5918631554\n",
      "====> Epoch: 8 Average eval loss: 4.5806827545\n",
      "====> Epoch: 9 Average eval loss: 3.7504589558\n",
      "====> Epoch: 10 Average eval loss: 3.0535631180\n",
      "====> Epoch: 11 Average eval loss: 2.4577336311\n",
      "====> Epoch: 12 Average eval loss: 1.9992538691\n",
      "====> Epoch: 13 Average eval loss: 1.5816576481\n",
      "====> Epoch: 14 Average eval loss: 1.3170291185\n",
      "====> Epoch: 15 Average eval loss: 1.1563256979\n",
      "====> Epoch: 16 Average eval loss: 1.0212298632\n",
      "====> Epoch: 17 Average eval loss: 0.9242466688\n",
      "====> Epoch: 18 Average eval loss: 0.8519048691\n",
      "====> Epoch: 19 Average eval loss: 0.7418558002\n",
      "====> Epoch: 20 Average eval loss: 0.7104594111\n",
      "====> Epoch: 21 Average eval loss: 0.6738933325\n",
      "====> Epoch: 22 Average eval loss: 0.7291895151\n",
      "====> Epoch: 23 Average eval loss: 0.5519877076\n",
      "====> Epoch: 24 Average eval loss: 0.6261840463\n",
      "====> Epoch: 25 Average eval loss: 0.6894688010\n",
      "====> Epoch: 26 Average eval loss: 0.5232186317\n",
      "====> Epoch: 27 Average eval loss: 0.5381165743\n",
      "====> Epoch: 28 Average eval loss: 0.4378660619\n",
      "====> Epoch: 29 Average eval loss: 0.3964415789\n",
      "====> Epoch: 30 Average eval loss: 0.4298646450\n",
      "====> Epoch: 31 Average eval loss: 0.3518047631\n",
      "====> Epoch: 32 Average eval loss: 0.3658835888\n",
      "====> Epoch: 33 Average eval loss: 0.3244312108\n",
      "====> Epoch: 34 Average eval loss: 0.3690727353\n",
      "====> Epoch: 35 Average eval loss: 0.3373778164\n",
      "====> Epoch: 36 Average eval loss: 0.3291418254\n",
      "====> Epoch: 37 Average eval loss: 0.3213290572\n",
      "====> Epoch: 38 Average eval loss: 0.2622298896\n",
      "====> Epoch: 39 Average eval loss: 0.3038079441\n",
      "====> Epoch: 40 Average eval loss: 0.2427206486\n",
      "====> Epoch: 41 Average eval loss: 0.2340903729\n",
      "====> Epoch: 42 Average eval loss: 0.2123378366\n",
      "====> Epoch: 43 Average eval loss: 0.2151446790\n",
      "====> Epoch: 44 Average eval loss: 0.2255819738\n",
      "====> Epoch: 45 Average eval loss: 0.2283885181\n",
      "====> Epoch: 46 Average eval loss: 0.1867039353\n",
      "====> Epoch: 47 Average eval loss: 0.1697600335\n",
      "====> Epoch: 48 Average eval loss: 0.1772757769\n",
      "====> Epoch: 49 Average eval loss: 0.1745606661\n",
      "====> Epoch: 50 Average eval loss: 0.1444579363\n",
      "====> Epoch: 51 Average eval loss: 0.1577296108\n",
      "====> Epoch: 52 Average eval loss: 0.1453125328\n",
      "====> Epoch: 53 Average eval loss: 0.1559876651\n",
      "====> Epoch: 54 Average eval loss: 0.1482113451\n",
      "====> Epoch: 55 Average eval loss: 0.1322172135\n",
      "====> Epoch: 56 Average eval loss: 0.1536952853\n",
      "====> Epoch: 57 Average eval loss: 0.1335000396\n",
      "====> Epoch: 58 Average eval loss: 0.1319521070\n",
      "====> Epoch: 59 Average eval loss: 0.1275869012\n",
      "====> Epoch: 60 Average eval loss: 0.1324968040\n",
      "====> Epoch: 61 Average eval loss: 0.1282138228\n",
      "====> Epoch: 62 Average eval loss: 0.1242016703\n",
      "====> Epoch: 63 Average eval loss: 0.1094062477\n",
      "====> Epoch: 64 Average eval loss: 0.1230245307\n",
      "====> Epoch: 65 Average eval loss: 0.1114943549\n",
      "====> Epoch: 66 Average eval loss: 0.1085609570\n",
      "====> Epoch: 67 Average eval loss: 0.1380565912\n",
      "====> Epoch: 68 Average eval loss: 0.1266964823\n",
      "====> Epoch: 69 Average eval loss: 0.1420483589\n",
      "====> Epoch: 70 Average eval loss: 0.1048968211\n",
      "====> Epoch: 71 Average eval loss: 0.1016885564\n",
      "====> Epoch: 72 Average eval loss: 0.1036925614\n",
      "====> Epoch: 73 Average eval loss: 0.1167401373\n",
      "====> Epoch: 74 Average eval loss: 0.1071690917\n",
      "====> Epoch: 75 Average eval loss: 0.1044505090\n",
      "====> Epoch: 76 Average eval loss: 0.1044211164\n",
      "====> Epoch: 77 Average eval loss: 0.1138737053\n",
      "====> Epoch: 78 Average eval loss: 0.1068071499\n",
      "====> Epoch: 79 Average eval loss: 0.1030170992\n",
      "====> Epoch: 80 Average eval loss: 0.1008104086\n",
      "====> Epoch: 81 Average eval loss: 0.1102764234\n",
      "====> Epoch: 82 Average eval loss: 0.0998351872\n",
      "====> Epoch: 83 Average eval loss: 0.0996156111\n",
      "====> Epoch: 84 Average eval loss: 0.0983995721\n",
      "====> Epoch: 85 Average eval loss: 0.1022464260\n",
      "====> Epoch: 86 Average eval loss: 0.1050362140\n",
      "====> Epoch: 87 Average eval loss: 0.1005807072\n",
      "====> Epoch: 88 Average eval loss: 0.1058217362\n",
      "====> Epoch: 89 Average eval loss: 0.1033588499\n",
      "====> Epoch: 90 Average eval loss: 0.1026608273\n",
      "====> Epoch: 91 Average eval loss: 0.1020412296\n",
      "====> Epoch: 92 Average eval loss: 0.1007195562\n",
      "====> Epoch: 93 Average eval loss: 0.0987692475\n",
      "====> Epoch: 94 Average eval loss: 0.0977155864\n",
      "====> Epoch: 95 Average eval loss: 0.1088911071\n",
      "====> Epoch: 96 Average eval loss: 0.1015994847\n",
      "====> Epoch: 97 Average eval loss: 0.0983923301\n",
      "====> Epoch: 98 Average eval loss: 0.0983280316\n",
      "====> Epoch: 99 Average eval loss: 0.1041776091\n",
      "====> Epoch: 100 Average eval loss: 0.0987519845\n",
      "====> Epoch: 101 Average eval loss: 0.0972050652\n",
      "====> Epoch: 102 Average eval loss: 0.1044748873\n",
      "====> Epoch: 103 Average eval loss: 0.0968732983\n",
      "====> Epoch: 104 Average eval loss: 0.1000140831\n",
      "====> Epoch: 105 Average eval loss: 0.0973926783\n",
      "====> Epoch: 106 Average eval loss: 0.0980893448\n",
      "====> Epoch: 107 Average eval loss: 0.0979435369\n",
      "====> Epoch: 108 Average eval loss: 0.0968057215\n",
      "====> Epoch: 109 Average eval loss: 0.0990530849\n",
      "====> Epoch: 110 Average eval loss: 0.0983288512\n",
      "====> Epoch: 111 Average eval loss: 0.1054076254\n",
      "====> Epoch: 112 Average eval loss: 0.0993344188\n",
      "====> Epoch: 113 Average eval loss: 0.1028704792\n",
      "====> Epoch: 114 Average eval loss: 0.0972751454\n",
      "====> Epoch: 115 Average eval loss: 0.0977501199\n",
      "====> Epoch: 116 Average eval loss: 0.0978769511\n",
      "====> Epoch: 117 Average eval loss: 0.0975179151\n",
      "====> Epoch: 118 Average eval loss: 0.0963494629\n",
      "====> Epoch: 119 Average eval loss: 0.0957737789\n",
      "====> Epoch: 120 Average eval loss: 0.0962260142\n",
      "====> Epoch: 121 Average eval loss: 0.0978957787\n",
      "====> Epoch: 122 Average eval loss: 0.0976609588\n",
      "====> Epoch: 123 Average eval loss: 0.0975109339\n",
      "====> Epoch: 124 Average eval loss: 0.0999964178\n",
      "====> Epoch: 125 Average eval loss: 0.0980878547\n",
      "====> Epoch: 126 Average eval loss: 0.0979142040\n",
      "====> Epoch: 127 Average eval loss: 0.0969938487\n",
      "====> Epoch: 128 Average eval loss: 0.0962520093\n",
      "====> Epoch: 129 Average eval loss: 0.0958789736\n",
      "====> Epoch: 130 Average eval loss: 0.0958268270\n",
      "====> Epoch: 131 Average eval loss: 0.0959225670\n",
      "====> Epoch: 132 Average eval loss: 0.0962596908\n",
      "====> Epoch: 133 Average eval loss: 0.0999615490\n",
      "====> Epoch: 134 Average eval loss: 0.0963159800\n",
      "====> Epoch: 135 Average eval loss: 0.0958878919\n",
      "====> Epoch: 136 Average eval loss: 0.0984297246\n",
      "====> Epoch: 137 Average eval loss: 0.0959294811\n",
      "====> Epoch: 138 Average eval loss: 0.0957088098\n",
      "====> Epoch: 139 Average eval loss: 0.0966409519\n",
      "====> Epoch: 140 Average eval loss: 0.0967147723\n",
      "====> Epoch: 141 Average eval loss: 0.0957958996\n",
      "====> Epoch: 142 Average eval loss: 0.0969143137\n",
      "====> Epoch: 143 Average eval loss: 0.0971125662\n",
      "====> Epoch: 144 Average eval loss: 0.0961728692\n",
      "====> Epoch: 145 Average eval loss: 0.0965329632\n",
      "====> Epoch: 146 Average eval loss: 0.0959663764\n",
      "====> Epoch: 147 Average eval loss: 0.0961652473\n",
      "====> Epoch: 148 Average eval loss: 0.0977436155\n",
      "====> Epoch: 149 Average eval loss: 0.0957631320\n",
      "====> Epoch: 150 Average eval loss: 0.0966987610\n",
      "====> Epoch: 151 Average eval loss: 0.0959060863\n",
      "====> Epoch: 152 Average eval loss: 0.0959329903\n",
      "====> Epoch: 153 Average eval loss: 0.0956082493\n",
      "====> Epoch: 154 Average eval loss: 0.0955836251\n",
      "====> Epoch: 155 Average eval loss: 0.0954869911\n",
      "====> Epoch: 156 Average eval loss: 0.0956698060\n",
      "====> Epoch: 157 Average eval loss: 0.0957194567\n",
      "====> Epoch: 158 Average eval loss: 0.0976401195\n",
      "====> Epoch: 159 Average eval loss: 0.0960824192\n",
      "====> Epoch: 160 Average eval loss: 0.0959699079\n",
      "====> Epoch: 161 Average eval loss: 0.0966464356\n",
      "====> Epoch: 162 Average eval loss: 0.0962686688\n",
      "====> Epoch: 163 Average eval loss: 0.0961816460\n",
      "====> Epoch: 164 Average eval loss: 0.0963384882\n",
      "====> Epoch: 165 Average eval loss: 0.0966804102\n",
      "====> Epoch: 166 Average eval loss: 0.0960073248\n",
      "====> Epoch: 167 Average eval loss: 0.0968069583\n",
      "====> Epoch: 168 Average eval loss: 0.0954940766\n",
      "====> Epoch: 169 Average eval loss: 0.0959631577\n",
      "====> Epoch: 170 Average eval loss: 0.0953948349\n",
      "====> Epoch: 171 Average eval loss: 0.0954868048\n",
      "====> Epoch: 172 Average eval loss: 0.0955878347\n",
      "====> Epoch: 173 Average eval loss: 0.0958591998\n",
      "====> Epoch: 174 Average eval loss: 0.0999287367\n",
      "====> Epoch: 175 Average eval loss: 0.0955099985\n",
      "====> Epoch: 176 Average eval loss: 0.1023972780\n",
      "====> Epoch: 177 Average eval loss: 0.0956289619\n",
      "====> Epoch: 178 Average eval loss: 0.0954809561\n",
      "====> Epoch: 179 Average eval loss: 0.0955013037\n",
      "====> Epoch: 180 Average eval loss: 0.0957052559\n",
      "====> Epoch: 181 Average eval loss: 0.0956313312\n",
      "====> Epoch: 182 Average eval loss: 0.0954148769\n",
      "====> Epoch: 183 Average eval loss: 0.0956103653\n",
      "====> Epoch: 184 Average eval loss: 0.0956898779\n",
      "====> Epoch: 185 Average eval loss: 0.0969427973\n",
      "====> Epoch: 186 Average eval loss: 0.0953886807\n",
      "====> Epoch: 187 Average eval loss: 0.0956538171\n",
      "====> Epoch: 188 Average eval loss: 0.0964194313\n",
      "====> Epoch: 189 Average eval loss: 0.0954067707\n",
      "====> Epoch: 190 Average eval loss: 0.0965352133\n",
      "====> Epoch: 191 Average eval loss: 0.0955000743\n",
      "====> Epoch: 192 Average eval loss: 0.0954123139\n",
      "====> Epoch: 193 Average eval loss: 0.0956154764\n",
      "====> Epoch: 194 Average eval loss: 0.0958327726\n",
      "====> Epoch: 195 Average eval loss: 0.0954380631\n",
      "====> Epoch: 196 Average eval loss: 0.0968538001\n",
      "====> Epoch: 197 Average eval loss: 0.0954485387\n",
      "====> Epoch: 198 Average eval loss: 0.0966372043\n",
      "====> Epoch: 199 Average eval loss: 0.0961269364\n",
      "====> Epoch: 200 Average eval loss: 0.0953765437\n",
      "====> Epoch: 201 Average eval loss: 0.0971122012\n",
      "====> Epoch: 202 Average eval loss: 0.0955730975\n",
      "====> Epoch: 203 Average eval loss: 0.0953910798\n",
      "====> Epoch: 204 Average eval loss: 0.0955743343\n",
      "====> Epoch: 205 Average eval loss: 0.0954168513\n",
      "====> Epoch: 206 Average eval loss: 0.0955224261\n",
      "====> Epoch: 207 Average eval loss: 0.0955953598\n",
      "====> Epoch: 208 Average eval loss: 0.0958078727\n",
      "====> Epoch: 209 Average eval loss: 0.0971154422\n",
      "====> Epoch: 210 Average eval loss: 0.0988824591\n",
      "====> Epoch: 211 Average eval loss: 0.0959733203\n",
      "====> Epoch: 212 Average eval loss: 0.0955041498\n",
      "====> Epoch: 213 Average eval loss: 0.0954227075\n",
      "====> Epoch: 214 Average eval loss: 0.0960828736\n",
      "====> Epoch: 215 Average eval loss: 0.0955497175\n",
      "====> Epoch: 216 Average eval loss: 0.0956118330\n",
      "====> Epoch: 217 Average eval loss: 0.0962629095\n",
      "====> Epoch: 218 Average eval loss: 0.0954466760\n",
      "====> Epoch: 219 Average eval loss: 0.0954120830\n",
      "====> Epoch: 220 Average eval loss: 0.0955277607\n",
      "====> Epoch: 221 Average eval loss: 0.0954983234\n",
      "====> Epoch: 222 Average eval loss: 0.1004800424\n",
      "====> Epoch: 223 Average eval loss: 0.0955484584\n",
      "====> Epoch: 224 Average eval loss: 0.0955922976\n",
      "====> Epoch: 225 Average eval loss: 0.0954527706\n",
      "====> Epoch: 226 Average eval loss: 0.0960554928\n",
      "====> Epoch: 227 Average eval loss: 0.0955223888\n",
      "====> Epoch: 228 Average eval loss: 0.0953776613\n",
      "====> Epoch: 229 Average eval loss: 0.0955051035\n",
      "====> Epoch: 230 Average eval loss: 0.0954177007\n",
      "====> Epoch: 231 Average eval loss: 0.0957550704\n",
      "====> Epoch: 232 Average eval loss: 0.0953366905\n",
      "====> Epoch: 233 Average eval loss: 0.0959491208\n",
      "====> Epoch: 234 Average eval loss: 0.0955541953\n",
      "====> Epoch: 235 Average eval loss: 0.0953509957\n",
      "====> Epoch: 236 Average eval loss: 0.0956978500\n",
      "====> Epoch: 237 Average eval loss: 0.0954606533\n",
      "====> Epoch: 238 Average eval loss: 0.0955612138\n",
      "====> Epoch: 239 Average eval loss: 0.0953773260\n",
      "====> Epoch: 240 Average eval loss: 0.0966712534\n",
      "====> Epoch: 241 Average eval loss: 0.0952237621\n",
      "====> Epoch: 242 Average eval loss: 0.0954245478\n",
      "====> Epoch: 243 Average eval loss: 0.0951307490\n",
      "====> Epoch: 244 Average eval loss: 0.1009054109\n",
      "====> Epoch: 245 Average eval loss: 0.0954377055\n",
      "====> Epoch: 246 Average eval loss: 0.0954877660\n",
      "====> Epoch: 247 Average eval loss: 0.0958025232\n",
      "====> Epoch: 248 Average eval loss: 0.0953470692\n",
      "====> Epoch: 249 Average eval loss: 0.0956621468\n",
      "Best testing error L0 SINDy is 0.09513074904680252 and it was found at epoch 243\n",
      "x0 = s1, x1 = s2, x2 = s3, x3 = u\n",
      "r is equal to:\n",
      "['1' 'x0' 'x0^2' 'x1^2' 'x2^2' 'x0^3' 'x0 x1^2' 'x0 x2^2']\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[-1.3945e+00,  2.2144e+00, -2.8890e+00, -9.0011e-01, -1.0041e-01,\n",
      "          2.3985e+00,  1.0950e-01, -1.2556e-03]], device='cuda:0',\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Eventually, we plot the mean-squared error between the predictions and the ground-truth over the training and test set.",
   "id": "70753772aa0dc06f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T12:51:37.602095Z",
     "start_time": "2024-06-19T12:51:37.435953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# creating the plots\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Training and Evaluation Metrics')\n",
    "\n",
    "data_train = {'FCNN': metrics_fcnn[0], 'SparseFCNN': metrics_sparsefcnn[0], 'L0SINDy': metrics_l0sindy[0]}\n",
    "methods_train = list(data_train.keys())\n",
    "values_train = list(data_train.values())\n",
    "\n",
    "# creating the bar plot\n",
    "ax1.bar(methods_train, values_train, color='maroon', width=0.4)\n",
    "\n",
    "data_eval = {'FCNN': metrics_fcnn[2], 'SparseFCNN': metrics_sparsefcnn[2], 'L0SINDy': metrics_l0sindy[2]}\n",
    "methods_eval = list(data_eval.keys())\n",
    "values_eval = list(data_eval.values())\n",
    "\n",
    "ax2.bar(methods_eval, values_eval, color='blue', width=0.4)\n",
    "\n",
    "save_dir = \"figures\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "fig.savefig('figures/LearningReward.png', dpi=300)"
   ],
   "id": "3f52e2a18f31e291",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "381edc8efce1d094"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
