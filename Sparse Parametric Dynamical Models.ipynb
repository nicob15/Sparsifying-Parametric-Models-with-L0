{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Learning Sparse Dynamical Models\n",
    "\n",
    "We consider the case of learning the dynamics of a pendulum. In particular, we aim to learn the transition function $T:\\mathcal{S}\\times \\mathcal{A}\\rightarrow \\mathcal{S}$, predicting the next state $\\hat{\\mathbf{s}}_{t+1}$ from the current state $\\mathbf{s}_t$ and action $\\mathbf{a}_t$: $$\\hat{\\mathbf{s}}_{t+1}=T(\\mathbf{s}_t, \\mathbf{a}_t).$$ We will approximate $T(\\mathbf{s}_t, \\mathbf{a}_t)$ using:\n",
    "* a fully-connected neural network (fcnn_model),\n",
    "* a fully-connected neural network sparsified by the L$_0$ regularization (sparsefcnn_model), and\n",
    "* a SINDy-like architecture again sparsified by the L$_0$ regularization (l0sindy_model)."
   ],
   "id": "7b7ca3e2b76dfbb6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-19T09:37:45.443241Z",
     "start_time": "2024-06-19T09:37:44.662323Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds\n",
    "seed = 23524\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We first create a training set composed of 1000 episodes of 200 steps each. The actions are sampled from a random policy.",
   "id": "8ddbdef861816cc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:37:55.261432Z",
     "start_time": "2024-06-19T09:37:45.444305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "render = False\n",
    "if render:\n",
    "    env = gym.make('Pendulum-v1', g=9.81, render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make('Pendulum-v1', g=9.81)\n",
    "max_episodes = 1000\n",
    "max_steps = 200\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "buf_dim = int(max_episodes*max_steps)\n",
    "\n",
    "training_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=buf_dim)\n",
    "\n",
    "# create training set\n",
    "for episode in range(max_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    for steps in range(max_steps+1):\n",
    "        action = env.action_space.sample()\n",
    "        next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        training_buffer.store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "print(\"Finished creating the training set\")"
   ],
   "id": "f30d49f053f6de6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/Documents/Sparsifying-Parametric-Models-with-L0/venv/lib/python3.10/site-packages/gymnasium/envs/classic_control/pendulum.py:173: UserWarning: \u001B[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\u001B[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating the training set\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Secondly, we create the test set that we will use to evaluate the models.",
   "id": "8273bd38694c4609"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:37:56.254407Z",
     "start_time": "2024-06-19T09:37:55.262229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create test set\n",
    "max_episodes_test = 100\n",
    "buf_dim = int(max_episodes*max_steps)\n",
    "\n",
    "testing_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=buf_dim)\n",
    "\n",
    "# create training set\n",
    "for episode in range(max_episodes_test):\n",
    "    observation, _ = env.reset()\n",
    "    for steps in range(max_steps + 1):\n",
    "        action = env.action_space.sample()\n",
    "        next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        testing_buffer.store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "print(\"Finished creating the test set\")"
   ],
   "id": "5c45b1ee713bfd84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating the test set\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After creating train and test set, we are now ready to train the three models. We utilize the same learning rate, batch size, and number of traning epochs.",
   "id": "1dd2318732f7de44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:41:28.987536Z",
     "start_time": "2024-06-19T10:41:28.984628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.models import FCNN, SparseFCNN, L0SINDy_dynamics\n",
    "from utils.trainer import train_eval_dynamics_model\n",
    "import torch\n",
    "\n",
    "# number of hidden units used by the fcnn_model and the sparsefcnn_model\n",
    "h_dim = 128\n",
    "\n",
    "# shared hyperparameter of the experiment\n",
    "lr = 3e-4\n",
    "batch_size = 256\n",
    "num_epochs = 500"
   ],
   "id": "5b9d333e476e4824",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train and evaluate the fcnn_model.",
   "id": "74aea84006ae75c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T10:44:30.464181Z",
     "start_time": "2024-06-19T10:41:32.408763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fcnn_model = FCNN(input_dim=obs_dim+act_dim, output_dim=obs_dim, h_dim=h_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    fcnn_model = fcnn_model.cuda()\n",
    "\n",
    "optimizer_fcnn = torch.optim.Adam([\n",
    "    {'params': fcnn_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_fcnn = train_eval_dynamics_model(fcnn_model, optimizer_fcnn, training_buffer, testing_buffer, batch_size, num_epochs)\n",
    "print(\"Best testing error FCNN is {} and it was found at epoch {}\".format(metrics_fcnn[2], metrics_fcnn[3]))"
   ],
   "id": "2cac37c5b681ba1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average train loss: 0.1849090033\n",
      "====> Epoch: 1 Average train loss: 0.0030612013\n",
      "====> Epoch: 2 Average train loss: 0.0014704300\n",
      "====> Epoch: 3 Average train loss: 0.0009033932\n",
      "====> Epoch: 4 Average train loss: 0.0006679485\n",
      "====> Epoch: 5 Average train loss: 0.0006027165\n",
      "====> Epoch: 6 Average train loss: 0.0006341432\n",
      "====> Epoch: 7 Average train loss: 0.0005952713\n",
      "====> Epoch: 8 Average train loss: 0.0005972260\n",
      "====> Epoch: 9 Average train loss: 0.0006539861\n",
      "====> Epoch: 10 Average train loss: 0.0005261095\n",
      "====> Epoch: 11 Average train loss: 0.0005985592\n",
      "====> Epoch: 12 Average train loss: 0.0005566600\n",
      "====> Epoch: 13 Average train loss: 0.0005715296\n",
      "====> Epoch: 14 Average train loss: 0.0005067738\n",
      "====> Epoch: 15 Average train loss: 0.0004967948\n",
      "====> Epoch: 16 Average train loss: 0.0004753288\n",
      "====> Epoch: 17 Average train loss: 0.0005569229\n",
      "====> Epoch: 18 Average train loss: 0.0005299216\n",
      "====> Epoch: 19 Average train loss: 0.0004183819\n",
      "====> Epoch: 20 Average train loss: 0.0005212155\n",
      "====> Epoch: 21 Average train loss: 0.0004807141\n",
      "====> Epoch: 22 Average train loss: 0.0004867494\n",
      "====> Epoch: 23 Average train loss: 0.0004688669\n",
      "====> Epoch: 24 Average train loss: 0.0004715604\n",
      "====> Epoch: 25 Average train loss: 0.0005064325\n",
      "====> Epoch: 26 Average train loss: 0.0004325097\n",
      "====> Epoch: 27 Average train loss: 0.0004430222\n",
      "====> Epoch: 28 Average train loss: 0.0004957195\n",
      "====> Epoch: 29 Average train loss: 0.0004401097\n",
      "====> Epoch: 30 Average train loss: 0.0004316446\n",
      "====> Epoch: 31 Average train loss: 0.0005264357\n",
      "====> Epoch: 32 Average train loss: 0.0004813968\n",
      "====> Epoch: 33 Average train loss: 0.0003803613\n",
      "====> Epoch: 34 Average train loss: 0.0004898765\n",
      "====> Epoch: 35 Average train loss: 0.0003885382\n",
      "====> Epoch: 36 Average train loss: 0.0004452252\n",
      "====> Epoch: 37 Average train loss: 0.0004571317\n",
      "====> Epoch: 38 Average train loss: 0.0004423031\n",
      "====> Epoch: 39 Average train loss: 0.0004292455\n",
      "====> Epoch: 40 Average train loss: 0.0004433825\n",
      "====> Epoch: 41 Average train loss: 0.0003952711\n",
      "====> Epoch: 42 Average train loss: 0.0004320231\n",
      "====> Epoch: 43 Average train loss: 0.0004032482\n",
      "====> Epoch: 44 Average train loss: 0.0004309104\n",
      "====> Epoch: 45 Average train loss: 0.0004253840\n",
      "====> Epoch: 46 Average train loss: 0.0004539377\n",
      "====> Epoch: 47 Average train loss: 0.0003964354\n",
      "====> Epoch: 48 Average train loss: 0.0004293938\n",
      "====> Epoch: 49 Average train loss: 0.0004137262\n",
      "====> Epoch: 50 Average train loss: 0.0004142987\n",
      "====> Epoch: 51 Average train loss: 0.0003813600\n",
      "====> Epoch: 52 Average train loss: 0.0004893305\n",
      "====> Epoch: 53 Average train loss: 0.0003463669\n",
      "====> Epoch: 54 Average train loss: 0.0004043321\n",
      "====> Epoch: 55 Average train loss: 0.0003796992\n",
      "====> Epoch: 56 Average train loss: 0.0004117542\n",
      "====> Epoch: 57 Average train loss: 0.0003829838\n",
      "====> Epoch: 58 Average train loss: 0.0004152131\n",
      "====> Epoch: 59 Average train loss: 0.0004036855\n",
      "====> Epoch: 60 Average train loss: 0.0004258751\n",
      "====> Epoch: 61 Average train loss: 0.0003937578\n",
      "====> Epoch: 62 Average train loss: 0.0004028111\n",
      "====> Epoch: 63 Average train loss: 0.0003768687\n",
      "====> Epoch: 64 Average train loss: 0.0003849651\n",
      "====> Epoch: 65 Average train loss: 0.0004202286\n",
      "====> Epoch: 66 Average train loss: 0.0003547081\n",
      "====> Epoch: 67 Average train loss: 0.0004864280\n",
      "====> Epoch: 68 Average train loss: 0.0002896755\n",
      "====> Epoch: 69 Average train loss: 0.0004101407\n",
      "====> Epoch: 70 Average train loss: 0.0004486996\n",
      "====> Epoch: 71 Average train loss: 0.0002978991\n",
      "====> Epoch: 72 Average train loss: 0.0004012511\n",
      "====> Epoch: 73 Average train loss: 0.0004328635\n",
      "====> Epoch: 74 Average train loss: 0.0003385713\n",
      "====> Epoch: 75 Average train loss: 0.0003627272\n",
      "====> Epoch: 76 Average train loss: 0.0003901780\n",
      "====> Epoch: 77 Average train loss: 0.0003475533\n",
      "====> Epoch: 78 Average train loss: 0.0003964183\n",
      "====> Epoch: 79 Average train loss: 0.0003381306\n",
      "====> Epoch: 80 Average train loss: 0.0003903004\n",
      "====> Epoch: 81 Average train loss: 0.0003352262\n",
      "====> Epoch: 82 Average train loss: 0.0003861523\n",
      "====> Epoch: 83 Average train loss: 0.0003744560\n",
      "====> Epoch: 84 Average train loss: 0.0003938825\n",
      "====> Epoch: 85 Average train loss: 0.0003371302\n",
      "====> Epoch: 86 Average train loss: 0.0003536017\n",
      "====> Epoch: 87 Average train loss: 0.0003659728\n",
      "====> Epoch: 88 Average train loss: 0.0004040186\n",
      "====> Epoch: 89 Average train loss: 0.0003303351\n",
      "====> Epoch: 90 Average train loss: 0.0003607802\n",
      "====> Epoch: 91 Average train loss: 0.0003453696\n",
      "====> Epoch: 92 Average train loss: 0.0003453776\n",
      "====> Epoch: 93 Average train loss: 0.0003466138\n",
      "====> Epoch: 94 Average train loss: 0.0003793113\n",
      "====> Epoch: 95 Average train loss: 0.0003054002\n",
      "====> Epoch: 96 Average train loss: 0.0003441165\n",
      "====> Epoch: 97 Average train loss: 0.0004278862\n",
      "====> Epoch: 98 Average train loss: 0.0003445903\n",
      "====> Epoch: 99 Average train loss: 0.0003403558\n",
      "====> Epoch: 100 Average train loss: 0.0003350154\n",
      "====> Epoch: 101 Average train loss: 0.0003583798\n",
      "====> Epoch: 102 Average train loss: 0.0003090197\n",
      "====> Epoch: 103 Average train loss: 0.0003946317\n",
      "====> Epoch: 104 Average train loss: 0.0003239664\n",
      "====> Epoch: 105 Average train loss: 0.0003720695\n",
      "====> Epoch: 106 Average train loss: 0.0003009921\n",
      "====> Epoch: 107 Average train loss: 0.0003438091\n",
      "====> Epoch: 108 Average train loss: 0.0003631540\n",
      "====> Epoch: 109 Average train loss: 0.0003371130\n",
      "====> Epoch: 110 Average train loss: 0.0002837538\n",
      "====> Epoch: 111 Average train loss: 0.0003709752\n",
      "====> Epoch: 112 Average train loss: 0.0003454908\n",
      "====> Epoch: 113 Average train loss: 0.0003228214\n",
      "====> Epoch: 114 Average train loss: 0.0003484875\n",
      "====> Epoch: 115 Average train loss: 0.0003292624\n",
      "====> Epoch: 116 Average train loss: 0.0003134664\n",
      "====> Epoch: 117 Average train loss: 0.0003299438\n",
      "====> Epoch: 118 Average train loss: 0.0003115494\n",
      "====> Epoch: 119 Average train loss: 0.0003160839\n",
      "====> Epoch: 120 Average train loss: 0.0003438407\n",
      "====> Epoch: 121 Average train loss: 0.0003062851\n",
      "====> Epoch: 122 Average train loss: 0.0003338083\n",
      "====> Epoch: 123 Average train loss: 0.0003264376\n",
      "====> Epoch: 124 Average train loss: 0.0003226177\n",
      "====> Epoch: 125 Average train loss: 0.0003642424\n",
      "====> Epoch: 126 Average train loss: 0.0003123802\n",
      "====> Epoch: 127 Average train loss: 0.0003321647\n",
      "====> Epoch: 128 Average train loss: 0.0002779338\n",
      "====> Epoch: 129 Average train loss: 0.0002865439\n",
      "====> Epoch: 130 Average train loss: 0.0003122263\n",
      "====> Epoch: 131 Average train loss: 0.0003255563\n",
      "====> Epoch: 132 Average train loss: 0.0003550243\n",
      "====> Epoch: 133 Average train loss: 0.0003000370\n",
      "====> Epoch: 134 Average train loss: 0.0003157091\n",
      "====> Epoch: 135 Average train loss: 0.0003057933\n",
      "====> Epoch: 136 Average train loss: 0.0002763771\n",
      "====> Epoch: 137 Average train loss: 0.0003105841\n",
      "====> Epoch: 138 Average train loss: 0.0003296145\n",
      "====> Epoch: 139 Average train loss: 0.0002726475\n",
      "====> Epoch: 140 Average train loss: 0.0003058139\n",
      "====> Epoch: 141 Average train loss: 0.0002974500\n",
      "====> Epoch: 142 Average train loss: 0.0003071167\n",
      "====> Epoch: 143 Average train loss: 0.0002930121\n",
      "====> Epoch: 144 Average train loss: 0.0003233750\n",
      "====> Epoch: 145 Average train loss: 0.0002901076\n",
      "====> Epoch: 146 Average train loss: 0.0002967224\n",
      "====> Epoch: 147 Average train loss: 0.0002831183\n",
      "====> Epoch: 148 Average train loss: 0.0003311401\n",
      "====> Epoch: 149 Average train loss: 0.0002695280\n",
      "====> Epoch: 150 Average train loss: 0.0003201609\n",
      "====> Epoch: 151 Average train loss: 0.0002836296\n",
      "====> Epoch: 152 Average train loss: 0.0002781423\n",
      "====> Epoch: 153 Average train loss: 0.0002747235\n",
      "====> Epoch: 154 Average train loss: 0.0002914638\n",
      "====> Epoch: 155 Average train loss: 0.0003109974\n",
      "====> Epoch: 156 Average train loss: 0.0002972021\n",
      "====> Epoch: 157 Average train loss: 0.0002699201\n",
      "====> Epoch: 158 Average train loss: 0.0003147515\n",
      "====> Epoch: 159 Average train loss: 0.0002662633\n",
      "====> Epoch: 160 Average train loss: 0.0002999791\n",
      "====> Epoch: 161 Average train loss: 0.0002894383\n",
      "====> Epoch: 162 Average train loss: 0.0003340612\n",
      "====> Epoch: 163 Average train loss: 0.0002276360\n",
      "====> Epoch: 164 Average train loss: 0.0002743125\n",
      "====> Epoch: 165 Average train loss: 0.0002786346\n",
      "====> Epoch: 166 Average train loss: 0.0002926989\n",
      "====> Epoch: 167 Average train loss: 0.0002708093\n",
      "====> Epoch: 168 Average train loss: 0.0002732766\n",
      "====> Epoch: 169 Average train loss: 0.0002774124\n",
      "====> Epoch: 170 Average train loss: 0.0002633799\n",
      "====> Epoch: 171 Average train loss: 0.0002703237\n",
      "====> Epoch: 172 Average train loss: 0.0002731228\n",
      "====> Epoch: 173 Average train loss: 0.0002627519\n",
      "====> Epoch: 174 Average train loss: 0.0002732471\n",
      "====> Epoch: 175 Average train loss: 0.0002921792\n",
      "====> Epoch: 176 Average train loss: 0.0002474519\n",
      "====> Epoch: 177 Average train loss: 0.0002824167\n",
      "====> Epoch: 178 Average train loss: 0.0002616459\n",
      "====> Epoch: 179 Average train loss: 0.0002923504\n",
      "====> Epoch: 180 Average train loss: 0.0002535086\n",
      "====> Epoch: 181 Average train loss: 0.0002806999\n",
      "====> Epoch: 182 Average train loss: 0.0002581967\n",
      "====> Epoch: 183 Average train loss: 0.0002565391\n",
      "====> Epoch: 184 Average train loss: 0.0002490672\n",
      "====> Epoch: 185 Average train loss: 0.0002854492\n",
      "====> Epoch: 186 Average train loss: 0.0002662511\n",
      "====> Epoch: 187 Average train loss: 0.0002588160\n",
      "====> Epoch: 188 Average train loss: 0.0002508010\n",
      "====> Epoch: 189 Average train loss: 0.0002313595\n",
      "====> Epoch: 190 Average train loss: 0.0002637452\n",
      "====> Epoch: 191 Average train loss: 0.0002627578\n",
      "====> Epoch: 192 Average train loss: 0.0002695443\n",
      "====> Epoch: 193 Average train loss: 0.0002563116\n",
      "====> Epoch: 194 Average train loss: 0.0002676382\n",
      "====> Epoch: 195 Average train loss: 0.0002766814\n",
      "====> Epoch: 196 Average train loss: 0.0002396897\n",
      "====> Epoch: 197 Average train loss: 0.0002543489\n",
      "====> Epoch: 198 Average train loss: 0.0002687155\n",
      "====> Epoch: 199 Average train loss: 0.0002444202\n",
      "====> Epoch: 200 Average train loss: 0.0002520981\n",
      "====> Epoch: 201 Average train loss: 0.0002868128\n",
      "====> Epoch: 202 Average train loss: 0.0002316246\n",
      "====> Epoch: 203 Average train loss: 0.0002405998\n",
      "====> Epoch: 204 Average train loss: 0.0002616378\n",
      "====> Epoch: 205 Average train loss: 0.0002683343\n",
      "====> Epoch: 206 Average train loss: 0.0002115475\n",
      "====> Epoch: 207 Average train loss: 0.0002464227\n",
      "====> Epoch: 208 Average train loss: 0.0002466885\n",
      "====> Epoch: 209 Average train loss: 0.0002392257\n",
      "====> Epoch: 210 Average train loss: 0.0002758751\n",
      "====> Epoch: 211 Average train loss: 0.0002401276\n",
      "====> Epoch: 212 Average train loss: 0.0002217835\n",
      "====> Epoch: 213 Average train loss: 0.0002592260\n",
      "====> Epoch: 214 Average train loss: 0.0002342271\n",
      "====> Epoch: 215 Average train loss: 0.0002370356\n",
      "====> Epoch: 216 Average train loss: 0.0002381450\n",
      "====> Epoch: 217 Average train loss: 0.0002334112\n",
      "====> Epoch: 218 Average train loss: 0.0002497734\n",
      "====> Epoch: 219 Average train loss: 0.0002438549\n",
      "====> Epoch: 220 Average train loss: 0.0002318165\n",
      "====> Epoch: 221 Average train loss: 0.0002208110\n",
      "====> Epoch: 222 Average train loss: 0.0002316882\n",
      "====> Epoch: 223 Average train loss: 0.0002390477\n",
      "====> Epoch: 224 Average train loss: 0.0002130217\n",
      "====> Epoch: 225 Average train loss: 0.0002403057\n",
      "====> Epoch: 226 Average train loss: 0.0002414512\n",
      "====> Epoch: 227 Average train loss: 0.0002210456\n",
      "====> Epoch: 228 Average train loss: 0.0002359309\n",
      "====> Epoch: 229 Average train loss: 0.0002556520\n",
      "====> Epoch: 230 Average train loss: 0.0002122220\n",
      "====> Epoch: 231 Average train loss: 0.0002254475\n",
      "====> Epoch: 232 Average train loss: 0.0002348199\n",
      "====> Epoch: 233 Average train loss: 0.0002357719\n",
      "====> Epoch: 234 Average train loss: 0.0002163596\n",
      "====> Epoch: 235 Average train loss: 0.0002362068\n",
      "====> Epoch: 236 Average train loss: 0.0002193164\n",
      "====> Epoch: 237 Average train loss: 0.0002321328\n",
      "====> Epoch: 238 Average train loss: 0.0002306011\n",
      "====> Epoch: 239 Average train loss: 0.0002262225\n",
      "====> Epoch: 240 Average train loss: 0.0001927003\n",
      "====> Epoch: 241 Average train loss: 0.0002433544\n",
      "====> Epoch: 242 Average train loss: 0.0002245979\n",
      "====> Epoch: 243 Average train loss: 0.0002227351\n",
      "====> Epoch: 244 Average train loss: 0.0002268163\n",
      "====> Epoch: 245 Average train loss: 0.0002293398\n",
      "====> Epoch: 246 Average train loss: 0.0002218151\n",
      "====> Epoch: 247 Average train loss: 0.0002155293\n",
      "====> Epoch: 248 Average train loss: 0.0002205175\n",
      "====> Epoch: 249 Average train loss: 0.0002350858\n",
      "Best testing error FCNN is 8.332785364473239e-05 and it was found at epoch 185\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train and evaluate the sparsefcnn_model.",
   "id": "9cfbed8187fd361a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T11:19:08.600153Z",
     "start_time": "2024-06-19T11:10:54.552314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reg_coefficient = 0.0001\n",
    "sparsefcnn_model = SparseFCNN(input_dim=obs_dim+act_dim, output_dim=obs_dim, h_dim=h_dim, lambda_coeff=reg_coefficient)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    sparsefcnn_model = sparsefcnn_model.cuda()\n",
    "\n",
    "optimizer_sparsefcnn = torch.optim.Adam([\n",
    "    {'params': sparsefcnn_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_sparsefcnn = train_eval_dynamics_model(sparsefcnn_model, optimizer_sparsefcnn, training_buffer, testing_buffer,\n",
    "                                               batch_size, num_epochs, l0=True)\n",
    "print(\"Best testing error sparse FCNN is {} and it was found at epoch {}\".format(metrics_sparsefcnn[2], metrics_sparsefcnn[3]))"
   ],
   "id": "5f6094f05663b753",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0Dense(4 -> 128, droprate_init=0.5, lamba=0.0, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False)\n",
      "L0Dense(128 -> 128, droprate_init=0.5, lamba=0.0, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False)\n",
      "L0Dense(128 -> 3, droprate_init=0.5, lamba=0.0, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False)\n",
      "====> Epoch: 0 Average train loss: 2.0172623499\n",
      "====> Epoch: 0 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 1 Average train loss: 1.6490040483\n",
      "====> Epoch: 1 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 2 Average train loss: 1.4646513843\n",
      "====> Epoch: 2 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 3 Average train loss: 1.3488714388\n",
      "====> Epoch: 3 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 4 Average train loss: 1.1866263170\n",
      "====> Epoch: 4 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 5 Average train loss: 1.2011892142\n",
      "====> Epoch: 5 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 6 Average train loss: 1.0462794258\n",
      "====> Epoch: 6 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 7 Average train loss: 1.0132716183\n",
      "====> Epoch: 7 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 8 Average train loss: 0.9884374253\n",
      "====> Epoch: 8 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 9 Average train loss: 0.8941413849\n",
      "====> Epoch: 9 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 10 Average train loss: 0.7968509870\n",
      "====> Epoch: 10 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 11 Average train loss: 0.8117280963\n",
      "====> Epoch: 11 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 12 Average train loss: 0.7356888036\n",
      "====> Epoch: 12 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 13 Average train loss: 0.7592872536\n",
      "====> Epoch: 13 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 14 Average train loss: 0.6330738800\n",
      "====> Epoch: 14 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 15 Average train loss: 0.5794371309\n",
      "====> Epoch: 15 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 16 Average train loss: 0.5603479179\n",
      "====> Epoch: 16 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 17 Average train loss: 0.5919657306\n",
      "====> Epoch: 17 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 18 Average train loss: 0.4802967435\n",
      "====> Epoch: 18 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 19 Average train loss: 0.4655548551\n",
      "====> Epoch: 19 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 20 Average train loss: 0.4510742434\n",
      "====> Epoch: 20 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 21 Average train loss: 0.4059491585\n",
      "====> Epoch: 21 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 22 Average train loss: 0.4031114451\n",
      "====> Epoch: 22 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 23 Average train loss: 0.4038046933\n",
      "====> Epoch: 23 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 24 Average train loss: 0.3554401877\n",
      "====> Epoch: 24 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 25 Average train loss: 0.3304425047\n",
      "====> Epoch: 25 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 26 Average train loss: 0.3257963846\n",
      "====> Epoch: 26 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 27 Average train loss: 0.3251338195\n",
      "====> Epoch: 27 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 28 Average train loss: 0.2690739629\n",
      "====> Epoch: 28 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 29 Average train loss: 0.3089273676\n",
      "====> Epoch: 29 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 30 Average train loss: 0.2759271668\n",
      "====> Epoch: 30 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 31 Average train loss: 0.2918633616\n",
      "====> Epoch: 31 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 32 Average train loss: 0.2782174961\n",
      "====> Epoch: 32 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 33 Average train loss: 0.2101562283\n",
      "====> Epoch: 33 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 34 Average train loss: 0.1797301946\n",
      "====> Epoch: 34 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 35 Average train loss: 0.2152463765\n",
      "====> Epoch: 35 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 36 Average train loss: 0.2145714223\n",
      "====> Epoch: 36 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 37 Average train loss: 0.2158023143\n",
      "====> Epoch: 37 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 38 Average train loss: 0.1898144121\n",
      "====> Epoch: 38 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 39 Average train loss: 0.1637749440\n",
      "====> Epoch: 39 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 40 Average train loss: 0.1929947438\n",
      "====> Epoch: 40 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 41 Average train loss: 0.1286055786\n",
      "====> Epoch: 41 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 42 Average train loss: 0.1807516064\n",
      "====> Epoch: 42 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 43 Average train loss: 0.1822382653\n",
      "====> Epoch: 43 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 44 Average train loss: 0.1543154223\n",
      "====> Epoch: 44 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 45 Average train loss: 0.1597093576\n",
      "====> Epoch: 45 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 46 Average train loss: 0.1432590873\n",
      "====> Epoch: 46 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 47 Average train loss: 0.1028547263\n",
      "====> Epoch: 47 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 48 Average train loss: 0.1303186440\n",
      "====> Epoch: 48 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 49 Average train loss: 0.1266992637\n",
      "====> Epoch: 49 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 50 Average train loss: 0.1188061273\n",
      "====> Epoch: 50 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 51 Average train loss: 0.1065459665\n",
      "====> Epoch: 51 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 52 Average train loss: 0.1219134338\n",
      "====> Epoch: 52 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 53 Average train loss: 0.1194742300\n",
      "====> Epoch: 53 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 54 Average train loss: 0.1295109654\n",
      "====> Epoch: 54 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 55 Average train loss: 0.0871203581\n",
      "====> Epoch: 55 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 56 Average train loss: 0.0932012217\n",
      "====> Epoch: 56 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 57 Average train loss: 0.1172428500\n",
      "====> Epoch: 57 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 58 Average train loss: 0.1070264721\n",
      "====> Epoch: 58 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 59 Average train loss: 0.1303029049\n",
      "====> Epoch: 59 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 60 Average train loss: 0.0893290646\n",
      "====> Epoch: 60 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 61 Average train loss: 0.0824759321\n",
      "====> Epoch: 61 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 62 Average train loss: 0.0928447060\n",
      "====> Epoch: 62 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 63 Average train loss: 0.0820339331\n",
      "====> Epoch: 63 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 64 Average train loss: 0.0704513726\n",
      "====> Epoch: 64 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 65 Average train loss: 0.0659806869\n",
      "====> Epoch: 65 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 66 Average train loss: 0.0596346667\n",
      "====> Epoch: 66 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 67 Average train loss: 0.0777191161\n",
      "====> Epoch: 67 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 68 Average train loss: 0.0705616504\n",
      "====> Epoch: 68 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 69 Average train loss: 0.0623987774\n",
      "====> Epoch: 69 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 70 Average train loss: 0.0853612435\n",
      "====> Epoch: 70 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 71 Average train loss: 0.0764575833\n",
      "====> Epoch: 71 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 72 Average train loss: 0.0641624327\n",
      "====> Epoch: 72 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 73 Average train loss: 0.0737397751\n",
      "====> Epoch: 73 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 74 Average train loss: 0.0505556423\n",
      "====> Epoch: 74 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 75 Average train loss: 0.0828200885\n",
      "====> Epoch: 75 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 76 Average train loss: 0.0593174794\n",
      "====> Epoch: 76 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 77 Average train loss: 0.0681212584\n",
      "====> Epoch: 77 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 78 Average train loss: 0.0718078936\n",
      "====> Epoch: 78 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 79 Average train loss: 0.0543303626\n",
      "====> Epoch: 79 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 80 Average train loss: 0.0491660968\n",
      "====> Epoch: 80 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 81 Average train loss: 0.0722384819\n",
      "====> Epoch: 81 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 82 Average train loss: 0.0570748300\n",
      "====> Epoch: 82 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 83 Average train loss: 0.0419871633\n",
      "====> Epoch: 83 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 84 Average train loss: 0.0521050432\n",
      "====> Epoch: 84 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 85 Average train loss: 0.0516792849\n",
      "====> Epoch: 85 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 86 Average train loss: 0.0572821949\n",
      "====> Epoch: 86 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 87 Average train loss: 0.0500569478\n",
      "====> Epoch: 87 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 88 Average train loss: 0.0467310161\n",
      "====> Epoch: 88 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 89 Average train loss: 0.0498226993\n",
      "====> Epoch: 89 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 90 Average train loss: 0.0473064423\n",
      "====> Epoch: 90 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 91 Average train loss: 0.0537431585\n",
      "====> Epoch: 91 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 92 Average train loss: 0.0392443454\n",
      "====> Epoch: 92 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 93 Average train loss: 0.0398086821\n",
      "====> Epoch: 93 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 94 Average train loss: 0.0334058071\n",
      "====> Epoch: 94 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 95 Average train loss: 0.0498248222\n",
      "====> Epoch: 95 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 96 Average train loss: 0.0358431687\n",
      "====> Epoch: 96 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 97 Average train loss: 0.0472413143\n",
      "====> Epoch: 97 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 98 Average train loss: 0.0316115189\n",
      "====> Epoch: 98 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 99 Average train loss: 0.0297282153\n",
      "====> Epoch: 99 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 100 Average train loss: 0.0485813252\n",
      "====> Epoch: 100 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 101 Average train loss: 0.0253515600\n",
      "====> Epoch: 101 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 102 Average train loss: 0.0307310746\n",
      "====> Epoch: 102 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 103 Average train loss: 0.0377984235\n",
      "====> Epoch: 103 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 104 Average train loss: 0.0435141221\n",
      "====> Epoch: 104 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 105 Average train loss: 0.0431711728\n",
      "====> Epoch: 105 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 106 Average train loss: 0.0360538900\n",
      "====> Epoch: 106 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 107 Average train loss: 0.0276865518\n",
      "====> Epoch: 107 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 108 Average train loss: 0.0363890882\n",
      "====> Epoch: 108 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 109 Average train loss: 0.0335838535\n",
      "====> Epoch: 109 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 110 Average train loss: 0.0361127925\n",
      "====> Epoch: 110 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 111 Average train loss: 0.0262850792\n",
      "====> Epoch: 111 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 112 Average train loss: 0.0228961369\n",
      "====> Epoch: 112 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 113 Average train loss: 0.0424494680\n",
      "====> Epoch: 113 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 114 Average train loss: 0.0280472432\n",
      "====> Epoch: 114 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 115 Average train loss: 0.0259943587\n",
      "====> Epoch: 115 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 116 Average train loss: 0.0391465325\n",
      "====> Epoch: 116 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 117 Average train loss: 0.0201756478\n",
      "====> Epoch: 117 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 118 Average train loss: 0.0322450109\n",
      "====> Epoch: 118 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 119 Average train loss: 0.0153475549\n",
      "====> Epoch: 119 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 120 Average train loss: 0.0387813570\n",
      "====> Epoch: 120 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 121 Average train loss: 0.0177358347\n",
      "====> Epoch: 121 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 122 Average train loss: 0.0230528672\n",
      "====> Epoch: 122 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 123 Average train loss: 0.0199208491\n",
      "====> Epoch: 123 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 124 Average train loss: 0.0213170884\n",
      "====> Epoch: 124 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 125 Average train loss: 0.0262842994\n",
      "====> Epoch: 125 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 126 Average train loss: 0.0192563940\n",
      "====> Epoch: 126 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 127 Average train loss: 0.0305556435\n",
      "====> Epoch: 127 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 128 Average train loss: 0.0136440904\n",
      "====> Epoch: 128 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 129 Average train loss: 0.0257806721\n",
      "====> Epoch: 129 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 130 Average train loss: 0.0319096975\n",
      "====> Epoch: 130 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 131 Average train loss: 0.0137912033\n",
      "====> Epoch: 131 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 132 Average train loss: 0.0169478089\n",
      "====> Epoch: 132 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 133 Average train loss: 0.0130418508\n",
      "====> Epoch: 133 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 134 Average train loss: 0.0179174067\n",
      "====> Epoch: 134 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 135 Average train loss: 0.0186284774\n",
      "====> Epoch: 135 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 136 Average train loss: 0.0314637649\n",
      "====> Epoch: 136 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 137 Average train loss: 0.0120701512\n",
      "====> Epoch: 137 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 138 Average train loss: 0.0288912569\n",
      "====> Epoch: 138 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 139 Average train loss: 0.0207587929\n",
      "====> Epoch: 139 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 140 Average train loss: 0.0120474584\n",
      "====> Epoch: 140 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 141 Average train loss: 0.0124773007\n",
      "====> Epoch: 141 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 142 Average train loss: 0.0122944367\n",
      "====> Epoch: 142 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 143 Average train loss: 0.0267667076\n",
      "====> Epoch: 143 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 144 Average train loss: 0.0242107871\n",
      "====> Epoch: 144 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 145 Average train loss: 0.0097630711\n",
      "====> Epoch: 145 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 146 Average train loss: 0.0177413523\n",
      "====> Epoch: 146 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 147 Average train loss: 0.0110422507\n",
      "====> Epoch: 147 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 148 Average train loss: 0.0193872138\n",
      "====> Epoch: 148 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 149 Average train loss: 0.0139241958\n",
      "====> Epoch: 149 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 150 Average train loss: 0.0119985842\n",
      "====> Epoch: 150 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 151 Average train loss: 0.0147801945\n",
      "====> Epoch: 151 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 152 Average train loss: 0.0191884223\n",
      "====> Epoch: 152 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 153 Average train loss: 0.0103235812\n",
      "====> Epoch: 153 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 154 Average train loss: 0.0130448971\n",
      "====> Epoch: 154 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 155 Average train loss: 0.0142953822\n",
      "====> Epoch: 155 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 156 Average train loss: 0.0113530422\n",
      "====> Epoch: 156 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 157 Average train loss: 0.0132477049\n",
      "====> Epoch: 157 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 158 Average train loss: 0.0183242995\n",
      "====> Epoch: 158 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 159 Average train loss: 0.0150395821\n",
      "====> Epoch: 159 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 160 Average train loss: 0.0142699391\n",
      "====> Epoch: 160 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 161 Average train loss: 0.0102541759\n",
      "====> Epoch: 161 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 162 Average train loss: 0.0082316542\n",
      "====> Epoch: 162 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 163 Average train loss: 0.0087776604\n",
      "====> Epoch: 163 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 164 Average train loss: 0.0097772457\n",
      "====> Epoch: 164 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 165 Average train loss: 0.0127568511\n",
      "====> Epoch: 165 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 166 Average train loss: 0.0187315432\n",
      "====> Epoch: 166 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 167 Average train loss: 0.0171374925\n",
      "====> Epoch: 167 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 168 Average train loss: 0.0150164440\n",
      "====> Epoch: 168 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 169 Average train loss: 0.0080175819\n",
      "====> Epoch: 169 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 170 Average train loss: 0.0074381115\n",
      "====> Epoch: 170 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 171 Average train loss: 0.0073567497\n",
      "====> Epoch: 171 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 172 Average train loss: 0.0170974008\n",
      "====> Epoch: 172 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 173 Average train loss: 0.0096460903\n",
      "====> Epoch: 173 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 174 Average train loss: 0.0125412348\n",
      "====> Epoch: 174 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 175 Average train loss: 0.0055394648\n",
      "====> Epoch: 175 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 176 Average train loss: 0.0204482352\n",
      "====> Epoch: 176 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 177 Average train loss: 0.0187469516\n",
      "====> Epoch: 177 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 178 Average train loss: 0.0075058723\n",
      "====> Epoch: 178 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 179 Average train loss: 0.0067292891\n",
      "====> Epoch: 179 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 180 Average train loss: 0.0081490799\n",
      "====> Epoch: 180 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 181 Average train loss: 0.0113294090\n",
      "====> Epoch: 181 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 182 Average train loss: 0.0079915929\n",
      "====> Epoch: 182 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 183 Average train loss: 0.0149259871\n",
      "====> Epoch: 183 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 184 Average train loss: 0.0072860883\n",
      "====> Epoch: 184 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 185 Average train loss: 0.0094882264\n",
      "====> Epoch: 185 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 186 Average train loss: 0.0075365469\n",
      "====> Epoch: 186 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 187 Average train loss: 0.0135356977\n",
      "====> Epoch: 187 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 188 Average train loss: 0.0043093151\n",
      "====> Epoch: 188 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 189 Average train loss: 0.0053041249\n",
      "====> Epoch: 189 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 190 Average train loss: 0.0096254865\n",
      "====> Epoch: 190 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 191 Average train loss: 0.0047685377\n",
      "====> Epoch: 191 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 192 Average train loss: 0.0117899415\n",
      "====> Epoch: 192 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 193 Average train loss: 0.0111724839\n",
      "====> Epoch: 193 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 194 Average train loss: 0.0051703740\n",
      "====> Epoch: 194 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 195 Average train loss: 0.0043849375\n",
      "====> Epoch: 195 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 196 Average train loss: 0.0148535779\n",
      "====> Epoch: 196 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 197 Average train loss: 0.0212544698\n",
      "====> Epoch: 197 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 198 Average train loss: 0.0128747305\n",
      "====> Epoch: 198 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 199 Average train loss: 0.0119354185\n",
      "====> Epoch: 199 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 200 Average train loss: 0.0036769933\n",
      "====> Epoch: 200 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 201 Average train loss: 0.0048934216\n",
      "====> Epoch: 201 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 202 Average train loss: 0.0043173547\n",
      "====> Epoch: 202 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 203 Average train loss: 0.0043124849\n",
      "====> Epoch: 203 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 204 Average train loss: 0.0041846001\n",
      "====> Epoch: 204 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 205 Average train loss: 0.0100597144\n",
      "====> Epoch: 205 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 206 Average train loss: 0.0110347544\n",
      "====> Epoch: 206 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 207 Average train loss: 0.0062394088\n",
      "====> Epoch: 207 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 208 Average train loss: 0.0094026021\n",
      "====> Epoch: 208 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 209 Average train loss: 0.0036411143\n",
      "====> Epoch: 209 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 210 Average train loss: 0.0123319979\n",
      "====> Epoch: 210 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 211 Average train loss: 0.0089975674\n",
      "====> Epoch: 211 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 212 Average train loss: 0.0035125729\n",
      "====> Epoch: 212 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 213 Average train loss: 0.0078804464\n",
      "====> Epoch: 213 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 214 Average train loss: 0.0088682962\n",
      "====> Epoch: 214 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 215 Average train loss: 0.0032765162\n",
      "====> Epoch: 215 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 216 Average train loss: 0.0059768276\n",
      "====> Epoch: 216 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 217 Average train loss: 0.0089866856\n",
      "====> Epoch: 217 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 218 Average train loss: 0.0074108658\n",
      "====> Epoch: 218 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 219 Average train loss: 0.0078525576\n",
      "====> Epoch: 219 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 220 Average train loss: 0.0032761454\n",
      "====> Epoch: 220 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 221 Average train loss: 0.0157658352\n",
      "====> Epoch: 221 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 222 Average train loss: 0.0031301329\n",
      "====> Epoch: 222 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 223 Average train loss: 0.0120555017\n",
      "====> Epoch: 223 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 224 Average train loss: 0.0025769188\n",
      "====> Epoch: 224 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 225 Average train loss: 0.0054880612\n",
      "====> Epoch: 225 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 226 Average train loss: 0.0056557145\n",
      "====> Epoch: 226 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 227 Average train loss: 0.0042479122\n",
      "====> Epoch: 227 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 228 Average train loss: 0.0025098630\n",
      "====> Epoch: 228 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 229 Average train loss: 0.0081911545\n",
      "====> Epoch: 229 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 230 Average train loss: 0.0029623518\n",
      "====> Epoch: 230 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 231 Average train loss: 0.0025971476\n",
      "====> Epoch: 231 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 232 Average train loss: 0.0035042419\n",
      "====> Epoch: 232 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 233 Average train loss: 0.0051495171\n",
      "====> Epoch: 233 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 234 Average train loss: 0.0083949091\n",
      "====> Epoch: 234 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 235 Average train loss: 0.0068981552\n",
      "====> Epoch: 235 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 236 Average train loss: 0.0026004225\n",
      "====> Epoch: 236 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 237 Average train loss: 0.0052376512\n",
      "====> Epoch: 237 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 238 Average train loss: 0.0090575086\n",
      "====> Epoch: 238 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 239 Average train loss: 0.0100559169\n",
      "====> Epoch: 239 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 240 Average train loss: 0.0054596980\n",
      "====> Epoch: 240 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 241 Average train loss: 0.0034568209\n",
      "====> Epoch: 241 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 242 Average train loss: 0.0102619438\n",
      "====> Epoch: 242 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 243 Average train loss: 0.0022599219\n",
      "====> Epoch: 243 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 244 Average train loss: 0.0035772124\n",
      "====> Epoch: 244 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 245 Average train loss: 0.0022448769\n",
      "====> Epoch: 245 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 246 Average train loss: 0.0088764613\n",
      "====> Epoch: 246 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 247 Average train loss: 0.0030937685\n",
      "====> Epoch: 247 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 248 Average train loss: 0.0036951790\n",
      "====> Epoch: 248 Average L0 reg loss: 0.0000000000\n",
      "====> Epoch: 249 Average train loss: 0.0044711042\n",
      "====> Epoch: 249 Average L0 reg loss: 0.0000000000\n",
      "Best testing error sparse FCNN is 0.0007151508470997214 and it was found at epoch 239\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train and evaluate the l0sindy_model.",
   "id": "e73746a15dbf55df"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-19T12:07:27.375771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# degree of the polynomial features\n",
    "degree = 3\n",
    "\n",
    "reg_coefficient = 0.0005\n",
    "l0sindy_model = L0SINDy_dynamics(input_dim=obs_dim+act_dim, output_dim=obs_dim, degree=degree, lambda_coeff=reg_coefficient)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    l0sindy_model = l0sindy_model.cuda()\n",
    "\n",
    "optimizer_fcnn = torch.optim.Adam([\n",
    "    {'params': l0sindy_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_l0sindy = train_eval_dynamics_model(l0sindy_model, optimizer_fcnn, training_buffer, testing_buffer, batch_size, num_epochs, l0=True)\n",
    "print(\"Best testing error L0 SINDy is {} and it was found at epoch {}\".format(metrics_l0sindy[2], metrics_l0sindy[3]))\n",
    "\n",
    "# print the close-form equation of the model. Great for interpretability!!!\n",
    "l0sindy_model.print_equations()"
   ],
   "id": "94717fb5ef5019b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial of order  3\n",
      "with 35 coefficients\n",
      "['1' 'x0' 'x1' 'x2' 'x3' 'x0^2' 'x0 x1' 'x0 x2' 'x0 x3' 'x1^2' 'x1 x2'\n",
      " 'x1 x3' 'x2^2' 'x2 x3' 'x3^2' 'x0^3' 'x0^2 x1' 'x0^2 x2' 'x0^2 x3'\n",
      " 'x0 x1^2' 'x0 x1 x2' 'x0 x1 x3' 'x0 x2^2' 'x0 x2 x3' 'x0 x3^2' 'x1^3'\n",
      " 'x1^2 x2' 'x1^2 x3' 'x1 x2^2' 'x1 x2 x3' 'x1 x3^2' 'x2^3' 'x2^2 x3'\n",
      " 'x2 x3^2' 'x3^3']\n",
      "L0Dense(35 -> 1, droprate_init=0.5, lamba=0.0003, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False, bias=False)\n",
      "L0Dense(35 -> 1, droprate_init=0.5, lamba=0.0003, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False, bias=False)\n",
      "L0Dense(35 -> 1, droprate_init=0.5, lamba=0.0003, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False, bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/Documents/Sparsifying-Parametric-Models-with-L0/utils/l0_layer.py:51: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.weights, mode='fan_out')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average train loss: 507.5071008111\n",
      "====> Epoch: 0 Average L0 reg loss: 0.0261946995\n",
      "====> Epoch: 1 Average train loss: 219.1650790600\n",
      "====> Epoch: 1 Average L0 reg loss: 0.0262014826\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Eventually, we plot the mean-squared error between the predictions and the ground-truth over the training and test set.",
   "id": "90695781b7ab4e92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T11:10:37.387243Z",
     "start_time": "2024-06-19T11:10:37.194726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# creating the plots\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Training and Evaluation Metrics')\n",
    "\n",
    "data_train = {'FCNN': metrics_fcnn[0], 'SparseFCNN': metrics_sparsefcnn[0], 'L0SINDy': metrics_l0sindy[0]}\n",
    "methods_train = list(data_train.keys())\n",
    "values_train = list(data_train.values())\n",
    "\n",
    "# creating the bar plot\n",
    "ax1.bar(methods_train, values_train, color='maroon', width=0.4)\n",
    "\n",
    "data_eval = {'FCNN': metrics_fcnn[2], 'SparseFCNN': metrics_sparsefcnn[2], 'L0SINDy': metrics_l0sindy[2]}\n",
    "methods_eval = list(data_eval.keys())\n",
    "values_eval = list(data_eval.values())\n",
    "\n",
    "ax2.bar(methods_eval, values_eval, color='blue', width=0.4)\n",
    "\n",
    "save_dir = \"figures\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "fig.savefig('figures/LearningDynamics.png', dpi=300)"
   ],
   "id": "5b52985fbff205cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
