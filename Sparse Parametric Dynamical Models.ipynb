{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-23T07:56:57.461955Z",
     "start_time": "2024-05-23T07:56:56.765323Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "from replay_buffer import ReplayBuffer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds\n",
    "seed = 23524\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T07:57:08.346331Z",
     "start_time": "2024-05-23T07:56:57.463251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "render = False\n",
    "if render:\n",
    "    env = gym.make('Pendulum-v1', g=9.81, render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make('Pendulum-v1', g=9.81)\n",
    "max_episodes = 1000\n",
    "max_steps = 200\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "buf_dim = int(max_episodes*max_steps)\n",
    "\n",
    "# create training set\n",
    "seed = 1\n",
    "training_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=buf_dim)\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    observation, info = env.reset()\n",
    "    for steps in range(max_steps+1):\n",
    "        action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        training_buffer.store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "print(\"Finished creating the training set\")"
   ],
   "id": "f30d49f053f6de6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/Documents/Sparsifying-Parametric-Models-with-L0/venv/lib/python3.10/site-packages/gymnasium/envs/classic_control/pendulum.py:173: UserWarning: \u001B[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\u001B[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating the training set\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T07:57:09.359028Z",
     "start_time": "2024-05-23T07:57:08.347445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create test set\n",
    "max_episodes_test = 100\n",
    "buf_dim = int(max_episodes*max_steps)\n",
    "\n",
    "seed = 7\n",
    "testing_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=buf_dim)\n",
    "\n",
    "for episode in range(max_episodes_test):\n",
    "    observation, info = env.reset()\n",
    "    for steps in range(max_steps + 1):\n",
    "        action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        testing_buffer.store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "print(\"Finished creating the test set\")"
   ],
   "id": "5c45b1ee713bfd84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating the test set\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T07:57:09.584363Z",
     "start_time": "2024-05-23T07:57:09.359889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# learning the dynamics of the pendulum\n",
    "from models import FCNN, SparseFCNN, L0SINDy_dynamics\n",
    "from trainer import train_eval_dynamics_model\n",
    "import torch\n",
    "\n",
    "h_dim = 64\n",
    "lr = 3e-4\n",
    "batch_size = 256\n",
    "num_epochs = 250"
   ],
   "id": "5b9d333e476e4824",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T07:59:48.107233Z",
     "start_time": "2024-05-23T07:57:09.585666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fcnn_model = FCNN(input_dim=obs_dim+act_dim, output_dim=obs_dim, h_dim=h_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    fcnn_model = fcnn_model.cuda()\n",
    "\n",
    "optimizer_fcnn = torch.optim.Adam([\n",
    "    {'params': fcnn_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_fcnn = train_eval_dynamics_model(fcnn_model, optimizer_fcnn, training_buffer, testing_buffer, batch_size, num_epochs)\n",
    "print(\"Best testing error FCNN is {} and it was found at epoch {}\".format(metrics_fcnn[2], metrics_fcnn[3]))\n"
   ],
   "id": "2cac37c5b681ba1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average train loss: 0.4135053428\n",
      "====> Epoch: 0 Average eval loss: 0.0131539498\n",
      "====> Epoch: 1 Average train loss: 0.0078189424\n",
      "====> Epoch: 1 Average eval loss: 0.0055474164\n",
      "====> Epoch: 2 Average train loss: 0.0037924307\n",
      "====> Epoch: 2 Average eval loss: 0.0032626847\n",
      "====> Epoch: 3 Average train loss: 0.0023496686\n",
      "====> Epoch: 3 Average eval loss: 0.0021513025\n",
      "====> Epoch: 4 Average train loss: 0.0016391078\n",
      "====> Epoch: 4 Average eval loss: 0.0014863977\n",
      "====> Epoch: 5 Average train loss: 0.0011687140\n",
      "====> Epoch: 5 Average eval loss: 0.0011458916\n",
      "====> Epoch: 6 Average train loss: 0.0009371059\n",
      "====> Epoch: 6 Average eval loss: 0.0008092728\n",
      "====> Epoch: 7 Average train loss: 0.0008021729\n",
      "====> Epoch: 7 Average eval loss: 0.0006587340\n",
      "====> Epoch: 8 Average train loss: 0.0007178263\n",
      "====> Epoch: 8 Average eval loss: 0.0005819441\n",
      "====> Epoch: 9 Average train loss: 0.0006708836\n",
      "====> Epoch: 9 Average eval loss: 0.0005890983\n",
      "====> Epoch: 10 Average train loss: 0.0006516932\n",
      "====> Epoch: 10 Average eval loss: 0.0006956423\n",
      "====> Epoch: 11 Average train loss: 0.0005931322\n",
      "====> Epoch: 11 Average eval loss: 0.0004353674\n",
      "====> Epoch: 12 Average train loss: 0.0005862897\n",
      "====> Epoch: 12 Average eval loss: 0.0005096287\n",
      "====> Epoch: 13 Average train loss: 0.0005737829\n",
      "====> Epoch: 13 Average eval loss: 0.0003838957\n",
      "====> Epoch: 14 Average train loss: 0.0005243423\n",
      "====> Epoch: 14 Average eval loss: 0.0005423760\n",
      "====> Epoch: 15 Average train loss: 0.0005440555\n",
      "====> Epoch: 15 Average eval loss: 0.0004351302\n",
      "====> Epoch: 16 Average train loss: 0.0005041249\n",
      "====> Epoch: 16 Average eval loss: 0.0004898602\n",
      "====> Epoch: 17 Average train loss: 0.0005176846\n",
      "====> Epoch: 17 Average eval loss: 0.0003252306\n",
      "====> Epoch: 18 Average train loss: 0.0005218405\n",
      "====> Epoch: 18 Average eval loss: 0.0005214591\n",
      "====> Epoch: 19 Average train loss: 0.0004814860\n",
      "====> Epoch: 19 Average eval loss: 0.0003408613\n",
      "====> Epoch: 20 Average train loss: 0.0004935153\n",
      "====> Epoch: 20 Average eval loss: 0.0004518686\n",
      "====> Epoch: 21 Average train loss: 0.0004892558\n",
      "====> Epoch: 21 Average eval loss: 0.0003043027\n",
      "====> Epoch: 22 Average train loss: 0.0004533945\n",
      "====> Epoch: 22 Average eval loss: 0.0003624685\n",
      "====> Epoch: 23 Average train loss: 0.0004647197\n",
      "====> Epoch: 23 Average eval loss: 0.0003091702\n",
      "====> Epoch: 24 Average train loss: 0.0004554296\n",
      "====> Epoch: 24 Average eval loss: 0.0003523593\n",
      "====> Epoch: 25 Average train loss: 0.0004620290\n",
      "====> Epoch: 25 Average eval loss: 0.0004160153\n",
      "====> Epoch: 26 Average train loss: 0.0004489533\n",
      "====> Epoch: 26 Average eval loss: 0.0003272691\n",
      "====> Epoch: 27 Average train loss: 0.0004489227\n",
      "====> Epoch: 27 Average eval loss: 0.0003830084\n",
      "====> Epoch: 28 Average train loss: 0.0004312611\n",
      "====> Epoch: 28 Average eval loss: 0.0003085384\n",
      "====> Epoch: 29 Average train loss: 0.0004602060\n",
      "====> Epoch: 29 Average eval loss: 0.0003288290\n",
      "====> Epoch: 30 Average train loss: 0.0004161809\n",
      "====> Epoch: 30 Average eval loss: 0.0003050777\n",
      "====> Epoch: 31 Average train loss: 0.0004238300\n",
      "====> Epoch: 31 Average eval loss: 0.0002887074\n",
      "====> Epoch: 32 Average train loss: 0.0004142755\n",
      "====> Epoch: 32 Average eval loss: 0.0002592747\n",
      "====> Epoch: 33 Average train loss: 0.0004193043\n",
      "====> Epoch: 33 Average eval loss: 0.0003383682\n",
      "====> Epoch: 34 Average train loss: 0.0004298798\n",
      "====> Epoch: 34 Average eval loss: 0.0002614311\n",
      "====> Epoch: 35 Average train loss: 0.0004131046\n",
      "====> Epoch: 35 Average eval loss: 0.0002695865\n",
      "====> Epoch: 36 Average train loss: 0.0004304154\n",
      "====> Epoch: 36 Average eval loss: 0.0003437550\n",
      "====> Epoch: 37 Average train loss: 0.0004221619\n",
      "====> Epoch: 37 Average eval loss: 0.0002840680\n",
      "====> Epoch: 38 Average train loss: 0.0004140961\n",
      "====> Epoch: 38 Average eval loss: 0.0002760849\n",
      "====> Epoch: 39 Average train loss: 0.0004110768\n",
      "====> Epoch: 39 Average eval loss: 0.0002620773\n",
      "====> Epoch: 40 Average train loss: 0.0004007400\n",
      "====> Epoch: 40 Average eval loss: 0.0004993933\n",
      "====> Epoch: 41 Average train loss: 0.0004037283\n",
      "====> Epoch: 41 Average eval loss: 0.0003248588\n",
      "====> Epoch: 42 Average train loss: 0.0003882172\n",
      "====> Epoch: 42 Average eval loss: 0.0003270087\n",
      "====> Epoch: 43 Average train loss: 0.0004056251\n",
      "====> Epoch: 43 Average eval loss: 0.0003240318\n",
      "====> Epoch: 44 Average train loss: 0.0004074731\n",
      "====> Epoch: 44 Average eval loss: 0.0002348155\n",
      "====> Epoch: 45 Average train loss: 0.0003926883\n",
      "====> Epoch: 45 Average eval loss: 0.0003694141\n",
      "====> Epoch: 46 Average train loss: 0.0003987406\n",
      "====> Epoch: 46 Average eval loss: 0.0003429518\n",
      "====> Epoch: 47 Average train loss: 0.0003921094\n",
      "====> Epoch: 47 Average eval loss: 0.0003700055\n",
      "====> Epoch: 48 Average train loss: 0.0004093500\n",
      "====> Epoch: 48 Average eval loss: 0.0002335475\n",
      "====> Epoch: 49 Average train loss: 0.0003889105\n",
      "====> Epoch: 49 Average eval loss: 0.0002698570\n",
      "====> Epoch: 50 Average train loss: 0.0003873810\n",
      "====> Epoch: 50 Average eval loss: 0.0002915642\n",
      "====> Epoch: 51 Average train loss: 0.0003827139\n",
      "====> Epoch: 51 Average eval loss: 0.0004831434\n",
      "====> Epoch: 52 Average train loss: 0.0003971611\n",
      "====> Epoch: 52 Average eval loss: 0.0003892804\n",
      "====> Epoch: 53 Average train loss: 0.0003712072\n",
      "====> Epoch: 53 Average eval loss: 0.0002482484\n",
      "====> Epoch: 54 Average train loss: 0.0003908792\n",
      "====> Epoch: 54 Average eval loss: 0.0002730388\n",
      "====> Epoch: 55 Average train loss: 0.0003787406\n",
      "====> Epoch: 55 Average eval loss: 0.0002459023\n",
      "====> Epoch: 56 Average train loss: 0.0003911607\n",
      "====> Epoch: 56 Average eval loss: 0.0003791891\n",
      "====> Epoch: 57 Average train loss: 0.0003644307\n",
      "====> Epoch: 57 Average eval loss: 0.0004400798\n",
      "====> Epoch: 58 Average train loss: 0.0003852914\n",
      "====> Epoch: 58 Average eval loss: 0.0003068525\n",
      "====> Epoch: 59 Average train loss: 0.0003759587\n",
      "====> Epoch: 59 Average eval loss: 0.0002194033\n",
      "====> Epoch: 60 Average train loss: 0.0003677600\n",
      "====> Epoch: 60 Average eval loss: 0.0002900704\n",
      "====> Epoch: 61 Average train loss: 0.0003875012\n",
      "====> Epoch: 61 Average eval loss: 0.0004541879\n",
      "====> Epoch: 62 Average train loss: 0.0003830194\n",
      "====> Epoch: 62 Average eval loss: 0.0002811460\n",
      "====> Epoch: 63 Average train loss: 0.0003673013\n",
      "====> Epoch: 63 Average eval loss: 0.0003896277\n",
      "====> Epoch: 64 Average train loss: 0.0003696099\n",
      "====> Epoch: 64 Average eval loss: 0.0002979282\n",
      "====> Epoch: 65 Average train loss: 0.0003790387\n",
      "====> Epoch: 65 Average eval loss: 0.0002653348\n",
      "====> Epoch: 66 Average train loss: 0.0003780609\n",
      "====> Epoch: 66 Average eval loss: 0.0002776135\n",
      "====> Epoch: 67 Average train loss: 0.0003799577\n",
      "====> Epoch: 67 Average eval loss: 0.0005577474\n",
      "====> Epoch: 68 Average train loss: 0.0003964718\n",
      "====> Epoch: 68 Average eval loss: 0.0002340042\n",
      "====> Epoch: 69 Average train loss: 0.0003613317\n",
      "====> Epoch: 69 Average eval loss: 0.0002205896\n",
      "====> Epoch: 70 Average train loss: 0.0003640696\n",
      "====> Epoch: 70 Average eval loss: 0.0002938537\n",
      "====> Epoch: 71 Average train loss: 0.0003573386\n",
      "====> Epoch: 71 Average eval loss: 0.0002655441\n",
      "====> Epoch: 72 Average train loss: 0.0003867185\n",
      "====> Epoch: 72 Average eval loss: 0.0003151600\n",
      "====> Epoch: 73 Average train loss: 0.0003610678\n",
      "====> Epoch: 73 Average eval loss: 0.0003740399\n",
      "====> Epoch: 74 Average train loss: 0.0003665456\n",
      "====> Epoch: 74 Average eval loss: 0.0003036878\n",
      "====> Epoch: 75 Average train loss: 0.0003768069\n",
      "====> Epoch: 75 Average eval loss: 0.0002056563\n",
      "====> Epoch: 76 Average train loss: 0.0003639753\n",
      "====> Epoch: 76 Average eval loss: 0.0002718782\n",
      "====> Epoch: 77 Average train loss: 0.0003288091\n",
      "====> Epoch: 77 Average eval loss: 0.0002350544\n",
      "====> Epoch: 78 Average train loss: 0.0003614313\n",
      "====> Epoch: 78 Average eval loss: 0.0002242088\n",
      "====> Epoch: 79 Average train loss: 0.0003702658\n",
      "====> Epoch: 79 Average eval loss: 0.0002657950\n",
      "====> Epoch: 80 Average train loss: 0.0003418649\n",
      "====> Epoch: 80 Average eval loss: 0.0002270598\n",
      "====> Epoch: 81 Average train loss: 0.0003674738\n",
      "====> Epoch: 81 Average eval loss: 0.0002996785\n",
      "====> Epoch: 82 Average train loss: 0.0003405731\n",
      "====> Epoch: 82 Average eval loss: 0.0003567730\n",
      "====> Epoch: 83 Average train loss: 0.0003520380\n",
      "====> Epoch: 83 Average eval loss: 0.0002720448\n",
      "====> Epoch: 84 Average train loss: 0.0003515946\n",
      "====> Epoch: 84 Average eval loss: 0.0002592746\n",
      "====> Epoch: 85 Average train loss: 0.0003714100\n",
      "====> Epoch: 85 Average eval loss: 0.0002625452\n",
      "====> Epoch: 86 Average train loss: 0.0003343661\n",
      "====> Epoch: 86 Average eval loss: 0.0002655577\n",
      "====> Epoch: 87 Average train loss: 0.0003424793\n",
      "====> Epoch: 87 Average eval loss: 0.0003597995\n",
      "====> Epoch: 88 Average train loss: 0.0003501523\n",
      "====> Epoch: 88 Average eval loss: 0.0002028123\n",
      "====> Epoch: 89 Average train loss: 0.0003566009\n",
      "====> Epoch: 89 Average eval loss: 0.0003264177\n",
      "====> Epoch: 90 Average train loss: 0.0003509771\n",
      "====> Epoch: 90 Average eval loss: 0.0002953295\n",
      "====> Epoch: 91 Average train loss: 0.0003476061\n",
      "====> Epoch: 91 Average eval loss: 0.0005170626\n",
      "====> Epoch: 92 Average train loss: 0.0003501007\n",
      "====> Epoch: 92 Average eval loss: 0.0002297738\n",
      "====> Epoch: 93 Average train loss: 0.0003605028\n",
      "====> Epoch: 93 Average eval loss: 0.0002579557\n",
      "====> Epoch: 94 Average train loss: 0.0003370688\n",
      "====> Epoch: 94 Average eval loss: 0.0001941457\n",
      "====> Epoch: 95 Average train loss: 0.0003276475\n",
      "====> Epoch: 95 Average eval loss: 0.0002580649\n",
      "====> Epoch: 96 Average train loss: 0.0003334993\n",
      "====> Epoch: 96 Average eval loss: 0.0003796451\n",
      "====> Epoch: 97 Average train loss: 0.0003590010\n",
      "====> Epoch: 97 Average eval loss: 0.0002603999\n",
      "====> Epoch: 98 Average train loss: 0.0003454358\n",
      "====> Epoch: 98 Average eval loss: 0.0002637805\n",
      "====> Epoch: 99 Average train loss: 0.0003359346\n",
      "====> Epoch: 99 Average eval loss: 0.0003915872\n",
      "====> Epoch: 100 Average train loss: 0.0003441294\n",
      "====> Epoch: 100 Average eval loss: 0.0003316028\n",
      "====> Epoch: 101 Average train loss: 0.0003301512\n",
      "====> Epoch: 101 Average eval loss: 0.0002216638\n",
      "====> Epoch: 102 Average train loss: 0.0003419428\n",
      "====> Epoch: 102 Average eval loss: 0.0002726247\n",
      "====> Epoch: 103 Average train loss: 0.0003353837\n",
      "====> Epoch: 103 Average eval loss: 0.0003054760\n",
      "====> Epoch: 104 Average train loss: 0.0003220328\n",
      "====> Epoch: 104 Average eval loss: 0.0003057891\n",
      "====> Epoch: 105 Average train loss: 0.0003363234\n",
      "====> Epoch: 105 Average eval loss: 0.0002401030\n",
      "====> Epoch: 106 Average train loss: 0.0003261363\n",
      "====> Epoch: 106 Average eval loss: 0.0002219992\n",
      "====> Epoch: 107 Average train loss: 0.0003425908\n",
      "====> Epoch: 107 Average eval loss: 0.0002160010\n",
      "====> Epoch: 108 Average train loss: 0.0003175955\n",
      "====> Epoch: 108 Average eval loss: 0.0001834163\n",
      "====> Epoch: 109 Average train loss: 0.0003172116\n",
      "====> Epoch: 109 Average eval loss: 0.0002077658\n",
      "====> Epoch: 110 Average train loss: 0.0003311498\n",
      "====> Epoch: 110 Average eval loss: 0.0001877985\n",
      "====> Epoch: 111 Average train loss: 0.0003185691\n",
      "====> Epoch: 111 Average eval loss: 0.0002905792\n",
      "====> Epoch: 112 Average train loss: 0.0003146615\n",
      "====> Epoch: 112 Average eval loss: 0.0002716021\n",
      "====> Epoch: 113 Average train loss: 0.0003238555\n",
      "====> Epoch: 113 Average eval loss: 0.0002625065\n",
      "====> Epoch: 114 Average train loss: 0.0003411720\n",
      "====> Epoch: 114 Average eval loss: 0.0002016613\n",
      "====> Epoch: 115 Average train loss: 0.0003213873\n",
      "====> Epoch: 115 Average eval loss: 0.0001956476\n",
      "====> Epoch: 116 Average train loss: 0.0003286853\n",
      "====> Epoch: 116 Average eval loss: 0.0002502483\n",
      "====> Epoch: 117 Average train loss: 0.0003339319\n",
      "====> Epoch: 117 Average eval loss: 0.0002885017\n",
      "====> Epoch: 118 Average train loss: 0.0003169853\n",
      "====> Epoch: 118 Average eval loss: 0.0002773405\n",
      "====> Epoch: 119 Average train loss: 0.0003215544\n",
      "====> Epoch: 119 Average eval loss: 0.0002214760\n",
      "====> Epoch: 120 Average train loss: 0.0003207985\n",
      "====> Epoch: 120 Average eval loss: 0.0003586619\n",
      "====> Epoch: 121 Average train loss: 0.0003179112\n",
      "====> Epoch: 121 Average eval loss: 0.0001991000\n",
      "====> Epoch: 122 Average train loss: 0.0003214307\n",
      "====> Epoch: 122 Average eval loss: 0.0002029297\n",
      "====> Epoch: 123 Average train loss: 0.0003330338\n",
      "====> Epoch: 123 Average eval loss: 0.0002189577\n",
      "====> Epoch: 124 Average train loss: 0.0003105183\n",
      "====> Epoch: 124 Average eval loss: 0.0002163270\n",
      "====> Epoch: 125 Average train loss: 0.0003314335\n",
      "====> Epoch: 125 Average eval loss: 0.0001797037\n",
      "====> Epoch: 126 Average train loss: 0.0002989174\n",
      "====> Epoch: 126 Average eval loss: 0.0001860016\n",
      "====> Epoch: 127 Average train loss: 0.0002938880\n",
      "====> Epoch: 127 Average eval loss: 0.0002289286\n",
      "====> Epoch: 128 Average train loss: 0.0003224412\n",
      "====> Epoch: 128 Average eval loss: 0.0002340610\n",
      "====> Epoch: 129 Average train loss: 0.0003104653\n",
      "====> Epoch: 129 Average eval loss: 0.0002513591\n",
      "====> Epoch: 130 Average train loss: 0.0003211539\n",
      "====> Epoch: 130 Average eval loss: 0.0002085710\n",
      "====> Epoch: 131 Average train loss: 0.0003085149\n",
      "====> Epoch: 131 Average eval loss: 0.0002142128\n",
      "====> Epoch: 132 Average train loss: 0.0003071634\n",
      "====> Epoch: 132 Average eval loss: 0.0002442261\n",
      "====> Epoch: 133 Average train loss: 0.0003142989\n",
      "====> Epoch: 133 Average eval loss: 0.0001827585\n",
      "====> Epoch: 134 Average train loss: 0.0003117403\n",
      "====> Epoch: 134 Average eval loss: 0.0002374416\n",
      "====> Epoch: 135 Average train loss: 0.0003141912\n",
      "====> Epoch: 135 Average eval loss: 0.0001877568\n",
      "====> Epoch: 136 Average train loss: 0.0003068512\n",
      "====> Epoch: 136 Average eval loss: 0.0002850186\n",
      "====> Epoch: 137 Average train loss: 0.0003047991\n",
      "====> Epoch: 137 Average eval loss: 0.0002271045\n",
      "====> Epoch: 138 Average train loss: 0.0003178127\n",
      "====> Epoch: 138 Average eval loss: 0.0002385647\n",
      "====> Epoch: 139 Average train loss: 0.0003162639\n",
      "====> Epoch: 139 Average eval loss: 0.0002957354\n",
      "====> Epoch: 140 Average train loss: 0.0002997015\n",
      "====> Epoch: 140 Average eval loss: 0.0002864146\n",
      "====> Epoch: 141 Average train loss: 0.0003034111\n",
      "====> Epoch: 141 Average eval loss: 0.0002006550\n",
      "====> Epoch: 142 Average train loss: 0.0002885756\n",
      "====> Epoch: 142 Average eval loss: 0.0002698748\n",
      "====> Epoch: 143 Average train loss: 0.0003209188\n",
      "====> Epoch: 143 Average eval loss: 0.0002916415\n",
      "====> Epoch: 144 Average train loss: 0.0003131552\n",
      "====> Epoch: 144 Average eval loss: 0.0002308230\n",
      "====> Epoch: 145 Average train loss: 0.0002986141\n",
      "====> Epoch: 145 Average eval loss: 0.0002272906\n",
      "====> Epoch: 146 Average train loss: 0.0002939180\n",
      "====> Epoch: 146 Average eval loss: 0.0002344457\n",
      "====> Epoch: 147 Average train loss: 0.0002957353\n",
      "====> Epoch: 147 Average eval loss: 0.0001857274\n",
      "====> Epoch: 148 Average train loss: 0.0003063282\n",
      "====> Epoch: 148 Average eval loss: 0.0002092143\n",
      "====> Epoch: 149 Average train loss: 0.0002919532\n",
      "====> Epoch: 149 Average eval loss: 0.0001629534\n",
      "====> Epoch: 150 Average train loss: 0.0003102953\n",
      "====> Epoch: 150 Average eval loss: 0.0001921706\n",
      "====> Epoch: 151 Average train loss: 0.0002979495\n",
      "====> Epoch: 151 Average eval loss: 0.0002731101\n",
      "====> Epoch: 152 Average train loss: 0.0002995363\n",
      "====> Epoch: 152 Average eval loss: 0.0002176635\n",
      "====> Epoch: 153 Average train loss: 0.0002926302\n",
      "====> Epoch: 153 Average eval loss: 0.0002168126\n",
      "====> Epoch: 154 Average train loss: 0.0003255663\n",
      "====> Epoch: 154 Average eval loss: 0.0002739143\n",
      "====> Epoch: 155 Average train loss: 0.0002843881\n",
      "====> Epoch: 155 Average eval loss: 0.0002356368\n",
      "====> Epoch: 156 Average train loss: 0.0003087367\n",
      "====> Epoch: 156 Average eval loss: 0.0002143825\n",
      "====> Epoch: 157 Average train loss: 0.0002947897\n",
      "====> Epoch: 157 Average eval loss: 0.0002372008\n",
      "====> Epoch: 158 Average train loss: 0.0002977999\n",
      "====> Epoch: 158 Average eval loss: 0.0001805311\n",
      "====> Epoch: 159 Average train loss: 0.0002967368\n",
      "====> Epoch: 159 Average eval loss: 0.0001953561\n",
      "====> Epoch: 160 Average train loss: 0.0002988942\n",
      "====> Epoch: 160 Average eval loss: 0.0001919450\n",
      "====> Epoch: 161 Average train loss: 0.0002916146\n",
      "====> Epoch: 161 Average eval loss: 0.0002587074\n",
      "====> Epoch: 162 Average train loss: 0.0003104122\n",
      "====> Epoch: 162 Average eval loss: 0.0002199427\n",
      "====> Epoch: 163 Average train loss: 0.0002893663\n",
      "====> Epoch: 163 Average eval loss: 0.0002413974\n",
      "====> Epoch: 164 Average train loss: 0.0002874601\n",
      "====> Epoch: 164 Average eval loss: 0.0002891963\n",
      "====> Epoch: 165 Average train loss: 0.0002899570\n",
      "====> Epoch: 165 Average eval loss: 0.0001903996\n",
      "====> Epoch: 166 Average train loss: 0.0003011814\n",
      "====> Epoch: 166 Average eval loss: 0.0002347444\n",
      "====> Epoch: 167 Average train loss: 0.0002949058\n",
      "====> Epoch: 167 Average eval loss: 0.0002391038\n",
      "====> Epoch: 168 Average train loss: 0.0002886597\n",
      "====> Epoch: 168 Average eval loss: 0.0002167036\n",
      "====> Epoch: 169 Average train loss: 0.0002998828\n",
      "====> Epoch: 169 Average eval loss: 0.0003087886\n",
      "====> Epoch: 170 Average train loss: 0.0002895467\n",
      "====> Epoch: 170 Average eval loss: 0.0001822052\n",
      "====> Epoch: 171 Average train loss: 0.0002894377\n",
      "====> Epoch: 171 Average eval loss: 0.0002247324\n",
      "====> Epoch: 172 Average train loss: 0.0002890856\n",
      "====> Epoch: 172 Average eval loss: 0.0003767725\n",
      "====> Epoch: 173 Average train loss: 0.0002932586\n",
      "====> Epoch: 173 Average eval loss: 0.0003239808\n",
      "====> Epoch: 174 Average train loss: 0.0002846892\n",
      "====> Epoch: 174 Average eval loss: 0.0001880563\n",
      "====> Epoch: 175 Average train loss: 0.0002904258\n",
      "====> Epoch: 175 Average eval loss: 0.0001989057\n",
      "====> Epoch: 176 Average train loss: 0.0002852083\n",
      "====> Epoch: 176 Average eval loss: 0.0001765149\n",
      "====> Epoch: 177 Average train loss: 0.0002839507\n",
      "====> Epoch: 177 Average eval loss: 0.0001936668\n",
      "====> Epoch: 178 Average train loss: 0.0002853759\n",
      "====> Epoch: 178 Average eval loss: 0.0001857985\n",
      "====> Epoch: 179 Average train loss: 0.0002881113\n",
      "====> Epoch: 179 Average eval loss: 0.0002751200\n",
      "====> Epoch: 180 Average train loss: 0.0002799280\n",
      "====> Epoch: 180 Average eval loss: 0.0002124303\n",
      "====> Epoch: 181 Average train loss: 0.0002886271\n",
      "====> Epoch: 181 Average eval loss: 0.0002383121\n",
      "====> Epoch: 182 Average train loss: 0.0002816198\n",
      "====> Epoch: 182 Average eval loss: 0.0004242628\n",
      "====> Epoch: 183 Average train loss: 0.0002960855\n",
      "====> Epoch: 183 Average eval loss: 0.0003010897\n",
      "====> Epoch: 184 Average train loss: 0.0002897033\n",
      "====> Epoch: 184 Average eval loss: 0.0001803327\n",
      "====> Epoch: 185 Average train loss: 0.0002865978\n",
      "====> Epoch: 185 Average eval loss: 0.0002478020\n",
      "====> Epoch: 186 Average train loss: 0.0003065442\n",
      "====> Epoch: 186 Average eval loss: 0.0003118511\n",
      "====> Epoch: 187 Average train loss: 0.0002739493\n",
      "====> Epoch: 187 Average eval loss: 0.0001840826\n",
      "====> Epoch: 188 Average train loss: 0.0002795868\n",
      "====> Epoch: 188 Average eval loss: 0.0001891512\n",
      "====> Epoch: 189 Average train loss: 0.0002776635\n",
      "====> Epoch: 189 Average eval loss: 0.0002818889\n",
      "====> Epoch: 190 Average train loss: 0.0002783631\n",
      "====> Epoch: 190 Average eval loss: 0.0001740490\n",
      "====> Epoch: 191 Average train loss: 0.0002800705\n",
      "====> Epoch: 191 Average eval loss: 0.0001794813\n",
      "====> Epoch: 192 Average train loss: 0.0002799130\n",
      "====> Epoch: 192 Average eval loss: 0.0001994178\n",
      "====> Epoch: 193 Average train loss: 0.0002904971\n",
      "====> Epoch: 193 Average eval loss: 0.0002223836\n",
      "====> Epoch: 194 Average train loss: 0.0002813754\n",
      "====> Epoch: 194 Average eval loss: 0.0001777975\n",
      "====> Epoch: 195 Average train loss: 0.0002802533\n",
      "====> Epoch: 195 Average eval loss: 0.0003451114\n",
      "====> Epoch: 196 Average train loss: 0.0002588148\n",
      "====> Epoch: 196 Average eval loss: 0.0002001507\n",
      "====> Epoch: 197 Average train loss: 0.0002601900\n",
      "====> Epoch: 197 Average eval loss: 0.0001834970\n",
      "====> Epoch: 198 Average train loss: 0.0002912137\n",
      "====> Epoch: 198 Average eval loss: 0.0001859571\n",
      "====> Epoch: 199 Average train loss: 0.0002672335\n",
      "====> Epoch: 199 Average eval loss: 0.0002617108\n",
      "====> Epoch: 200 Average train loss: 0.0002816361\n",
      "====> Epoch: 200 Average eval loss: 0.0002197818\n",
      "====> Epoch: 201 Average train loss: 0.0002799397\n",
      "====> Epoch: 201 Average eval loss: 0.0002212502\n",
      "====> Epoch: 202 Average train loss: 0.0002763908\n",
      "====> Epoch: 202 Average eval loss: 0.0001817355\n",
      "====> Epoch: 203 Average train loss: 0.0002778688\n",
      "====> Epoch: 203 Average eval loss: 0.0001814191\n",
      "====> Epoch: 204 Average train loss: 0.0002699412\n",
      "====> Epoch: 204 Average eval loss: 0.0002066962\n",
      "====> Epoch: 205 Average train loss: 0.0002719677\n",
      "====> Epoch: 205 Average eval loss: 0.0001916454\n",
      "====> Epoch: 206 Average train loss: 0.0002763087\n",
      "====> Epoch: 206 Average eval loss: 0.0001922258\n",
      "====> Epoch: 207 Average train loss: 0.0002701947\n",
      "====> Epoch: 207 Average eval loss: 0.0003102744\n",
      "====> Epoch: 208 Average train loss: 0.0002782979\n",
      "====> Epoch: 208 Average eval loss: 0.0001567623\n",
      "====> Epoch: 209 Average train loss: 0.0002760080\n",
      "====> Epoch: 209 Average eval loss: 0.0002592077\n",
      "====> Epoch: 210 Average train loss: 0.0002684178\n",
      "====> Epoch: 210 Average eval loss: 0.0002148223\n",
      "====> Epoch: 211 Average train loss: 0.0002657530\n",
      "====> Epoch: 211 Average eval loss: 0.0001584943\n",
      "====> Epoch: 212 Average train loss: 0.0002705409\n",
      "====> Epoch: 212 Average eval loss: 0.0001528539\n",
      "====> Epoch: 213 Average train loss: 0.0002761937\n",
      "====> Epoch: 213 Average eval loss: 0.0001536857\n",
      "====> Epoch: 214 Average train loss: 0.0002710661\n",
      "====> Epoch: 214 Average eval loss: 0.0001983592\n",
      "====> Epoch: 215 Average train loss: 0.0002620271\n",
      "====> Epoch: 215 Average eval loss: 0.0001888752\n",
      "====> Epoch: 216 Average train loss: 0.0002687449\n",
      "====> Epoch: 216 Average eval loss: 0.0002384101\n",
      "====> Epoch: 217 Average train loss: 0.0002756376\n",
      "====> Epoch: 217 Average eval loss: 0.0002270246\n",
      "====> Epoch: 218 Average train loss: 0.0002738927\n",
      "====> Epoch: 218 Average eval loss: 0.0002059171\n",
      "====> Epoch: 219 Average train loss: 0.0002623201\n",
      "====> Epoch: 219 Average eval loss: 0.0002924190\n",
      "====> Epoch: 220 Average train loss: 0.0002692757\n",
      "====> Epoch: 220 Average eval loss: 0.0001562354\n",
      "====> Epoch: 221 Average train loss: 0.0002720209\n",
      "====> Epoch: 221 Average eval loss: 0.0002025966\n",
      "====> Epoch: 222 Average train loss: 0.0002697644\n",
      "====> Epoch: 222 Average eval loss: 0.0003151271\n",
      "====> Epoch: 223 Average train loss: 0.0002670562\n",
      "====> Epoch: 223 Average eval loss: 0.0002736047\n",
      "====> Epoch: 224 Average train loss: 0.0002763779\n",
      "====> Epoch: 224 Average eval loss: 0.0002406024\n",
      "====> Epoch: 225 Average train loss: 0.0002730356\n",
      "====> Epoch: 225 Average eval loss: 0.0001801850\n",
      "====> Epoch: 226 Average train loss: 0.0002696947\n",
      "====> Epoch: 226 Average eval loss: 0.0002278131\n",
      "====> Epoch: 227 Average train loss: 0.0002663641\n",
      "====> Epoch: 227 Average eval loss: 0.0001494031\n",
      "====> Epoch: 228 Average train loss: 0.0002651309\n",
      "====> Epoch: 228 Average eval loss: 0.0002179843\n",
      "====> Epoch: 229 Average train loss: 0.0002724523\n",
      "====> Epoch: 229 Average eval loss: 0.0001762007\n",
      "====> Epoch: 230 Average train loss: 0.0002576532\n",
      "====> Epoch: 230 Average eval loss: 0.0001520812\n",
      "====> Epoch: 231 Average train loss: 0.0002702409\n",
      "====> Epoch: 231 Average eval loss: 0.0001782477\n",
      "====> Epoch: 232 Average train loss: 0.0002634753\n",
      "====> Epoch: 232 Average eval loss: 0.0002407104\n",
      "====> Epoch: 233 Average train loss: 0.0002672647\n",
      "====> Epoch: 233 Average eval loss: 0.0001913812\n",
      "====> Epoch: 234 Average train loss: 0.0002566532\n",
      "====> Epoch: 234 Average eval loss: 0.0001931685\n",
      "====> Epoch: 235 Average train loss: 0.0002703057\n",
      "====> Epoch: 235 Average eval loss: 0.0002234225\n",
      "====> Epoch: 236 Average train loss: 0.0002675407\n",
      "====> Epoch: 236 Average eval loss: 0.0002530316\n",
      "====> Epoch: 237 Average train loss: 0.0002594419\n",
      "====> Epoch: 237 Average eval loss: 0.0002795455\n",
      "====> Epoch: 238 Average train loss: 0.0002624224\n",
      "====> Epoch: 238 Average eval loss: 0.0002301897\n",
      "====> Epoch: 239 Average train loss: 0.0002554051\n",
      "====> Epoch: 239 Average eval loss: 0.0002456353\n",
      "====> Epoch: 240 Average train loss: 0.0002626120\n",
      "====> Epoch: 240 Average eval loss: 0.0001914732\n",
      "====> Epoch: 241 Average train loss: 0.0002519115\n",
      "====> Epoch: 241 Average eval loss: 0.0001906112\n",
      "====> Epoch: 242 Average train loss: 0.0002690039\n",
      "====> Epoch: 242 Average eval loss: 0.0001565521\n",
      "====> Epoch: 243 Average train loss: 0.0002562638\n",
      "====> Epoch: 243 Average eval loss: 0.0001684483\n",
      "====> Epoch: 244 Average train loss: 0.0002592786\n",
      "====> Epoch: 244 Average eval loss: 0.0001857458\n",
      "====> Epoch: 245 Average train loss: 0.0002658481\n",
      "====> Epoch: 245 Average eval loss: 0.0001883454\n",
      "====> Epoch: 246 Average train loss: 0.0002642350\n",
      "====> Epoch: 246 Average eval loss: 0.0001731325\n",
      "====> Epoch: 247 Average train loss: 0.0002611660\n",
      "====> Epoch: 247 Average eval loss: 0.0001713484\n",
      "====> Epoch: 248 Average train loss: 0.0002634421\n",
      "====> Epoch: 248 Average eval loss: 0.0001666920\n",
      "====> Epoch: 249 Average train loss: 0.0002670496\n",
      "====> Epoch: 249 Average eval loss: 0.0002195981\n",
      "Best testing error FCNN is 0.000149403145769611 and it was found at epoch 227\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:08:18.479864Z",
     "start_time": "2024-05-23T07:59:48.108113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reg_coefficient = 0.0001\n",
    "sparsefcnn_model = SparseFCNN(input_dim=obs_dim+act_dim, output_dim=obs_dim, h_dim=h_dim, lambda_coeff=reg_coefficient)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    sparsefcnn_model = sparsefcnn_model.cuda()\n",
    "\n",
    "optimizer_sparsefcnn = torch.optim.Adam([\n",
    "    {'params': sparsefcnn_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_sparsefcnn = train_eval_dynamics_model(sparsefcnn_model, optimizer_sparsefcnn, training_buffer, testing_buffer,\n",
    "                                               batch_size, num_epochs, l0=True)\n",
    "print(\"Best testing error sparse FCNN is {} and it was found at epoch {}\".format(metrics_sparsefcnn[2], metrics_sparsefcnn[3]))\n"
   ],
   "id": "5f6094f05663b753",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0Dense(4 -> 64, droprate_init=0.5, lamba=0.0005, temperature=0.6666666666666666, weight_decay=0.0, local_rep=True)\n",
      "L0Dense(64 -> 64, droprate_init=0.5, lamba=0.0005, temperature=0.6666666666666666, weight_decay=0.0, local_rep=True)\n",
      "L0Dense(64 -> 3, droprate_init=0.5, lamba=0.0005, temperature=0.6666666666666666, weight_decay=0.0, local_rep=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/Documents/Sparsifying-Parametric-Models-with-L0/l0_layer.py:51: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.weights, mode='fan_out')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average train loss: 2.8459006229\n",
      "====> Epoch: 0 Average L0 reg loss: 1.8826638904\n",
      "====> Epoch: 0 Average eval loss: 0.3199042380\n",
      "====> Epoch: 1 Average train loss: 2.0034541777\n",
      "====> Epoch: 1 Average L0 reg loss: 1.8544191461\n",
      "====> Epoch: 1 Average eval loss: 0.2490444332\n",
      "====> Epoch: 2 Average train loss: 1.7002191493\n",
      "====> Epoch: 2 Average L0 reg loss: 1.8147246961\n",
      "====> Epoch: 2 Average eval loss: 0.2281710058\n",
      "====> Epoch: 3 Average train loss: 1.4717888113\n",
      "====> Epoch: 3 Average L0 reg loss: 1.7624580547\n",
      "====> Epoch: 3 Average eval loss: 0.2255056053\n",
      "====> Epoch: 4 Average train loss: 1.3076768141\n",
      "====> Epoch: 4 Average L0 reg loss: 1.6973882262\n",
      "====> Epoch: 4 Average eval loss: 0.2363859415\n",
      "====> Epoch: 5 Average train loss: 1.1589949448\n",
      "====> Epoch: 5 Average L0 reg loss: 1.6213837806\n",
      "====> Epoch: 5 Average eval loss: 0.2387410849\n",
      "====> Epoch: 6 Average train loss: 1.0325635542\n",
      "====> Epoch: 6 Average L0 reg loss: 1.5376805026\n",
      "====> Epoch: 6 Average eval loss: 0.2534143031\n",
      "====> Epoch: 7 Average train loss: 0.9442417461\n",
      "====> Epoch: 7 Average L0 reg loss: 1.4488500602\n",
      "====> Epoch: 7 Average eval loss: 0.2806915939\n",
      "====> Epoch: 8 Average train loss: 0.8462314259\n",
      "====> Epoch: 8 Average L0 reg loss: 1.3584911297\n",
      "====> Epoch: 8 Average eval loss: 0.3113417327\n",
      "====> Epoch: 9 Average train loss: 0.7706615815\n",
      "====> Epoch: 9 Average L0 reg loss: 1.2687399177\n",
      "====> Epoch: 9 Average eval loss: 0.3423793018\n",
      "====> Epoch: 10 Average train loss: 0.7107292003\n",
      "====> Epoch: 10 Average L0 reg loss: 1.1818687201\n",
      "====> Epoch: 10 Average eval loss: 0.3564748764\n",
      "====> Epoch: 11 Average train loss: 0.6413845318\n",
      "====> Epoch: 11 Average L0 reg loss: 1.0988390269\n",
      "====> Epoch: 11 Average eval loss: 0.3357861936\n",
      "====> Epoch: 12 Average train loss: 0.5773811952\n",
      "====> Epoch: 12 Average L0 reg loss: 1.0203621491\n",
      "====> Epoch: 12 Average eval loss: 0.2901931703\n",
      "====> Epoch: 13 Average train loss: 0.5156759276\n",
      "====> Epoch: 13 Average L0 reg loss: 0.9465555320\n",
      "====> Epoch: 13 Average eval loss: 0.2194707841\n",
      "====> Epoch: 14 Average train loss: 0.4516464918\n",
      "====> Epoch: 14 Average L0 reg loss: 0.8781927231\n",
      "====> Epoch: 14 Average eval loss: 0.1803522855\n",
      "====> Epoch: 15 Average train loss: 0.4003491135\n",
      "====> Epoch: 15 Average L0 reg loss: 0.8164192392\n",
      "====> Epoch: 15 Average eval loss: 0.1592807323\n",
      "====> Epoch: 16 Average train loss: 0.3493096138\n",
      "====> Epoch: 16 Average L0 reg loss: 0.7616372522\n",
      "====> Epoch: 16 Average eval loss: 0.1539092809\n",
      "====> Epoch: 17 Average train loss: 0.3022687034\n",
      "====> Epoch: 17 Average L0 reg loss: 0.7138326219\n",
      "====> Epoch: 17 Average eval loss: 0.1683986634\n",
      "====> Epoch: 18 Average train loss: 0.2688072132\n",
      "====> Epoch: 18 Average L0 reg loss: 0.6725450228\n",
      "====> Epoch: 18 Average eval loss: 0.1617624462\n",
      "====> Epoch: 19 Average train loss: 0.2326423611\n",
      "====> Epoch: 19 Average L0 reg loss: 0.6372472084\n",
      "====> Epoch: 19 Average eval loss: 0.1749835014\n",
      "====> Epoch: 20 Average train loss: 0.1965213880\n",
      "====> Epoch: 20 Average L0 reg loss: 0.6070350542\n",
      "====> Epoch: 20 Average eval loss: 0.1615349352\n",
      "====> Epoch: 21 Average train loss: 0.1754658209\n",
      "====> Epoch: 21 Average L0 reg loss: 0.5813768543\n",
      "====> Epoch: 21 Average eval loss: 0.1529257149\n",
      "====> Epoch: 22 Average train loss: 0.1523883387\n",
      "====> Epoch: 22 Average L0 reg loss: 0.5595730806\n",
      "====> Epoch: 22 Average eval loss: 0.1600364894\n",
      "====> Epoch: 23 Average train loss: 0.1348445746\n",
      "====> Epoch: 23 Average L0 reg loss: 0.5409801450\n",
      "====> Epoch: 23 Average eval loss: 0.1411881000\n",
      "====> Epoch: 24 Average train loss: 0.1183323877\n",
      "====> Epoch: 24 Average L0 reg loss: 0.5250525166\n",
      "====> Epoch: 24 Average eval loss: 0.1241029799\n",
      "====> Epoch: 25 Average train loss: 0.1056260154\n",
      "====> Epoch: 25 Average L0 reg loss: 0.5112467489\n",
      "====> Epoch: 25 Average eval loss: 0.1175835058\n",
      "====> Epoch: 26 Average train loss: 0.0916111196\n",
      "====> Epoch: 26 Average L0 reg loss: 0.4991535753\n",
      "====> Epoch: 26 Average eval loss: 0.1074655876\n",
      "====> Epoch: 27 Average train loss: 0.0830611098\n",
      "====> Epoch: 27 Average L0 reg loss: 0.4885352700\n",
      "====> Epoch: 27 Average eval loss: 0.0932366773\n",
      "====> Epoch: 28 Average train loss: 0.0723690597\n",
      "====> Epoch: 28 Average L0 reg loss: 0.4791029989\n",
      "====> Epoch: 28 Average eval loss: 0.0877748430\n",
      "====> Epoch: 29 Average train loss: 0.0647830226\n",
      "====> Epoch: 29 Average L0 reg loss: 0.4706878440\n",
      "====> Epoch: 29 Average eval loss: 0.0721193403\n",
      "====> Epoch: 30 Average train loss: 0.0565562431\n",
      "====> Epoch: 30 Average L0 reg loss: 0.4631388981\n",
      "====> Epoch: 30 Average eval loss: 0.0691457540\n",
      "====> Epoch: 31 Average train loss: 0.0515750390\n",
      "====> Epoch: 31 Average L0 reg loss: 0.4563461977\n",
      "====> Epoch: 31 Average eval loss: 0.0478479080\n",
      "====> Epoch: 32 Average train loss: 0.0476241879\n",
      "====> Epoch: 32 Average L0 reg loss: 0.4502218060\n",
      "====> Epoch: 32 Average eval loss: 0.0461724214\n",
      "====> Epoch: 33 Average train loss: 0.0407407458\n",
      "====> Epoch: 33 Average L0 reg loss: 0.4447164319\n",
      "====> Epoch: 33 Average eval loss: 0.0404412113\n",
      "====> Epoch: 34 Average train loss: 0.0371699430\n",
      "====> Epoch: 34 Average L0 reg loss: 0.4397809212\n",
      "====> Epoch: 34 Average eval loss: 0.0340949297\n",
      "====> Epoch: 35 Average train loss: 0.0334923557\n",
      "====> Epoch: 35 Average L0 reg loss: 0.4353614239\n",
      "====> Epoch: 35 Average eval loss: 0.0308031496\n",
      "====> Epoch: 36 Average train loss: 0.0324145246\n",
      "====> Epoch: 36 Average L0 reg loss: 0.4314255147\n",
      "====> Epoch: 36 Average eval loss: 0.0240978859\n",
      "====> Epoch: 37 Average train loss: 0.0309912180\n",
      "====> Epoch: 37 Average L0 reg loss: 0.4279306723\n",
      "====> Epoch: 37 Average eval loss: 0.0247886870\n",
      "====> Epoch: 38 Average train loss: 0.0286429989\n",
      "====> Epoch: 38 Average L0 reg loss: 0.4248251826\n",
      "====> Epoch: 38 Average eval loss: 0.0247947425\n",
      "====> Epoch: 39 Average train loss: 0.0252079441\n",
      "====> Epoch: 39 Average L0 reg loss: 0.4220528439\n",
      "====> Epoch: 39 Average eval loss: 0.0177641269\n",
      "====> Epoch: 40 Average train loss: 0.0245163030\n",
      "====> Epoch: 40 Average L0 reg loss: 0.4195579910\n",
      "====> Epoch: 40 Average eval loss: 0.0176463984\n",
      "====> Epoch: 41 Average train loss: 0.0231722112\n",
      "====> Epoch: 41 Average L0 reg loss: 0.4172898138\n",
      "====> Epoch: 41 Average eval loss: 0.0193568673\n",
      "====> Epoch: 42 Average train loss: 0.0225271516\n",
      "====> Epoch: 42 Average L0 reg loss: 0.4151859768\n",
      "====> Epoch: 42 Average eval loss: 0.0152875176\n",
      "====> Epoch: 43 Average train loss: 0.0205139483\n",
      "====> Epoch: 43 Average L0 reg loss: 0.4132285447\n",
      "====> Epoch: 43 Average eval loss: 0.0153574282\n",
      "====> Epoch: 44 Average train loss: 0.0198292909\n",
      "====> Epoch: 44 Average L0 reg loss: 0.4113345899\n",
      "====> Epoch: 44 Average eval loss: 0.0139071988\n",
      "====> Epoch: 45 Average train loss: 0.0193609837\n",
      "====> Epoch: 45 Average L0 reg loss: 0.4094536519\n",
      "====> Epoch: 45 Average eval loss: 0.0141103966\n",
      "====> Epoch: 46 Average train loss: 0.0185620595\n",
      "====> Epoch: 46 Average L0 reg loss: 0.4075382044\n",
      "====> Epoch: 46 Average eval loss: 0.0125020938\n",
      "====> Epoch: 47 Average train loss: 0.0177715100\n",
      "====> Epoch: 47 Average L0 reg loss: 0.4055485364\n",
      "====> Epoch: 47 Average eval loss: 0.0113425916\n",
      "====> Epoch: 48 Average train loss: 0.0179061209\n",
      "====> Epoch: 48 Average L0 reg loss: 0.4034505462\n",
      "====> Epoch: 48 Average eval loss: 0.0124788536\n",
      "====> Epoch: 49 Average train loss: 0.0163813873\n",
      "====> Epoch: 49 Average L0 reg loss: 0.4012152244\n",
      "====> Epoch: 49 Average eval loss: 0.0113710733\n",
      "====> Epoch: 50 Average train loss: 0.0167397739\n",
      "====> Epoch: 50 Average L0 reg loss: 0.3988328871\n",
      "====> Epoch: 50 Average eval loss: 0.0117285810\n",
      "====> Epoch: 51 Average train loss: 0.0160822532\n",
      "====> Epoch: 51 Average L0 reg loss: 0.3963085896\n",
      "====> Epoch: 51 Average eval loss: 0.0110194972\n",
      "====> Epoch: 52 Average train loss: 0.0155571791\n",
      "====> Epoch: 52 Average L0 reg loss: 0.3936626856\n",
      "====> Epoch: 52 Average eval loss: 0.0115855271\n",
      "====> Epoch: 53 Average train loss: 0.0150874010\n",
      "====> Epoch: 53 Average L0 reg loss: 0.3909298507\n",
      "====> Epoch: 53 Average eval loss: 0.0114251357\n",
      "====> Epoch: 54 Average train loss: 0.0152482686\n",
      "====> Epoch: 54 Average L0 reg loss: 0.3881682301\n",
      "====> Epoch: 54 Average eval loss: 0.0108140064\n",
      "====> Epoch: 55 Average train loss: 0.0143453332\n",
      "====> Epoch: 55 Average L0 reg loss: 0.3854278136\n",
      "====> Epoch: 55 Average eval loss: 0.0110045699\n",
      "====> Epoch: 56 Average train loss: 0.0142839933\n",
      "====> Epoch: 56 Average L0 reg loss: 0.3827481648\n",
      "====> Epoch: 56 Average eval loss: 0.0106119690\n",
      "====> Epoch: 57 Average train loss: 0.0139793530\n",
      "====> Epoch: 57 Average L0 reg loss: 0.3801562574\n",
      "====> Epoch: 57 Average eval loss: 0.0104691554\n",
      "====> Epoch: 58 Average train loss: 0.0135944273\n",
      "====> Epoch: 58 Average L0 reg loss: 0.3776665616\n",
      "====> Epoch: 58 Average eval loss: 0.0107183522\n",
      "====> Epoch: 59 Average train loss: 0.0135028973\n",
      "====> Epoch: 59 Average L0 reg loss: 0.3752825229\n",
      "====> Epoch: 59 Average eval loss: 0.0105583305\n",
      "====> Epoch: 60 Average train loss: 0.0132890203\n",
      "====> Epoch: 60 Average L0 reg loss: 0.3730023716\n",
      "====> Epoch: 60 Average eval loss: 0.0105491653\n",
      "====> Epoch: 61 Average train loss: 0.0130938426\n",
      "====> Epoch: 61 Average L0 reg loss: 0.3708189889\n",
      "====> Epoch: 61 Average eval loss: 0.0105417809\n",
      "====> Epoch: 62 Average train loss: 0.0131431742\n",
      "====> Epoch: 62 Average L0 reg loss: 0.3687263995\n",
      "====> Epoch: 62 Average eval loss: 0.0105514498\n",
      "====> Epoch: 63 Average train loss: 0.0124337526\n",
      "====> Epoch: 63 Average L0 reg loss: 0.3667238468\n",
      "====> Epoch: 63 Average eval loss: 0.0105456645\n",
      "====> Epoch: 64 Average train loss: 0.0130732249\n",
      "====> Epoch: 64 Average L0 reg loss: 0.3648119137\n",
      "====> Epoch: 64 Average eval loss: 0.0109213050\n",
      "====> Epoch: 65 Average train loss: 0.0127748613\n",
      "====> Epoch: 65 Average L0 reg loss: 0.3629894877\n",
      "====> Epoch: 65 Average eval loss: 0.0104544926\n",
      "====> Epoch: 66 Average train loss: 0.0125150599\n",
      "====> Epoch: 66 Average L0 reg loss: 0.3612565051\n",
      "====> Epoch: 66 Average eval loss: 0.0106504289\n",
      "====> Epoch: 67 Average train loss: 0.0124667723\n",
      "====> Epoch: 67 Average L0 reg loss: 0.3596102485\n",
      "====> Epoch: 67 Average eval loss: 0.0103904316\n",
      "====> Epoch: 68 Average train loss: 0.0120528778\n",
      "====> Epoch: 68 Average L0 reg loss: 0.3580469080\n",
      "====> Epoch: 68 Average eval loss: 0.0105916476\n",
      "====> Epoch: 69 Average train loss: 0.0122005250\n",
      "====> Epoch: 69 Average L0 reg loss: 0.3565627153\n",
      "====> Epoch: 69 Average eval loss: 0.0103796618\n",
      "====> Epoch: 70 Average train loss: 0.0120917390\n",
      "====> Epoch: 70 Average L0 reg loss: 0.3551542245\n",
      "====> Epoch: 70 Average eval loss: 0.0104589872\n",
      "====> Epoch: 71 Average train loss: 0.0115590556\n",
      "====> Epoch: 71 Average L0 reg loss: 0.3538183362\n",
      "====> Epoch: 71 Average eval loss: 0.0104223313\n",
      "====> Epoch: 72 Average train loss: 0.0117957017\n",
      "====> Epoch: 72 Average L0 reg loss: 0.3525565716\n",
      "====> Epoch: 72 Average eval loss: 0.0104076751\n",
      "====> Epoch: 73 Average train loss: 0.0117056619\n",
      "====> Epoch: 73 Average L0 reg loss: 0.3513704143\n",
      "====> Epoch: 73 Average eval loss: 0.0102886073\n",
      "====> Epoch: 74 Average train loss: 0.0118812458\n",
      "====> Epoch: 74 Average L0 reg loss: 0.3502631407\n",
      "====> Epoch: 74 Average eval loss: 0.0104947304\n",
      "====> Epoch: 75 Average train loss: 0.0118354033\n",
      "====> Epoch: 75 Average L0 reg loss: 0.3492384041\n",
      "====> Epoch: 75 Average eval loss: 0.0103529477\n",
      "====> Epoch: 76 Average train loss: 0.0114306297\n",
      "====> Epoch: 76 Average L0 reg loss: 0.3482958077\n",
      "====> Epoch: 76 Average eval loss: 0.0102582239\n",
      "====> Epoch: 77 Average train loss: 0.0115666983\n",
      "====> Epoch: 77 Average L0 reg loss: 0.3474371379\n",
      "====> Epoch: 77 Average eval loss: 0.0102522578\n",
      "====> Epoch: 78 Average train loss: 0.0119002202\n",
      "====> Epoch: 78 Average L0 reg loss: 0.3466602340\n",
      "====> Epoch: 78 Average eval loss: 0.0103310477\n",
      "====> Epoch: 79 Average train loss: 0.0114110466\n",
      "====> Epoch: 79 Average L0 reg loss: 0.3459620264\n",
      "====> Epoch: 79 Average eval loss: 0.0103663355\n",
      "====> Epoch: 80 Average train loss: 0.0115317122\n",
      "====> Epoch: 80 Average L0 reg loss: 0.3453369339\n",
      "====> Epoch: 80 Average eval loss: 0.0102781374\n",
      "====> Epoch: 81 Average train loss: 0.0113222800\n",
      "====> Epoch: 81 Average L0 reg loss: 0.3447790753\n",
      "====> Epoch: 81 Average eval loss: 0.0102736540\n",
      "====> Epoch: 82 Average train loss: 0.0113404748\n",
      "====> Epoch: 82 Average L0 reg loss: 0.3442812744\n",
      "====> Epoch: 82 Average eval loss: 0.0103411498\n",
      "====> Epoch: 83 Average train loss: 0.0111953522\n",
      "====> Epoch: 83 Average L0 reg loss: 0.3438357169\n",
      "====> Epoch: 83 Average eval loss: 0.0102871647\n",
      "====> Epoch: 84 Average train loss: 0.0111075407\n",
      "====> Epoch: 84 Average L0 reg loss: 0.3434354122\n",
      "====> Epoch: 84 Average eval loss: 0.0103128543\n",
      "====> Epoch: 85 Average train loss: 0.0111219308\n",
      "====> Epoch: 85 Average L0 reg loss: 0.3430737492\n",
      "====> Epoch: 85 Average eval loss: 0.0104155233\n",
      "====> Epoch: 86 Average train loss: 0.0112673611\n",
      "====> Epoch: 86 Average L0 reg loss: 0.3427458193\n",
      "====> Epoch: 86 Average eval loss: 0.0102269752\n",
      "====> Epoch: 87 Average train loss: 0.0109192737\n",
      "====> Epoch: 87 Average L0 reg loss: 0.3424473629\n",
      "====> Epoch: 87 Average eval loss: 0.0102282576\n",
      "====> Epoch: 88 Average train loss: 0.0111385696\n",
      "====> Epoch: 88 Average L0 reg loss: 0.3421752512\n",
      "====> Epoch: 88 Average eval loss: 0.0102591123\n",
      "====> Epoch: 89 Average train loss: 0.0109466847\n",
      "====> Epoch: 89 Average L0 reg loss: 0.3419285045\n",
      "====> Epoch: 89 Average eval loss: 0.0102295000\n",
      "====> Epoch: 90 Average train loss: 0.0107912448\n",
      "====> Epoch: 90 Average L0 reg loss: 0.3417047780\n",
      "====> Epoch: 90 Average eval loss: 0.0103853550\n",
      "====> Epoch: 91 Average train loss: 0.0111383405\n",
      "====> Epoch: 91 Average L0 reg loss: 0.3415035191\n",
      "====> Epoch: 91 Average eval loss: 0.0102306260\n",
      "====> Epoch: 92 Average train loss: 0.0109451332\n",
      "====> Epoch: 92 Average L0 reg loss: 0.3413215772\n",
      "====> Epoch: 92 Average eval loss: 0.0102950400\n",
      "====> Epoch: 93 Average train loss: 0.0109200759\n",
      "====> Epoch: 93 Average L0 reg loss: 0.3411563098\n",
      "====> Epoch: 93 Average eval loss: 0.0103016952\n",
      "====> Epoch: 94 Average train loss: 0.0110108626\n",
      "====> Epoch: 94 Average L0 reg loss: 0.3410052233\n",
      "====> Epoch: 94 Average eval loss: 0.0102693718\n",
      "====> Epoch: 95 Average train loss: 0.0109527637\n",
      "====> Epoch: 95 Average L0 reg loss: 0.3408649970\n",
      "====> Epoch: 95 Average eval loss: 0.0102293827\n",
      "====> Epoch: 96 Average train loss: 0.0108555673\n",
      "====> Epoch: 96 Average L0 reg loss: 0.3407316470\n",
      "====> Epoch: 96 Average eval loss: 0.0106885359\n",
      "====> Epoch: 97 Average train loss: 0.0109318843\n",
      "====> Epoch: 97 Average L0 reg loss: 0.3406017796\n",
      "====> Epoch: 97 Average eval loss: 0.0102467723\n",
      "====> Epoch: 98 Average train loss: 0.0109873577\n",
      "====> Epoch: 98 Average L0 reg loss: 0.3404735083\n",
      "====> Epoch: 98 Average eval loss: 0.0104021551\n",
      "====> Epoch: 99 Average train loss: 0.0107034479\n",
      "====> Epoch: 99 Average L0 reg loss: 0.3403458035\n",
      "====> Epoch: 99 Average eval loss: 0.0104004107\n",
      "====> Epoch: 100 Average train loss: 0.0109310918\n",
      "====> Epoch: 100 Average L0 reg loss: 0.3402170944\n",
      "====> Epoch: 100 Average eval loss: 0.0102125518\n",
      "====> Epoch: 101 Average train loss: 0.0108942818\n",
      "====> Epoch: 101 Average L0 reg loss: 0.3400885462\n",
      "====> Epoch: 101 Average eval loss: 0.0102391886\n",
      "====> Epoch: 102 Average train loss: 0.0109005485\n",
      "====> Epoch: 102 Average L0 reg loss: 0.3399615475\n",
      "====> Epoch: 102 Average eval loss: 0.0102393450\n",
      "====> Epoch: 103 Average train loss: 0.0108010690\n",
      "====> Epoch: 103 Average L0 reg loss: 0.3398374128\n",
      "====> Epoch: 103 Average eval loss: 0.0102399699\n",
      "====> Epoch: 104 Average train loss: 0.0108318480\n",
      "====> Epoch: 104 Average L0 reg loss: 0.3397182539\n",
      "====> Epoch: 104 Average eval loss: 0.0102470294\n",
      "====> Epoch: 105 Average train loss: 0.0109083171\n",
      "====> Epoch: 105 Average L0 reg loss: 0.3396054909\n",
      "====> Epoch: 105 Average eval loss: 0.0102316877\n",
      "====> Epoch: 106 Average train loss: 0.0107446562\n",
      "====> Epoch: 106 Average L0 reg loss: 0.3395006926\n",
      "====> Epoch: 106 Average eval loss: 0.0102362772\n",
      "====> Epoch: 107 Average train loss: 0.0106677169\n",
      "====> Epoch: 107 Average L0 reg loss: 0.3394030112\n",
      "====> Epoch: 107 Average eval loss: 0.0104088541\n",
      "====> Epoch: 108 Average train loss: 0.0107066811\n",
      "====> Epoch: 108 Average L0 reg loss: 0.3393114730\n",
      "====> Epoch: 108 Average eval loss: 0.0102529125\n",
      "====> Epoch: 109 Average train loss: 0.0107440652\n",
      "====> Epoch: 109 Average L0 reg loss: 0.3392246517\n",
      "====> Epoch: 109 Average eval loss: 0.0101792030\n",
      "====> Epoch: 110 Average train loss: 0.0106888013\n",
      "====> Epoch: 110 Average L0 reg loss: 0.3391411034\n",
      "====> Epoch: 110 Average eval loss: 0.0103221256\n",
      "====> Epoch: 111 Average train loss: 0.0105658909\n",
      "====> Epoch: 111 Average L0 reg loss: 0.3390585703\n",
      "====> Epoch: 111 Average eval loss: 0.0101852920\n",
      "====> Epoch: 112 Average train loss: 0.0106977046\n",
      "====> Epoch: 112 Average L0 reg loss: 0.3389753317\n",
      "====> Epoch: 112 Average eval loss: 0.0101987654\n",
      "====> Epoch: 113 Average train loss: 0.0106640839\n",
      "====> Epoch: 113 Average L0 reg loss: 0.3388894416\n",
      "====> Epoch: 113 Average eval loss: 0.0102622155\n",
      "====> Epoch: 114 Average train loss: 0.0106511620\n",
      "====> Epoch: 114 Average L0 reg loss: 0.3387994237\n",
      "====> Epoch: 114 Average eval loss: 0.0102060148\n",
      "====> Epoch: 115 Average train loss: 0.0106931459\n",
      "====> Epoch: 115 Average L0 reg loss: 0.3387035745\n",
      "====> Epoch: 115 Average eval loss: 0.0102203377\n",
      "====> Epoch: 116 Average train loss: 0.0105826733\n",
      "====> Epoch: 116 Average L0 reg loss: 0.3386019870\n",
      "====> Epoch: 116 Average eval loss: 0.0101676714\n",
      "====> Epoch: 117 Average train loss: 0.0105695409\n",
      "====> Epoch: 117 Average L0 reg loss: 0.3384957232\n",
      "====> Epoch: 117 Average eval loss: 0.0101978732\n",
      "====> Epoch: 118 Average train loss: 0.0105460750\n",
      "====> Epoch: 118 Average L0 reg loss: 0.3383874879\n",
      "====> Epoch: 118 Average eval loss: 0.0102703730\n",
      "====> Epoch: 119 Average train loss: 0.0104712156\n",
      "====> Epoch: 119 Average L0 reg loss: 0.3382806021\n",
      "====> Epoch: 119 Average eval loss: 0.0102093462\n",
      "====> Epoch: 120 Average train loss: 0.0107869069\n",
      "====> Epoch: 120 Average L0 reg loss: 0.3381771932\n",
      "====> Epoch: 120 Average eval loss: 0.0101682777\n",
      "====> Epoch: 121 Average train loss: 0.0105937703\n",
      "====> Epoch: 121 Average L0 reg loss: 0.3380804120\n",
      "====> Epoch: 121 Average eval loss: 0.0102024553\n",
      "====> Epoch: 122 Average train loss: 0.0104719529\n",
      "====> Epoch: 122 Average L0 reg loss: 0.3379907550\n",
      "====> Epoch: 122 Average eval loss: 0.0101816505\n",
      "====> Epoch: 123 Average train loss: 0.0104938800\n",
      "====> Epoch: 123 Average L0 reg loss: 0.3379089024\n",
      "====> Epoch: 123 Average eval loss: 0.0101874927\n",
      "====> Epoch: 124 Average train loss: 0.0105709176\n",
      "====> Epoch: 124 Average L0 reg loss: 0.3378361261\n",
      "====> Epoch: 124 Average eval loss: 0.0104072141\n",
      "====> Epoch: 125 Average train loss: 0.0105046870\n",
      "====> Epoch: 125 Average L0 reg loss: 0.3377724583\n",
      "====> Epoch: 125 Average eval loss: 0.0103262477\n",
      "====> Epoch: 126 Average train loss: 0.0106243055\n",
      "====> Epoch: 126 Average L0 reg loss: 0.3377174953\n",
      "====> Epoch: 126 Average eval loss: 0.0102449870\n",
      "====> Epoch: 127 Average train loss: 0.0104591593\n",
      "====> Epoch: 127 Average L0 reg loss: 0.3376698976\n",
      "====> Epoch: 127 Average eval loss: 0.0102333548\n",
      "====> Epoch: 128 Average train loss: 0.0106356833\n",
      "====> Epoch: 128 Average L0 reg loss: 0.3376286573\n",
      "====> Epoch: 128 Average eval loss: 0.0101980083\n",
      "====> Epoch: 129 Average train loss: 0.0105563451\n",
      "====> Epoch: 129 Average L0 reg loss: 0.3375929539\n",
      "====> Epoch: 129 Average eval loss: 0.0103137922\n",
      "====> Epoch: 130 Average train loss: 0.0104799090\n",
      "====> Epoch: 130 Average L0 reg loss: 0.3375632242\n",
      "====> Epoch: 130 Average eval loss: 0.0103063006\n",
      "====> Epoch: 131 Average train loss: 0.0105672297\n",
      "====> Epoch: 131 Average L0 reg loss: 0.3375385032\n",
      "====> Epoch: 131 Average eval loss: 0.0101907412\n",
      "====> Epoch: 132 Average train loss: 0.0106170152\n",
      "====> Epoch: 132 Average L0 reg loss: 0.3375167925\n",
      "====> Epoch: 132 Average eval loss: 0.0103270821\n",
      "====> Epoch: 133 Average train loss: 0.0104186705\n",
      "====> Epoch: 133 Average L0 reg loss: 0.3374982151\n",
      "====> Epoch: 133 Average eval loss: 0.0102939000\n",
      "====> Epoch: 134 Average train loss: 0.0104486532\n",
      "====> Epoch: 134 Average L0 reg loss: 0.3374820032\n",
      "====> Epoch: 134 Average eval loss: 0.0102085797\n",
      "====> Epoch: 135 Average train loss: 0.0104461817\n",
      "====> Epoch: 135 Average L0 reg loss: 0.3374671528\n",
      "====> Epoch: 135 Average eval loss: 0.0101458225\n",
      "====> Epoch: 136 Average train loss: 0.0107110017\n",
      "====> Epoch: 136 Average L0 reg loss: 0.3374528866\n",
      "====> Epoch: 136 Average eval loss: 0.0102543104\n",
      "====> Epoch: 137 Average train loss: 0.0105793533\n",
      "====> Epoch: 137 Average L0 reg loss: 0.3374428375\n",
      "====> Epoch: 137 Average eval loss: 0.0101554655\n",
      "====> Epoch: 138 Average train loss: 0.0103931326\n",
      "====> Epoch: 138 Average L0 reg loss: 0.3374349647\n",
      "====> Epoch: 138 Average eval loss: 0.0102571389\n",
      "====> Epoch: 139 Average train loss: 0.0103905934\n",
      "====> Epoch: 139 Average L0 reg loss: 0.3374280437\n",
      "====> Epoch: 139 Average eval loss: 0.0101796975\n",
      "====> Epoch: 140 Average train loss: 0.0104643240\n",
      "====> Epoch: 140 Average L0 reg loss: 0.3374221791\n",
      "====> Epoch: 140 Average eval loss: 0.0101877162\n",
      "====> Epoch: 141 Average train loss: 0.0103876519\n",
      "====> Epoch: 141 Average L0 reg loss: 0.3374169576\n",
      "====> Epoch: 141 Average eval loss: 0.0101779299\n",
      "====> Epoch: 142 Average train loss: 0.0104876464\n",
      "====> Epoch: 142 Average L0 reg loss: 0.3374123687\n",
      "====> Epoch: 142 Average eval loss: 0.0101574948\n",
      "====> Epoch: 143 Average train loss: 0.0104256846\n",
      "====> Epoch: 143 Average L0 reg loss: 0.3374087537\n",
      "====> Epoch: 143 Average eval loss: 0.0102134915\n",
      "====> Epoch: 144 Average train loss: 0.0104093393\n",
      "====> Epoch: 144 Average L0 reg loss: 0.3374058385\n",
      "====> Epoch: 144 Average eval loss: 0.0101879379\n",
      "====> Epoch: 145 Average train loss: 0.0105059457\n",
      "====> Epoch: 145 Average L0 reg loss: 0.3374017293\n",
      "====> Epoch: 145 Average eval loss: 0.0101692900\n",
      "====> Epoch: 146 Average train loss: 0.0104925618\n",
      "====> Epoch: 146 Average L0 reg loss: 0.3373972696\n",
      "====> Epoch: 146 Average eval loss: 0.0101508908\n",
      "====> Epoch: 147 Average train loss: 0.0104770780\n",
      "====> Epoch: 147 Average L0 reg loss: 0.3373918584\n",
      "====> Epoch: 147 Average eval loss: 0.0101614445\n",
      "====> Epoch: 148 Average train loss: 0.0103881290\n",
      "====> Epoch: 148 Average L0 reg loss: 0.3373880454\n",
      "====> Epoch: 148 Average eval loss: 0.0102221705\n",
      "====> Epoch: 149 Average train loss: 0.0104460102\n",
      "====> Epoch: 149 Average L0 reg loss: 0.3373863759\n",
      "====> Epoch: 149 Average eval loss: 0.0103793535\n",
      "====> Epoch: 150 Average train loss: 0.0104548631\n",
      "====> Epoch: 150 Average L0 reg loss: 0.3373836527\n",
      "====> Epoch: 150 Average eval loss: 0.0102782780\n",
      "====> Epoch: 151 Average train loss: 0.0103544468\n",
      "====> Epoch: 151 Average L0 reg loss: 0.3373797482\n",
      "====> Epoch: 151 Average eval loss: 0.0101729734\n",
      "====> Epoch: 152 Average train loss: 0.0105105386\n",
      "====> Epoch: 152 Average L0 reg loss: 0.3373746968\n",
      "====> Epoch: 152 Average eval loss: 0.0101535572\n",
      "====> Epoch: 153 Average train loss: 0.0103495800\n",
      "====> Epoch: 153 Average L0 reg loss: 0.3373671097\n",
      "====> Epoch: 153 Average eval loss: 0.0102322716\n",
      "====> Epoch: 154 Average train loss: 0.0103576070\n",
      "====> Epoch: 154 Average L0 reg loss: 0.3373557623\n",
      "====> Epoch: 154 Average eval loss: 0.0101821227\n",
      "====> Epoch: 155 Average train loss: 0.0104166216\n",
      "====> Epoch: 155 Average L0 reg loss: 0.3373392272\n",
      "====> Epoch: 155 Average eval loss: 0.0101726623\n",
      "====> Epoch: 156 Average train loss: 0.0103422902\n",
      "====> Epoch: 156 Average L0 reg loss: 0.3373283629\n",
      "====> Epoch: 156 Average eval loss: 0.0101551795\n",
      "====> Epoch: 157 Average train loss: 0.0104145080\n",
      "====> Epoch: 157 Average L0 reg loss: 0.3373178021\n",
      "====> Epoch: 157 Average eval loss: 0.0101804864\n",
      "====> Epoch: 158 Average train loss: 0.0103993904\n",
      "====> Epoch: 158 Average L0 reg loss: 0.3373100648\n",
      "====> Epoch: 158 Average eval loss: 0.0101450281\n",
      "====> Epoch: 159 Average train loss: 0.0105425322\n",
      "====> Epoch: 159 Average L0 reg loss: 0.3373036169\n",
      "====> Epoch: 159 Average eval loss: 0.0101588257\n",
      "====> Epoch: 160 Average train loss: 0.0102932857\n",
      "====> Epoch: 160 Average L0 reg loss: 0.3372936700\n",
      "====> Epoch: 160 Average eval loss: 0.0102839563\n",
      "====> Epoch: 161 Average train loss: 0.0104949929\n",
      "====> Epoch: 161 Average L0 reg loss: 0.3372798719\n",
      "====> Epoch: 161 Average eval loss: 0.0101874107\n",
      "====> Epoch: 162 Average train loss: 0.0104859888\n",
      "====> Epoch: 162 Average L0 reg loss: 0.3372642446\n",
      "====> Epoch: 162 Average eval loss: 0.0101358062\n",
      "====> Epoch: 163 Average train loss: 0.0103677203\n",
      "====> Epoch: 163 Average L0 reg loss: 0.3372442559\n",
      "====> Epoch: 163 Average eval loss: 0.0101670884\n",
      "====> Epoch: 164 Average train loss: 0.0103214673\n",
      "====> Epoch: 164 Average L0 reg loss: 0.3372123546\n",
      "====> Epoch: 164 Average eval loss: 0.0102592101\n",
      "====> Epoch: 165 Average train loss: 0.0104056788\n",
      "====> Epoch: 165 Average L0 reg loss: 0.3371740830\n",
      "====> Epoch: 165 Average eval loss: 0.0101771699\n",
      "====> Epoch: 166 Average train loss: 0.0103956213\n",
      "====> Epoch: 166 Average L0 reg loss: 0.3371517820\n",
      "====> Epoch: 166 Average eval loss: 0.0102781430\n",
      "====> Epoch: 167 Average train loss: 0.0105623678\n",
      "====> Epoch: 167 Average L0 reg loss: 0.3371216046\n",
      "====> Epoch: 167 Average eval loss: 0.0101482710\n",
      "====> Epoch: 168 Average train loss: 0.0103339682\n",
      "====> Epoch: 168 Average L0 reg loss: 0.3370766711\n",
      "====> Epoch: 168 Average eval loss: 0.0103552518\n",
      "====> Epoch: 169 Average train loss: 0.0103487806\n",
      "====> Epoch: 169 Average L0 reg loss: 0.3370054363\n",
      "====> Epoch: 169 Average eval loss: 0.0101657575\n",
      "====> Epoch: 170 Average train loss: 0.0103978189\n",
      "====> Epoch: 170 Average L0 reg loss: 0.3369155167\n",
      "====> Epoch: 170 Average eval loss: 0.0102935983\n",
      "====> Epoch: 171 Average train loss: 0.0105431472\n",
      "====> Epoch: 171 Average L0 reg loss: 0.3368204467\n",
      "====> Epoch: 171 Average eval loss: 0.0103301760\n",
      "====> Epoch: 172 Average train loss: 0.0104661729\n",
      "====> Epoch: 172 Average L0 reg loss: 0.3367190806\n",
      "====> Epoch: 172 Average eval loss: 0.0102431895\n",
      "====> Epoch: 173 Average train loss: 0.0104609205\n",
      "====> Epoch: 173 Average L0 reg loss: 0.3366273474\n",
      "====> Epoch: 173 Average eval loss: 0.0101583702\n",
      "====> Epoch: 174 Average train loss: 0.0103616983\n",
      "====> Epoch: 174 Average L0 reg loss: 0.3365030039\n",
      "====> Epoch: 174 Average eval loss: 0.0101564685\n",
      "====> Epoch: 175 Average train loss: 0.0104497849\n",
      "====> Epoch: 175 Average L0 reg loss: 0.3363116522\n",
      "====> Epoch: 175 Average eval loss: 0.0102643687\n",
      "====> Epoch: 176 Average train loss: 0.0104766867\n",
      "====> Epoch: 176 Average L0 reg loss: 0.3360617225\n",
      "====> Epoch: 176 Average eval loss: 0.0102551309\n",
      "====> Epoch: 177 Average train loss: 0.0105507461\n",
      "====> Epoch: 177 Average L0 reg loss: 0.3357220866\n",
      "====> Epoch: 177 Average eval loss: 0.0101907775\n",
      "====> Epoch: 178 Average train loss: 0.0105780252\n",
      "====> Epoch: 178 Average L0 reg loss: 0.3352931585\n",
      "====> Epoch: 178 Average eval loss: 0.0101584923\n",
      "====> Epoch: 179 Average train loss: 0.0107296688\n",
      "====> Epoch: 179 Average L0 reg loss: 0.3347176146\n",
      "====> Epoch: 179 Average eval loss: 0.0102104517\n",
      "====> Epoch: 180 Average train loss: 0.0105223092\n",
      "====> Epoch: 180 Average L0 reg loss: 0.3339644446\n",
      "====> Epoch: 180 Average eval loss: 0.0101950429\n",
      "====> Epoch: 181 Average train loss: 0.0105416254\n",
      "====> Epoch: 181 Average L0 reg loss: 0.3330140044\n",
      "====> Epoch: 181 Average eval loss: 0.0102312034\n",
      "====> Epoch: 182 Average train loss: 0.0106046050\n",
      "====> Epoch: 182 Average L0 reg loss: 0.3318505720\n",
      "====> Epoch: 182 Average eval loss: 0.0101893973\n",
      "====> Epoch: 183 Average train loss: 0.0106163597\n",
      "====> Epoch: 183 Average L0 reg loss: 0.3304486174\n",
      "====> Epoch: 183 Average eval loss: 0.0101699131\n",
      "====> Epoch: 184 Average train loss: 0.0106677223\n",
      "====> Epoch: 184 Average L0 reg loss: 0.3288273596\n",
      "====> Epoch: 184 Average eval loss: 0.0103046084\n",
      "====> Epoch: 185 Average train loss: 0.0107081396\n",
      "====> Epoch: 185 Average L0 reg loss: 0.3270330766\n",
      "====> Epoch: 185 Average eval loss: 0.0102005554\n",
      "====> Epoch: 186 Average train loss: 0.0105671476\n",
      "====> Epoch: 186 Average L0 reg loss: 0.3251190532\n",
      "====> Epoch: 186 Average eval loss: 0.0101630250\n",
      "====> Epoch: 187 Average train loss: 0.0107689024\n",
      "====> Epoch: 187 Average L0 reg loss: 0.3231615995\n",
      "====> Epoch: 187 Average eval loss: 0.0101841539\n",
      "====> Epoch: 188 Average train loss: 0.0106778774\n",
      "====> Epoch: 188 Average L0 reg loss: 0.3212062910\n",
      "====> Epoch: 188 Average eval loss: 0.0101680681\n",
      "====> Epoch: 189 Average train loss: 0.0106883175\n",
      "====> Epoch: 189 Average L0 reg loss: 0.3193159584\n",
      "====> Epoch: 189 Average eval loss: 0.0102176517\n",
      "====> Epoch: 190 Average train loss: 0.0107370712\n",
      "====> Epoch: 190 Average L0 reg loss: 0.3175400935\n",
      "====> Epoch: 190 Average eval loss: 0.0101382313\n",
      "====> Epoch: 191 Average train loss: 0.0105694815\n",
      "====> Epoch: 191 Average L0 reg loss: 0.3159125967\n",
      "====> Epoch: 191 Average eval loss: 0.0101793716\n",
      "====> Epoch: 192 Average train loss: 0.0104996819\n",
      "====> Epoch: 192 Average L0 reg loss: 0.3144506251\n",
      "====> Epoch: 192 Average eval loss: 0.0101461140\n",
      "====> Epoch: 193 Average train loss: 0.0105408324\n",
      "====> Epoch: 193 Average L0 reg loss: 0.3131542781\n",
      "====> Epoch: 193 Average eval loss: 0.0101628555\n",
      "====> Epoch: 194 Average train loss: 0.0107045359\n",
      "====> Epoch: 194 Average L0 reg loss: 0.3120204523\n",
      "====> Epoch: 194 Average eval loss: 0.0102163469\n",
      "====> Epoch: 195 Average train loss: 0.0106362171\n",
      "====> Epoch: 195 Average L0 reg loss: 0.3110363796\n",
      "====> Epoch: 195 Average eval loss: 0.0102446545\n",
      "====> Epoch: 196 Average train loss: 0.0104751962\n",
      "====> Epoch: 196 Average L0 reg loss: 0.3101903313\n",
      "====> Epoch: 196 Average eval loss: 0.0102372393\n",
      "====> Epoch: 197 Average train loss: 0.0105905290\n",
      "====> Epoch: 197 Average L0 reg loss: 0.3094675671\n",
      "====> Epoch: 197 Average eval loss: 0.0101542603\n",
      "====> Epoch: 198 Average train loss: 0.0108073924\n",
      "====> Epoch: 198 Average L0 reg loss: 0.3088572745\n",
      "====> Epoch: 198 Average eval loss: 0.0101738581\n",
      "====> Epoch: 199 Average train loss: 0.0105910530\n",
      "====> Epoch: 199 Average L0 reg loss: 0.3083325697\n",
      "====> Epoch: 199 Average eval loss: 0.0101221045\n",
      "====> Epoch: 200 Average train loss: 0.0103968517\n",
      "====> Epoch: 200 Average L0 reg loss: 0.3078863838\n",
      "====> Epoch: 200 Average eval loss: 0.0102292486\n",
      "====> Epoch: 201 Average train loss: 0.0105155812\n",
      "====> Epoch: 201 Average L0 reg loss: 0.3075086558\n",
      "====> Epoch: 201 Average eval loss: 0.0102737285\n",
      "====> Epoch: 202 Average train loss: 0.0104926059\n",
      "====> Epoch: 202 Average L0 reg loss: 0.3071893038\n",
      "====> Epoch: 202 Average eval loss: 0.0101180356\n",
      "====> Epoch: 203 Average train loss: 0.0103676897\n",
      "====> Epoch: 203 Average L0 reg loss: 0.3069196423\n",
      "====> Epoch: 203 Average eval loss: 0.0101529295\n",
      "====> Epoch: 204 Average train loss: 0.0106492885\n",
      "====> Epoch: 204 Average L0 reg loss: 0.3066923654\n",
      "====> Epoch: 204 Average eval loss: 0.0101295235\n",
      "====> Epoch: 205 Average train loss: 0.0103667989\n",
      "====> Epoch: 205 Average L0 reg loss: 0.3065010493\n",
      "====> Epoch: 205 Average eval loss: 0.0102611631\n",
      "====> Epoch: 206 Average train loss: 0.0106074071\n",
      "====> Epoch: 206 Average L0 reg loss: 0.3063398038\n",
      "====> Epoch: 206 Average eval loss: 0.0101409443\n",
      "====> Epoch: 207 Average train loss: 0.0104760169\n",
      "====> Epoch: 207 Average L0 reg loss: 0.3062042450\n",
      "====> Epoch: 207 Average eval loss: 0.0102148205\n",
      "====> Epoch: 208 Average train loss: 0.0104987710\n",
      "====> Epoch: 208 Average L0 reg loss: 0.3060901076\n",
      "====> Epoch: 208 Average eval loss: 0.0101595763\n",
      "====> Epoch: 209 Average train loss: 0.0105894132\n",
      "====> Epoch: 209 Average L0 reg loss: 0.3059941434\n",
      "====> Epoch: 209 Average eval loss: 0.0101801576\n",
      "====> Epoch: 210 Average train loss: 0.0105079199\n",
      "====> Epoch: 210 Average L0 reg loss: 0.3059133755\n",
      "====> Epoch: 210 Average eval loss: 0.0101216286\n",
      "====> Epoch: 211 Average train loss: 0.0104709117\n",
      "====> Epoch: 211 Average L0 reg loss: 0.3058456587\n",
      "====> Epoch: 211 Average eval loss: 0.0101390602\n",
      "====> Epoch: 212 Average train loss: 0.0104238771\n",
      "====> Epoch: 212 Average L0 reg loss: 0.3057886140\n",
      "====> Epoch: 212 Average eval loss: 0.0102615301\n",
      "====> Epoch: 213 Average train loss: 0.0104017669\n",
      "====> Epoch: 213 Average L0 reg loss: 0.3057408339\n",
      "====> Epoch: 213 Average eval loss: 0.0101514691\n",
      "====> Epoch: 214 Average train loss: 0.0103167976\n",
      "====> Epoch: 214 Average L0 reg loss: 0.3057006570\n",
      "====> Epoch: 214 Average eval loss: 0.0101218438\n",
      "====> Epoch: 215 Average train loss: 0.0104305495\n",
      "====> Epoch: 215 Average L0 reg loss: 0.3056668072\n",
      "====> Epoch: 215 Average eval loss: 0.0102198347\n",
      "====> Epoch: 216 Average train loss: 0.0105777397\n",
      "====> Epoch: 216 Average L0 reg loss: 0.3056383416\n",
      "====> Epoch: 216 Average eval loss: 0.0101809492\n",
      "====> Epoch: 217 Average train loss: 0.0102800134\n",
      "====> Epoch: 217 Average L0 reg loss: 0.3056143704\n",
      "====> Epoch: 217 Average eval loss: 0.0102723259\n",
      "====> Epoch: 218 Average train loss: 0.0105440731\n",
      "====> Epoch: 218 Average L0 reg loss: 0.3055940330\n",
      "====> Epoch: 218 Average eval loss: 0.0101277642\n",
      "====> Epoch: 219 Average train loss: 0.0103023808\n",
      "====> Epoch: 219 Average L0 reg loss: 0.3055770215\n",
      "====> Epoch: 219 Average eval loss: 0.0101649314\n",
      "====> Epoch: 220 Average train loss: 0.0103302390\n",
      "====> Epoch: 220 Average L0 reg loss: 0.3055626101\n",
      "====> Epoch: 220 Average eval loss: 0.0101252524\n",
      "====> Epoch: 221 Average train loss: 0.0103784357\n",
      "====> Epoch: 221 Average L0 reg loss: 0.3055503787\n",
      "====> Epoch: 221 Average eval loss: 0.0101709105\n",
      "====> Epoch: 222 Average train loss: 0.0103015490\n",
      "====> Epoch: 222 Average L0 reg loss: 0.3055399875\n",
      "====> Epoch: 222 Average eval loss: 0.0101223653\n",
      "====> Epoch: 223 Average train loss: 0.0103032291\n",
      "====> Epoch: 223 Average L0 reg loss: 0.3055310258\n",
      "====> Epoch: 223 Average eval loss: 0.0101357438\n",
      "====> Epoch: 224 Average train loss: 0.0102160775\n",
      "====> Epoch: 224 Average L0 reg loss: 0.3055231694\n",
      "====> Epoch: 224 Average eval loss: 0.0101178419\n",
      "====> Epoch: 225 Average train loss: 0.0104916623\n",
      "====> Epoch: 225 Average L0 reg loss: 0.3055161835\n",
      "====> Epoch: 225 Average eval loss: 0.0101213623\n",
      "====> Epoch: 226 Average train loss: 0.0104092676\n",
      "====> Epoch: 226 Average L0 reg loss: 0.3055096731\n",
      "====> Epoch: 226 Average eval loss: 0.0101296026\n",
      "====> Epoch: 227 Average train loss: 0.0102459930\n",
      "====> Epoch: 227 Average L0 reg loss: 0.3055032022\n",
      "====> Epoch: 227 Average eval loss: 0.0101962853\n",
      "====> Epoch: 228 Average train loss: 0.0102643925\n",
      "====> Epoch: 228 Average L0 reg loss: 0.3054965329\n",
      "====> Epoch: 228 Average eval loss: 0.0101644658\n",
      "====> Epoch: 229 Average train loss: 0.0102370982\n",
      "====> Epoch: 229 Average L0 reg loss: 0.3054892009\n",
      "====> Epoch: 229 Average eval loss: 0.0101384213\n",
      "====> Epoch: 230 Average train loss: 0.0103379666\n",
      "====> Epoch: 230 Average L0 reg loss: 0.3054806444\n",
      "====> Epoch: 230 Average eval loss: 0.0101219639\n",
      "====> Epoch: 231 Average train loss: 0.0104090369\n",
      "====> Epoch: 231 Average L0 reg loss: 0.3054703810\n",
      "====> Epoch: 231 Average eval loss: 0.0101100346\n",
      "====> Epoch: 232 Average train loss: 0.0103057677\n",
      "====> Epoch: 232 Average L0 reg loss: 0.3054574740\n",
      "====> Epoch: 232 Average eval loss: 0.0101450244\n",
      "====> Epoch: 233 Average train loss: 0.0103738227\n",
      "====> Epoch: 233 Average L0 reg loss: 0.3054410080\n",
      "====> Epoch: 233 Average eval loss: 0.0101129282\n",
      "====> Epoch: 234 Average train loss: 0.0103295037\n",
      "====> Epoch: 234 Average L0 reg loss: 0.3054195594\n",
      "====> Epoch: 234 Average eval loss: 0.0101628704\n",
      "====> Epoch: 235 Average train loss: 0.0102002051\n",
      "====> Epoch: 235 Average L0 reg loss: 0.3053919006\n",
      "====> Epoch: 235 Average eval loss: 0.0102272388\n",
      "====> Epoch: 236 Average train loss: 0.0103302027\n",
      "====> Epoch: 236 Average L0 reg loss: 0.3053562451\n",
      "====> Epoch: 236 Average eval loss: 0.0101441322\n",
      "====> Epoch: 237 Average train loss: 0.0102215726\n",
      "====> Epoch: 237 Average L0 reg loss: 0.3053105471\n",
      "====> Epoch: 237 Average eval loss: 0.0101195257\n",
      "====> Epoch: 238 Average train loss: 0.0103846548\n",
      "====> Epoch: 238 Average L0 reg loss: 0.3052528583\n",
      "====> Epoch: 238 Average eval loss: 0.0101547800\n",
      "====> Epoch: 239 Average train loss: 0.0103864396\n",
      "====> Epoch: 239 Average L0 reg loss: 0.3051814016\n",
      "====> Epoch: 239 Average eval loss: 0.0100949900\n",
      "====> Epoch: 240 Average train loss: 0.0102921835\n",
      "====> Epoch: 240 Average L0 reg loss: 0.3050947249\n",
      "====> Epoch: 240 Average eval loss: 0.0101029007\n",
      "====> Epoch: 241 Average train loss: 0.0102930778\n",
      "====> Epoch: 241 Average L0 reg loss: 0.3049922423\n",
      "====> Epoch: 241 Average eval loss: 0.0101057077\n",
      "====> Epoch: 242 Average train loss: 0.0103710320\n",
      "====> Epoch: 242 Average L0 reg loss: 0.3048741835\n",
      "====> Epoch: 242 Average eval loss: 0.0102462014\n",
      "====> Epoch: 243 Average train loss: 0.0102861889\n",
      "====> Epoch: 243 Average L0 reg loss: 0.3047415695\n",
      "====> Epoch: 243 Average eval loss: 0.0101260971\n",
      "====> Epoch: 244 Average train loss: 0.0101882752\n",
      "====> Epoch: 244 Average L0 reg loss: 0.3045964092\n",
      "====> Epoch: 244 Average eval loss: 0.0101782484\n",
      "====> Epoch: 245 Average train loss: 0.0102624281\n",
      "====> Epoch: 245 Average L0 reg loss: 0.3044414016\n",
      "====> Epoch: 245 Average eval loss: 0.0102049904\n",
      "====> Epoch: 246 Average train loss: 0.0101779381\n",
      "====> Epoch: 246 Average L0 reg loss: 0.3042795144\n",
      "====> Epoch: 246 Average eval loss: 0.0101891095\n",
      "====> Epoch: 247 Average train loss: 0.0104280565\n",
      "====> Epoch: 247 Average L0 reg loss: 0.3041140757\n",
      "====> Epoch: 247 Average eval loss: 0.0102921799\n",
      "====> Epoch: 248 Average train loss: 0.0103322896\n",
      "====> Epoch: 248 Average L0 reg loss: 0.3039487817\n",
      "====> Epoch: 248 Average eval loss: 0.0101497769\n",
      "====> Epoch: 249 Average train loss: 0.0103036899\n",
      "====> Epoch: 249 Average L0 reg loss: 0.3037873405\n",
      "====> Epoch: 249 Average eval loss: 0.0100975027\n",
      "Best testing error sparse FCNN is 0.010094990022480488 and it was found at epoch 239\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:16:26.926555Z",
     "start_time": "2024-05-23T08:08:18.481202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "degree = 3\n",
    "reg_coefficient = 0.01\n",
    "l0sindy_model = L0SINDy_dynamics(input_dim=obs_dim+act_dim, output_dim=obs_dim, degree=degree, lambda_coeff=reg_coefficient)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    l0sindy_model = l0sindy_model.cuda()\n",
    "\n",
    "optimizer_fcnn = torch.optim.Adam([\n",
    "    {'params': l0sindy_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_l0sindy = train_eval_dynamics_model(l0sindy_model, optimizer_fcnn, training_buffer, testing_buffer, batch_size, num_epochs, l0=True)\n",
    "print(\"Best testing error L0 SINDy is {} and it was found at epoch {}\".format(metrics_l0sindy[2], metrics_l0sindy[3]))\n",
    "\n",
    "l0sindy_model.print_equations()\n"
   ],
   "id": "94717fb5ef5019b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy polynomial of order  3\n",
      "with 35 coefficients\n",
      "['1' 'x0' 'x1' 'x2' 'x3' 'x0^2' 'x0 x1' 'x0 x2' 'x0 x3' 'x1^2' 'x1 x2'\n",
      " 'x1 x3' 'x2^2' 'x2 x3' 'x3^2' 'x0^3' 'x0^2 x1' 'x0^2 x2' 'x0^2 x3'\n",
      " 'x0 x1^2' 'x0 x1 x2' 'x0 x1 x3' 'x0 x2^2' 'x0 x2 x3' 'x0 x3^2' 'x1^3'\n",
      " 'x1^2 x2' 'x1^2 x3' 'x1 x2^2' 'x1 x2 x3' 'x1 x3^2' 'x2^3' 'x2^2 x3'\n",
      " 'x2 x3^2' 'x3^3']\n",
      "L0Dense(35 -> 1, droprate_init=0.5, lamba=0.01, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False, bias=False)\n",
      "L0Dense(35 -> 1, droprate_init=0.5, lamba=0.01, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False, bias=False)\n",
      "L0Dense(35 -> 1, droprate_init=0.5, lamba=0.01, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False, bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/Documents/Sparsifying-Parametric-Models-with-L0/l0_layer.py:51: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.weights, mode='fan_out')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average train loss: 319.7959352491\n",
      "====> Epoch: 0 Average L0 reg loss: 0.8727923302\n",
      "====> Epoch: 0 Average eval loss: 54.8959655762\n",
      "====> Epoch: 1 Average train loss: 64.4014845885\n",
      "====> Epoch: 1 Average L0 reg loss: 0.8736853299\n",
      "====> Epoch: 1 Average eval loss: 8.9632978439\n",
      "====> Epoch: 2 Average train loss: 17.7100030226\n",
      "====> Epoch: 2 Average L0 reg loss: 0.8734335920\n",
      "====> Epoch: 2 Average eval loss: 4.2931256294\n",
      "====> Epoch: 3 Average train loss: 8.6056048129\n",
      "====> Epoch: 3 Average L0 reg loss: 0.8712947539\n",
      "====> Epoch: 3 Average eval loss: 2.9521474838\n",
      "====> Epoch: 4 Average train loss: 5.3963759893\n",
      "====> Epoch: 4 Average L0 reg loss: 0.8675687006\n",
      "====> Epoch: 4 Average eval loss: 2.0385997295\n",
      "====> Epoch: 5 Average train loss: 3.5526521243\n",
      "====> Epoch: 5 Average L0 reg loss: 0.8621523715\n",
      "====> Epoch: 5 Average eval loss: 1.3991067410\n",
      "====> Epoch: 6 Average train loss: 2.4377582501\n",
      "====> Epoch: 6 Average L0 reg loss: 0.8551133633\n",
      "====> Epoch: 6 Average eval loss: 1.1068958044\n",
      "====> Epoch: 7 Average train loss: 1.8958958155\n",
      "====> Epoch: 7 Average L0 reg loss: 0.8461994135\n",
      "====> Epoch: 7 Average eval loss: 0.7079518437\n",
      "====> Epoch: 8 Average train loss: 1.3710837264\n",
      "====> Epoch: 8 Average L0 reg loss: 0.8351652544\n",
      "====> Epoch: 8 Average eval loss: 0.4625417292\n",
      "====> Epoch: 9 Average train loss: 1.1591162768\n",
      "====> Epoch: 9 Average L0 reg loss: 0.8215983382\n",
      "====> Epoch: 9 Average eval loss: 0.3013551235\n",
      "====> Epoch: 10 Average train loss: 0.9599502740\n",
      "====> Epoch: 10 Average L0 reg loss: 0.8051086766\n",
      "====> Epoch: 10 Average eval loss: 0.1517590731\n",
      "====> Epoch: 11 Average train loss: 0.8549765864\n",
      "====> Epoch: 11 Average L0 reg loss: 0.7853279453\n",
      "====> Epoch: 11 Average eval loss: 0.1072831228\n",
      "====> Epoch: 12 Average train loss: 0.7091578338\n",
      "====> Epoch: 12 Average L0 reg loss: 0.7622731363\n",
      "====> Epoch: 12 Average eval loss: 0.0786578134\n",
      "====> Epoch: 13 Average train loss: 0.6621791001\n",
      "====> Epoch: 13 Average L0 reg loss: 0.7360940633\n",
      "====> Epoch: 13 Average eval loss: 0.0524703339\n",
      "====> Epoch: 14 Average train loss: 0.5887791982\n",
      "====> Epoch: 14 Average L0 reg loss: 0.7073421786\n",
      "====> Epoch: 14 Average eval loss: 0.0405512080\n",
      "====> Epoch: 15 Average train loss: 0.6017106890\n",
      "====> Epoch: 15 Average L0 reg loss: 0.6764101852\n",
      "====> Epoch: 15 Average eval loss: 0.0341654681\n",
      "====> Epoch: 16 Average train loss: 0.5357167358\n",
      "====> Epoch: 16 Average L0 reg loss: 0.6437877944\n",
      "====> Epoch: 16 Average eval loss: 0.0483262166\n",
      "====> Epoch: 17 Average train loss: 0.4987173124\n",
      "====> Epoch: 17 Average L0 reg loss: 0.6101541117\n",
      "====> Epoch: 17 Average eval loss: 0.0256633647\n",
      "====> Epoch: 18 Average train loss: 0.4600850348\n",
      "====> Epoch: 18 Average L0 reg loss: 0.5759026484\n",
      "====> Epoch: 18 Average eval loss: 0.0328948759\n",
      "====> Epoch: 19 Average train loss: 0.4559901499\n",
      "====> Epoch: 19 Average L0 reg loss: 0.5415685347\n",
      "====> Epoch: 19 Average eval loss: 0.0512370057\n",
      "====> Epoch: 20 Average train loss: 0.4162078256\n",
      "====> Epoch: 20 Average L0 reg loss: 0.5077870963\n",
      "====> Epoch: 20 Average eval loss: 0.0308180302\n",
      "====> Epoch: 21 Average train loss: 0.4127391364\n",
      "====> Epoch: 21 Average L0 reg loss: 0.4749281247\n",
      "====> Epoch: 21 Average eval loss: 0.0695022792\n",
      "====> Epoch: 22 Average train loss: 0.3699165348\n",
      "====> Epoch: 22 Average L0 reg loss: 0.4433161661\n",
      "====> Epoch: 22 Average eval loss: 0.0394514911\n",
      "====> Epoch: 23 Average train loss: 0.3894191930\n",
      "====> Epoch: 23 Average L0 reg loss: 0.4131252294\n",
      "====> Epoch: 23 Average eval loss: 0.0520818383\n",
      "====> Epoch: 24 Average train loss: 0.3164024191\n",
      "====> Epoch: 24 Average L0 reg loss: 0.3846225294\n",
      "====> Epoch: 24 Average eval loss: 0.0488339923\n",
      "====> Epoch: 25 Average train loss: 0.3453644465\n",
      "====> Epoch: 25 Average L0 reg loss: 0.3580141693\n",
      "====> Epoch: 25 Average eval loss: 0.0686489344\n",
      "====> Epoch: 26 Average train loss: 0.3000347125\n",
      "====> Epoch: 26 Average L0 reg loss: 0.3334251446\n",
      "====> Epoch: 26 Average eval loss: 0.0804789513\n",
      "====> Epoch: 27 Average train loss: 0.2808147784\n",
      "====> Epoch: 27 Average L0 reg loss: 0.3110491374\n",
      "====> Epoch: 27 Average eval loss: 0.0479508303\n",
      "====> Epoch: 28 Average train loss: 0.2635194059\n",
      "====> Epoch: 28 Average L0 reg loss: 0.2906746569\n",
      "====> Epoch: 28 Average eval loss: 0.1008180976\n",
      "====> Epoch: 29 Average train loss: 0.2524839021\n",
      "====> Epoch: 29 Average L0 reg loss: 0.2722710298\n",
      "====> Epoch: 29 Average eval loss: 0.0691109374\n",
      "====> Epoch: 30 Average train loss: 0.2406656201\n",
      "====> Epoch: 30 Average L0 reg loss: 0.2558102497\n",
      "====> Epoch: 30 Average eval loss: 0.0788589418\n",
      "====> Epoch: 31 Average train loss: 0.2510473330\n",
      "====> Epoch: 31 Average L0 reg loss: 0.2411981360\n",
      "====> Epoch: 31 Average eval loss: 0.0825909525\n",
      "====> Epoch: 32 Average train loss: 0.2246132973\n",
      "====> Epoch: 32 Average L0 reg loss: 0.2283491318\n",
      "====> Epoch: 32 Average eval loss: 0.0455105565\n",
      "====> Epoch: 33 Average train loss: 0.1888802126\n",
      "====> Epoch: 33 Average L0 reg loss: 0.2169858004\n",
      "====> Epoch: 33 Average eval loss: 0.0493093058\n",
      "====> Epoch: 34 Average train loss: 0.2000726522\n",
      "====> Epoch: 34 Average L0 reg loss: 0.2069307582\n",
      "====> Epoch: 34 Average eval loss: 0.0509460419\n",
      "====> Epoch: 35 Average train loss: 0.1776239305\n",
      "====> Epoch: 35 Average L0 reg loss: 0.1981146393\n",
      "====> Epoch: 35 Average eval loss: 0.0440446958\n",
      "====> Epoch: 36 Average train loss: 0.1752392311\n",
      "====> Epoch: 36 Average L0 reg loss: 0.1903209180\n",
      "====> Epoch: 36 Average eval loss: 0.0655560493\n",
      "====> Epoch: 37 Average train loss: 0.1688068977\n",
      "====> Epoch: 37 Average L0 reg loss: 0.1834412245\n",
      "====> Epoch: 37 Average eval loss: 0.0370691866\n",
      "====> Epoch: 38 Average train loss: 0.1571667098\n",
      "====> Epoch: 38 Average L0 reg loss: 0.1774182857\n",
      "====> Epoch: 38 Average eval loss: 0.0351962745\n",
      "====> Epoch: 39 Average train loss: 0.1517957704\n",
      "====> Epoch: 39 Average L0 reg loss: 0.1719645996\n",
      "====> Epoch: 39 Average eval loss: 0.0391708314\n",
      "====> Epoch: 40 Average train loss: 0.1533457893\n",
      "====> Epoch: 40 Average L0 reg loss: 0.1670771121\n",
      "====> Epoch: 40 Average eval loss: 0.0451075248\n",
      "====> Epoch: 41 Average train loss: 0.1258516952\n",
      "====> Epoch: 41 Average L0 reg loss: 0.1626180293\n",
      "====> Epoch: 41 Average eval loss: 0.0301767997\n",
      "====> Epoch: 42 Average train loss: 0.1395680201\n",
      "====> Epoch: 42 Average L0 reg loss: 0.1585538947\n",
      "====> Epoch: 42 Average eval loss: 0.0358317159\n",
      "====> Epoch: 43 Average train loss: 0.1312927768\n",
      "====> Epoch: 43 Average L0 reg loss: 0.1547689578\n",
      "====> Epoch: 43 Average eval loss: 0.0505161285\n",
      "====> Epoch: 44 Average train loss: 0.1231138766\n",
      "====> Epoch: 44 Average L0 reg loss: 0.1513506663\n",
      "====> Epoch: 44 Average eval loss: 0.0491833985\n",
      "====> Epoch: 45 Average train loss: 0.1112457707\n",
      "====> Epoch: 45 Average L0 reg loss: 0.1482445758\n",
      "====> Epoch: 45 Average eval loss: 0.0318439603\n",
      "====> Epoch: 46 Average train loss: 0.0987115365\n",
      "====> Epoch: 46 Average L0 reg loss: 0.1453628481\n",
      "====> Epoch: 46 Average eval loss: 0.0230346769\n",
      "====> Epoch: 47 Average train loss: 0.0909494135\n",
      "====> Epoch: 47 Average L0 reg loss: 0.1427570186\n",
      "====> Epoch: 47 Average eval loss: 0.0236941371\n",
      "====> Epoch: 48 Average train loss: 0.0960158735\n",
      "====> Epoch: 48 Average L0 reg loss: 0.1404243944\n",
      "====> Epoch: 48 Average eval loss: 0.0195461996\n",
      "====> Epoch: 49 Average train loss: 0.1104410880\n",
      "====> Epoch: 49 Average L0 reg loss: 0.1382650296\n",
      "====> Epoch: 49 Average eval loss: 0.0234303679\n",
      "====> Epoch: 50 Average train loss: 0.0900539678\n",
      "====> Epoch: 50 Average L0 reg loss: 0.1362896598\n",
      "====> Epoch: 50 Average eval loss: 0.0201988332\n",
      "====> Epoch: 51 Average train loss: 0.0928403642\n",
      "====> Epoch: 51 Average L0 reg loss: 0.1344528109\n",
      "====> Epoch: 51 Average eval loss: 0.0219701752\n",
      "====> Epoch: 52 Average train loss: 0.0757729580\n",
      "====> Epoch: 52 Average L0 reg loss: 0.1326583944\n",
      "====> Epoch: 52 Average eval loss: 0.0219117161\n",
      "====> Epoch: 53 Average train loss: 0.0813273209\n",
      "====> Epoch: 53 Average L0 reg loss: 0.1308923890\n",
      "====> Epoch: 53 Average eval loss: 0.0194109045\n",
      "====> Epoch: 54 Average train loss: 0.0819648799\n",
      "====> Epoch: 54 Average L0 reg loss: 0.1292176021\n",
      "====> Epoch: 54 Average eval loss: 0.0195243116\n",
      "====> Epoch: 55 Average train loss: 0.0766548816\n",
      "====> Epoch: 55 Average L0 reg loss: 0.1275106828\n",
      "====> Epoch: 55 Average eval loss: 0.0187660661\n",
      "====> Epoch: 56 Average train loss: 0.0702634984\n",
      "====> Epoch: 56 Average L0 reg loss: 0.1257771533\n",
      "====> Epoch: 56 Average eval loss: 0.0188320596\n",
      "====> Epoch: 57 Average train loss: 0.0764590594\n",
      "====> Epoch: 57 Average L0 reg loss: 0.1240440425\n",
      "====> Epoch: 57 Average eval loss: 0.0208019093\n",
      "====> Epoch: 58 Average train loss: 0.0728155295\n",
      "====> Epoch: 58 Average L0 reg loss: 0.1223580051\n",
      "====> Epoch: 58 Average eval loss: 0.0202923268\n",
      "====> Epoch: 59 Average train loss: 0.0677780150\n",
      "====> Epoch: 59 Average L0 reg loss: 0.1207009302\n",
      "====> Epoch: 59 Average eval loss: 0.0208884887\n",
      "====> Epoch: 60 Average train loss: 0.0651904890\n",
      "====> Epoch: 60 Average L0 reg loss: 0.1190421835\n",
      "====> Epoch: 60 Average eval loss: 0.0199691970\n",
      "====> Epoch: 61 Average train loss: 0.0564289924\n",
      "====> Epoch: 61 Average L0 reg loss: 0.1174529398\n",
      "====> Epoch: 61 Average eval loss: 0.0190583896\n",
      "====> Epoch: 62 Average train loss: 0.0628088277\n",
      "====> Epoch: 62 Average L0 reg loss: 0.1159489849\n",
      "====> Epoch: 62 Average eval loss: 0.0191733260\n",
      "====> Epoch: 63 Average train loss: 0.0611663553\n",
      "====> Epoch: 63 Average L0 reg loss: 0.1145528168\n",
      "====> Epoch: 63 Average eval loss: 0.0193483531\n",
      "====> Epoch: 64 Average train loss: 0.0683100034\n",
      "====> Epoch: 64 Average L0 reg loss: 0.1132382867\n",
      "====> Epoch: 64 Average eval loss: 0.0207922570\n",
      "====> Epoch: 65 Average train loss: 0.0708996732\n",
      "====> Epoch: 65 Average L0 reg loss: 0.1120860654\n",
      "====> Epoch: 65 Average eval loss: 0.0209041014\n",
      "====> Epoch: 66 Average train loss: 0.0621048407\n",
      "====> Epoch: 66 Average L0 reg loss: 0.1110460954\n",
      "====> Epoch: 66 Average eval loss: 0.0205847975\n",
      "====> Epoch: 67 Average train loss: 0.0533663553\n",
      "====> Epoch: 67 Average L0 reg loss: 0.1100761729\n",
      "====> Epoch: 67 Average eval loss: 0.0190469176\n",
      "====> Epoch: 68 Average train loss: 0.0497024645\n",
      "====> Epoch: 68 Average L0 reg loss: 0.1092042825\n",
      "====> Epoch: 68 Average eval loss: 0.0189446304\n",
      "====> Epoch: 69 Average train loss: 0.0587315667\n",
      "====> Epoch: 69 Average L0 reg loss: 0.1084697858\n",
      "====> Epoch: 69 Average eval loss: 0.0186838619\n",
      "====> Epoch: 70 Average train loss: 0.0645603538\n",
      "====> Epoch: 70 Average L0 reg loss: 0.1078598809\n",
      "====> Epoch: 70 Average eval loss: 0.0200841147\n",
      "====> Epoch: 71 Average train loss: 0.0464297952\n",
      "====> Epoch: 71 Average L0 reg loss: 0.1073007221\n",
      "====> Epoch: 71 Average eval loss: 0.0188266318\n",
      "====> Epoch: 72 Average train loss: 0.0473051757\n",
      "====> Epoch: 72 Average L0 reg loss: 0.1067868565\n",
      "====> Epoch: 72 Average eval loss: 0.0196368117\n",
      "====> Epoch: 73 Average train loss: 0.0423795009\n",
      "====> Epoch: 73 Average L0 reg loss: 0.1062679405\n",
      "====> Epoch: 73 Average eval loss: 0.0185573827\n",
      "====> Epoch: 74 Average train loss: 0.0490963899\n",
      "====> Epoch: 74 Average L0 reg loss: 0.1057702172\n",
      "====> Epoch: 74 Average eval loss: 0.0188805684\n",
      "====> Epoch: 75 Average train loss: 0.0464536384\n",
      "====> Epoch: 75 Average L0 reg loss: 0.1052815580\n",
      "====> Epoch: 75 Average eval loss: 0.0183722209\n",
      "====> Epoch: 76 Average train loss: 0.0456627228\n",
      "====> Epoch: 76 Average L0 reg loss: 0.1048571846\n",
      "====> Epoch: 76 Average eval loss: 0.0186091550\n",
      "====> Epoch: 77 Average train loss: 0.0436168438\n",
      "====> Epoch: 77 Average L0 reg loss: 0.1044511141\n",
      "====> Epoch: 77 Average eval loss: 0.0188163035\n",
      "====> Epoch: 78 Average train loss: 0.0447623900\n",
      "====> Epoch: 78 Average L0 reg loss: 0.1041753632\n",
      "====> Epoch: 78 Average eval loss: 0.0196525957\n",
      "====> Epoch: 79 Average train loss: 0.0431031936\n",
      "====> Epoch: 79 Average L0 reg loss: 0.1039225956\n",
      "====> Epoch: 79 Average eval loss: 0.0194641110\n",
      "====> Epoch: 80 Average train loss: 0.0416880014\n",
      "====> Epoch: 80 Average L0 reg loss: 0.1036678016\n",
      "====> Epoch: 80 Average eval loss: 0.0184531845\n",
      "====> Epoch: 81 Average train loss: 0.0435337072\n",
      "====> Epoch: 81 Average L0 reg loss: 0.1033955645\n",
      "====> Epoch: 81 Average eval loss: 0.0189611036\n",
      "====> Epoch: 82 Average train loss: 0.0393132445\n",
      "====> Epoch: 82 Average L0 reg loss: 0.1031244252\n",
      "====> Epoch: 82 Average eval loss: 0.0184403341\n",
      "====> Epoch: 83 Average train loss: 0.0428300525\n",
      "====> Epoch: 83 Average L0 reg loss: 0.1029362604\n",
      "====> Epoch: 83 Average eval loss: 0.0187871698\n",
      "====> Epoch: 84 Average train loss: 0.0404041812\n",
      "====> Epoch: 84 Average L0 reg loss: 0.1027902127\n",
      "====> Epoch: 84 Average eval loss: 0.0188020095\n",
      "====> Epoch: 85 Average train loss: 0.0349439901\n",
      "====> Epoch: 85 Average L0 reg loss: 0.1026539838\n",
      "====> Epoch: 85 Average eval loss: 0.0181843098\n",
      "====> Epoch: 86 Average train loss: 0.0356457902\n",
      "====> Epoch: 86 Average L0 reg loss: 0.1025268908\n",
      "====> Epoch: 86 Average eval loss: 0.0185183287\n",
      "====> Epoch: 87 Average train loss: 0.0288920581\n",
      "====> Epoch: 87 Average L0 reg loss: 0.1023949305\n",
      "====> Epoch: 87 Average eval loss: 0.0181872491\n",
      "====> Epoch: 88 Average train loss: 0.0371653206\n",
      "====> Epoch: 88 Average L0 reg loss: 0.1022431215\n",
      "====> Epoch: 88 Average eval loss: 0.0185085088\n",
      "====> Epoch: 89 Average train loss: 0.0352420937\n",
      "====> Epoch: 89 Average L0 reg loss: 0.1020787884\n",
      "====> Epoch: 89 Average eval loss: 0.0182162561\n",
      "====> Epoch: 90 Average train loss: 0.0424900529\n",
      "====> Epoch: 90 Average L0 reg loss: 0.1019078882\n",
      "====> Epoch: 90 Average eval loss: 0.0182625726\n",
      "====> Epoch: 91 Average train loss: 0.0336852745\n",
      "====> Epoch: 91 Average L0 reg loss: 0.1018159117\n",
      "====> Epoch: 91 Average eval loss: 0.0184644461\n",
      "====> Epoch: 92 Average train loss: 0.0384107100\n",
      "====> Epoch: 92 Average L0 reg loss: 0.1017418857\n",
      "====> Epoch: 92 Average eval loss: 0.0183996111\n",
      "====> Epoch: 93 Average train loss: 0.0330682647\n",
      "====> Epoch: 93 Average L0 reg loss: 0.1016580217\n",
      "====> Epoch: 93 Average eval loss: 0.0182732977\n",
      "====> Epoch: 94 Average train loss: 0.0280847566\n",
      "====> Epoch: 94 Average L0 reg loss: 0.1015704944\n",
      "====> Epoch: 94 Average eval loss: 0.0185197946\n",
      "====> Epoch: 95 Average train loss: 0.0301416389\n",
      "====> Epoch: 95 Average L0 reg loss: 0.1015012888\n",
      "====> Epoch: 95 Average eval loss: 0.0185392480\n",
      "====> Epoch: 96 Average train loss: 0.0339744454\n",
      "====> Epoch: 96 Average L0 reg loss: 0.1014211625\n",
      "====> Epoch: 96 Average eval loss: 0.0183409061\n",
      "====> Epoch: 97 Average train loss: 0.0308572962\n",
      "====> Epoch: 97 Average L0 reg loss: 0.1013298713\n",
      "====> Epoch: 97 Average eval loss: 0.0183281936\n",
      "====> Epoch: 98 Average train loss: 0.0329985631\n",
      "====> Epoch: 98 Average L0 reg loss: 0.1012806820\n",
      "====> Epoch: 98 Average eval loss: 0.0185651723\n",
      "====> Epoch: 99 Average train loss: 0.0312782225\n",
      "====> Epoch: 99 Average L0 reg loss: 0.1012371954\n",
      "====> Epoch: 99 Average eval loss: 0.0182751901\n",
      "====> Epoch: 100 Average train loss: 0.0329522752\n",
      "====> Epoch: 100 Average L0 reg loss: 0.1011955756\n",
      "====> Epoch: 100 Average eval loss: 0.0182891078\n",
      "====> Epoch: 101 Average train loss: 0.0300969455\n",
      "====> Epoch: 101 Average L0 reg loss: 0.1011492448\n",
      "====> Epoch: 101 Average eval loss: 0.0184515174\n",
      "====> Epoch: 102 Average train loss: 0.0309973617\n",
      "====> Epoch: 102 Average L0 reg loss: 0.1011024461\n",
      "====> Epoch: 102 Average eval loss: 0.0182300247\n",
      "====> Epoch: 103 Average train loss: 0.0312042370\n",
      "====> Epoch: 103 Average L0 reg loss: 0.1010533662\n",
      "====> Epoch: 103 Average eval loss: 0.0182030033\n",
      "====> Epoch: 104 Average train loss: 0.0316802442\n",
      "====> Epoch: 104 Average L0 reg loss: 0.1010134327\n",
      "====> Epoch: 104 Average eval loss: 0.0181893520\n",
      "====> Epoch: 105 Average train loss: 0.0265239696\n",
      "====> Epoch: 105 Average L0 reg loss: 0.1009796016\n",
      "====> Epoch: 105 Average eval loss: 0.0182727464\n",
      "====> Epoch: 106 Average train loss: 0.0298450172\n",
      "====> Epoch: 106 Average L0 reg loss: 0.1009341845\n",
      "====> Epoch: 106 Average eval loss: 0.0181495249\n",
      "====> Epoch: 107 Average train loss: 0.0295180714\n",
      "====> Epoch: 107 Average L0 reg loss: 0.1008949650\n",
      "====> Epoch: 107 Average eval loss: 0.0182253271\n",
      "====> Epoch: 108 Average train loss: 0.0307900694\n",
      "====> Epoch: 108 Average L0 reg loss: 0.1008625951\n",
      "====> Epoch: 108 Average eval loss: 0.0181804616\n",
      "====> Epoch: 109 Average train loss: 0.0292804931\n",
      "====> Epoch: 109 Average L0 reg loss: 0.1008372766\n",
      "====> Epoch: 109 Average eval loss: 0.0183209591\n",
      "====> Epoch: 110 Average train loss: 0.0353772685\n",
      "====> Epoch: 110 Average L0 reg loss: 0.1008200126\n",
      "====> Epoch: 110 Average eval loss: 0.0182610005\n",
      "====> Epoch: 111 Average train loss: 0.0274535063\n",
      "====> Epoch: 111 Average L0 reg loss: 0.1008034669\n",
      "====> Epoch: 111 Average eval loss: 0.0181999374\n",
      "====> Epoch: 112 Average train loss: 0.0299378947\n",
      "====> Epoch: 112 Average L0 reg loss: 0.1007873345\n",
      "====> Epoch: 112 Average eval loss: 0.0181866586\n",
      "====> Epoch: 113 Average train loss: 0.0308773219\n",
      "====> Epoch: 113 Average L0 reg loss: 0.1007656788\n",
      "====> Epoch: 113 Average eval loss: 0.0187186077\n",
      "====> Epoch: 114 Average train loss: 0.0276289655\n",
      "====> Epoch: 114 Average L0 reg loss: 0.1007389206\n",
      "====> Epoch: 114 Average eval loss: 0.0181484129\n",
      "====> Epoch: 115 Average train loss: 0.0295339545\n",
      "====> Epoch: 115 Average L0 reg loss: 0.1007226317\n",
      "====> Epoch: 115 Average eval loss: 0.0182448085\n",
      "====> Epoch: 116 Average train loss: 0.0254980640\n",
      "====> Epoch: 116 Average L0 reg loss: 0.1007084261\n",
      "====> Epoch: 116 Average eval loss: 0.0181879420\n",
      "====> Epoch: 117 Average train loss: 0.0287012528\n",
      "====> Epoch: 117 Average L0 reg loss: 0.1006927975\n",
      "====> Epoch: 117 Average eval loss: 0.0181819387\n",
      "====> Epoch: 118 Average train loss: 0.0261306851\n",
      "====> Epoch: 118 Average L0 reg loss: 0.1006699993\n",
      "====> Epoch: 118 Average eval loss: 0.0181505848\n",
      "====> Epoch: 119 Average train loss: 0.0281034139\n",
      "====> Epoch: 119 Average L0 reg loss: 0.1006523974\n",
      "====> Epoch: 119 Average eval loss: 0.0181547292\n",
      "====> Epoch: 120 Average train loss: 0.0260720823\n",
      "====> Epoch: 120 Average L0 reg loss: 0.1006357424\n",
      "====> Epoch: 120 Average eval loss: 0.0181710254\n",
      "====> Epoch: 121 Average train loss: 0.0309719314\n",
      "====> Epoch: 121 Average L0 reg loss: 0.1006216555\n",
      "====> Epoch: 121 Average eval loss: 0.0184774566\n",
      "====> Epoch: 122 Average train loss: 0.0246128170\n",
      "====> Epoch: 122 Average L0 reg loss: 0.1006014444\n",
      "====> Epoch: 122 Average eval loss: 0.0181581378\n",
      "====> Epoch: 123 Average train loss: 0.0243930556\n",
      "====> Epoch: 123 Average L0 reg loss: 0.1005777030\n",
      "====> Epoch: 123 Average eval loss: 0.0181501191\n",
      "====> Epoch: 124 Average train loss: 0.0232549959\n",
      "====> Epoch: 124 Average L0 reg loss: 0.1005571649\n",
      "====> Epoch: 124 Average eval loss: 0.0182041377\n",
      "====> Epoch: 125 Average train loss: 0.0243013699\n",
      "====> Epoch: 125 Average L0 reg loss: 0.1005417341\n",
      "====> Epoch: 125 Average eval loss: 0.0185278282\n",
      "====> Epoch: 126 Average train loss: 0.0260776791\n",
      "====> Epoch: 126 Average L0 reg loss: 0.1005256244\n",
      "====> Epoch: 126 Average eval loss: 0.0181646869\n",
      "====> Epoch: 127 Average train loss: 0.0286028609\n",
      "====> Epoch: 127 Average L0 reg loss: 0.1005166715\n",
      "====> Epoch: 127 Average eval loss: 0.0181856733\n",
      "====> Epoch: 128 Average train loss: 0.0242823141\n",
      "====> Epoch: 128 Average L0 reg loss: 0.1005043899\n",
      "====> Epoch: 128 Average eval loss: 0.0181498993\n",
      "====> Epoch: 129 Average train loss: 0.0254524339\n",
      "====> Epoch: 129 Average L0 reg loss: 0.1004872000\n",
      "====> Epoch: 129 Average eval loss: 0.0181800500\n",
      "====> Epoch: 130 Average train loss: 0.0301132141\n",
      "====> Epoch: 130 Average L0 reg loss: 0.1004743943\n",
      "====> Epoch: 130 Average eval loss: 0.0183420815\n",
      "====> Epoch: 131 Average train loss: 0.0222334098\n",
      "====> Epoch: 131 Average L0 reg loss: 0.1004635301\n",
      "====> Epoch: 131 Average eval loss: 0.0181499068\n",
      "====> Epoch: 132 Average train loss: 0.0278129363\n",
      "====> Epoch: 132 Average L0 reg loss: 0.1004479261\n",
      "====> Epoch: 132 Average eval loss: 0.0181730259\n",
      "====> Epoch: 133 Average train loss: 0.0245331452\n",
      "====> Epoch: 133 Average L0 reg loss: 0.1004305479\n",
      "====> Epoch: 133 Average eval loss: 0.0181610696\n",
      "====> Epoch: 134 Average train loss: 0.0302630267\n",
      "====> Epoch: 134 Average L0 reg loss: 0.1004210698\n",
      "====> Epoch: 134 Average eval loss: 0.0181520749\n",
      "====> Epoch: 135 Average train loss: 0.0229979825\n",
      "====> Epoch: 135 Average L0 reg loss: 0.1004140982\n",
      "====> Epoch: 135 Average eval loss: 0.0182543360\n",
      "====> Epoch: 136 Average train loss: 0.0272685077\n",
      "====> Epoch: 136 Average L0 reg loss: 0.1004064408\n",
      "====> Epoch: 136 Average eval loss: 0.0182261821\n",
      "====> Epoch: 137 Average train loss: 0.0243257786\n",
      "====> Epoch: 137 Average L0 reg loss: 0.1003999075\n",
      "====> Epoch: 137 Average eval loss: 0.0181500576\n",
      "====> Epoch: 138 Average train loss: 0.0269182570\n",
      "====> Epoch: 138 Average L0 reg loss: 0.1003931397\n",
      "====> Epoch: 138 Average eval loss: 0.0181587767\n",
      "====> Epoch: 139 Average train loss: 0.0317847701\n",
      "====> Epoch: 139 Average L0 reg loss: 0.1003859933\n",
      "====> Epoch: 139 Average eval loss: 0.0185105223\n",
      "====> Epoch: 140 Average train loss: 0.0234635297\n",
      "====> Epoch: 140 Average L0 reg loss: 0.1003806510\n",
      "====> Epoch: 140 Average eval loss: 0.0182132572\n",
      "====> Epoch: 141 Average train loss: 0.0208304642\n",
      "====> Epoch: 141 Average L0 reg loss: 0.1003741104\n",
      "====> Epoch: 141 Average eval loss: 0.0181553122\n",
      "====> Epoch: 142 Average train loss: 0.0268943516\n",
      "====> Epoch: 142 Average L0 reg loss: 0.1003674351\n",
      "====> Epoch: 142 Average eval loss: 0.0181487631\n",
      "====> Epoch: 143 Average train loss: 0.0290911175\n",
      "====> Epoch: 143 Average L0 reg loss: 0.1003591690\n",
      "====> Epoch: 143 Average eval loss: 0.0182350054\n",
      "====> Epoch: 144 Average train loss: 0.0266287984\n",
      "====> Epoch: 144 Average L0 reg loss: 0.1003503820\n",
      "====> Epoch: 144 Average eval loss: 0.0182016864\n",
      "====> Epoch: 145 Average train loss: 0.0248289092\n",
      "====> Epoch: 145 Average L0 reg loss: 0.1003421115\n",
      "====> Epoch: 145 Average eval loss: 0.0181550030\n",
      "====> Epoch: 146 Average train loss: 0.0230842572\n",
      "====> Epoch: 146 Average L0 reg loss: 0.1003313968\n",
      "====> Epoch: 146 Average eval loss: 0.0181584079\n",
      "====> Epoch: 147 Average train loss: 0.0239741010\n",
      "====> Epoch: 147 Average L0 reg loss: 0.1003201279\n",
      "====> Epoch: 147 Average eval loss: 0.0182746612\n",
      "====> Epoch: 148 Average train loss: 0.0214248155\n",
      "====> Epoch: 148 Average L0 reg loss: 0.1003107611\n",
      "====> Epoch: 148 Average eval loss: 0.0181470104\n",
      "====> Epoch: 149 Average train loss: 0.0246935821\n",
      "====> Epoch: 149 Average L0 reg loss: 0.1003056632\n",
      "====> Epoch: 149 Average eval loss: 0.0184322316\n",
      "====> Epoch: 150 Average train loss: 0.0265190579\n",
      "====> Epoch: 150 Average L0 reg loss: 0.1002993111\n",
      "====> Epoch: 150 Average eval loss: 0.0181495193\n",
      "====> Epoch: 151 Average train loss: 0.0245033346\n",
      "====> Epoch: 151 Average L0 reg loss: 0.1002960313\n",
      "====> Epoch: 151 Average eval loss: 0.0182339251\n",
      "====> Epoch: 152 Average train loss: 0.0237164903\n",
      "====> Epoch: 152 Average L0 reg loss: 0.1002931390\n",
      "====> Epoch: 152 Average eval loss: 0.0182134770\n",
      "====> Epoch: 153 Average train loss: 0.0225297116\n",
      "====> Epoch: 153 Average L0 reg loss: 0.1002905811\n",
      "====> Epoch: 153 Average eval loss: 0.0181701425\n",
      "====> Epoch: 154 Average train loss: 0.0263306455\n",
      "====> Epoch: 154 Average L0 reg loss: 0.1002866503\n",
      "====> Epoch: 154 Average eval loss: 0.0181674324\n",
      "====> Epoch: 155 Average train loss: 0.0209447317\n",
      "====> Epoch: 155 Average L0 reg loss: 0.1002812399\n",
      "====> Epoch: 155 Average eval loss: 0.0181474574\n",
      "====> Epoch: 156 Average train loss: 0.0223340065\n",
      "====> Epoch: 156 Average L0 reg loss: 0.1002745481\n",
      "====> Epoch: 156 Average eval loss: 0.0181913096\n",
      "====> Epoch: 157 Average train loss: 0.0255296086\n",
      "====> Epoch: 157 Average L0 reg loss: 0.1002673356\n",
      "====> Epoch: 157 Average eval loss: 0.0181525759\n",
      "====> Epoch: 158 Average train loss: 0.0228642209\n",
      "====> Epoch: 158 Average L0 reg loss: 0.1002592251\n",
      "====> Epoch: 158 Average eval loss: 0.0181440189\n",
      "====> Epoch: 159 Average train loss: 0.0275138664\n",
      "====> Epoch: 159 Average L0 reg loss: 0.1002543339\n",
      "====> Epoch: 159 Average eval loss: 0.0181527995\n",
      "====> Epoch: 160 Average train loss: 0.0262234127\n",
      "====> Epoch: 160 Average L0 reg loss: 0.1002491583\n",
      "====> Epoch: 160 Average eval loss: 0.0181799810\n",
      "====> Epoch: 161 Average train loss: 0.0237552792\n",
      "====> Epoch: 161 Average L0 reg loss: 0.1002418226\n",
      "====> Epoch: 161 Average eval loss: 0.0181485228\n",
      "====> Epoch: 162 Average train loss: 0.0254909140\n",
      "====> Epoch: 162 Average L0 reg loss: 0.1002359293\n",
      "====> Epoch: 162 Average eval loss: 0.0181537624\n",
      "====> Epoch: 163 Average train loss: 0.0219881322\n",
      "====> Epoch: 163 Average L0 reg loss: 0.1002332703\n",
      "====> Epoch: 163 Average eval loss: 0.0181506965\n",
      "====> Epoch: 164 Average train loss: 0.0230065738\n",
      "====> Epoch: 164 Average L0 reg loss: 0.1002298594\n",
      "====> Epoch: 164 Average eval loss: 0.0181489326\n",
      "====> Epoch: 165 Average train loss: 0.0236265145\n",
      "====> Epoch: 165 Average L0 reg loss: 0.1002268468\n",
      "====> Epoch: 165 Average eval loss: 0.0183203798\n",
      "====> Epoch: 166 Average train loss: 0.0227900657\n",
      "====> Epoch: 166 Average L0 reg loss: 0.1002247165\n",
      "====> Epoch: 166 Average eval loss: 0.0181569904\n",
      "====> Epoch: 167 Average train loss: 0.0220067094\n",
      "====> Epoch: 167 Average L0 reg loss: 0.1002221953\n",
      "====> Epoch: 167 Average eval loss: 0.0181498937\n",
      "====> Epoch: 168 Average train loss: 0.0242077765\n",
      "====> Epoch: 168 Average L0 reg loss: 0.1002184276\n",
      "====> Epoch: 168 Average eval loss: 0.0181611311\n",
      "====> Epoch: 169 Average train loss: 0.0257864373\n",
      "====> Epoch: 169 Average L0 reg loss: 0.1002140198\n",
      "====> Epoch: 169 Average eval loss: 0.0181693137\n",
      "====> Epoch: 170 Average train loss: 0.0219769651\n",
      "====> Epoch: 170 Average L0 reg loss: 0.1002120659\n",
      "====> Epoch: 170 Average eval loss: 0.0181440711\n",
      "====> Epoch: 171 Average train loss: 0.0208518594\n",
      "====> Epoch: 171 Average L0 reg loss: 0.1002098650\n",
      "====> Epoch: 171 Average eval loss: 0.0182366036\n",
      "====> Epoch: 172 Average train loss: 0.0214108962\n",
      "====> Epoch: 172 Average L0 reg loss: 0.1002081325\n",
      "====> Epoch: 172 Average eval loss: 0.0182293653\n",
      "====> Epoch: 173 Average train loss: 0.0219755386\n",
      "====> Epoch: 173 Average L0 reg loss: 0.1002050512\n",
      "====> Epoch: 173 Average eval loss: 0.0181590952\n",
      "====> Epoch: 174 Average train loss: 0.0257287592\n",
      "====> Epoch: 174 Average L0 reg loss: 0.1002006934\n",
      "====> Epoch: 174 Average eval loss: 0.0181575157\n",
      "====> Epoch: 175 Average train loss: 0.0244710632\n",
      "====> Epoch: 175 Average L0 reg loss: 0.1001971612\n",
      "====> Epoch: 175 Average eval loss: 0.0181801356\n",
      "====> Epoch: 176 Average train loss: 0.0225051503\n",
      "====> Epoch: 176 Average L0 reg loss: 0.1001946050\n",
      "====> Epoch: 176 Average eval loss: 0.0181541573\n",
      "====> Epoch: 177 Average train loss: 0.0250970761\n",
      "====> Epoch: 177 Average L0 reg loss: 0.1001927322\n",
      "====> Epoch: 177 Average eval loss: 0.0183559172\n",
      "====> Epoch: 178 Average train loss: 0.0262151458\n",
      "====> Epoch: 178 Average L0 reg loss: 0.1001903906\n",
      "====> Epoch: 178 Average eval loss: 0.0181518290\n",
      "====> Epoch: 179 Average train loss: 0.0199946127\n",
      "====> Epoch: 179 Average L0 reg loss: 0.1001877592\n",
      "====> Epoch: 179 Average eval loss: 0.0181514006\n",
      "====> Epoch: 180 Average train loss: 0.0207744273\n",
      "====> Epoch: 180 Average L0 reg loss: 0.1001846665\n",
      "====> Epoch: 180 Average eval loss: 0.0181834269\n",
      "====> Epoch: 181 Average train loss: 0.0243540649\n",
      "====> Epoch: 181 Average L0 reg loss: 0.1001828262\n",
      "====> Epoch: 181 Average eval loss: 0.0181533210\n",
      "====> Epoch: 182 Average train loss: 0.0225416420\n",
      "====> Epoch: 182 Average L0 reg loss: 0.1001806085\n",
      "====> Epoch: 182 Average eval loss: 0.0181612838\n",
      "====> Epoch: 183 Average train loss: 0.0203885137\n",
      "====> Epoch: 183 Average L0 reg loss: 0.1001788020\n",
      "====> Epoch: 183 Average eval loss: 0.0183450934\n",
      "====> Epoch: 184 Average train loss: 0.0196887889\n",
      "====> Epoch: 184 Average L0 reg loss: 0.1001769788\n",
      "====> Epoch: 184 Average eval loss: 0.0181536917\n",
      "====> Epoch: 185 Average train loss: 0.0243321231\n",
      "====> Epoch: 185 Average L0 reg loss: 0.1001749277\n",
      "====> Epoch: 185 Average eval loss: 0.0181733705\n",
      "====> Epoch: 186 Average train loss: 0.0216733029\n",
      "====> Epoch: 186 Average L0 reg loss: 0.1001714700\n",
      "====> Epoch: 186 Average eval loss: 0.0181505941\n",
      "====> Epoch: 187 Average train loss: 0.0238501316\n",
      "====> Epoch: 187 Average L0 reg loss: 0.1001691779\n",
      "====> Epoch: 187 Average eval loss: 0.0181583930\n",
      "====> Epoch: 188 Average train loss: 0.0199430904\n",
      "====> Epoch: 188 Average L0 reg loss: 0.1001665280\n",
      "====> Epoch: 188 Average eval loss: 0.0181539841\n",
      "====> Epoch: 189 Average train loss: 0.0241733473\n",
      "====> Epoch: 189 Average L0 reg loss: 0.1001640102\n",
      "====> Epoch: 189 Average eval loss: 0.0181556214\n",
      "====> Epoch: 190 Average train loss: 0.0220966098\n",
      "====> Epoch: 190 Average L0 reg loss: 0.1001617390\n",
      "====> Epoch: 190 Average eval loss: 0.0181561969\n",
      "====> Epoch: 191 Average train loss: 0.0223222060\n",
      "====> Epoch: 191 Average L0 reg loss: 0.1001591069\n",
      "====> Epoch: 191 Average eval loss: 0.0181538034\n",
      "====> Epoch: 192 Average train loss: 0.0221526685\n",
      "====> Epoch: 192 Average L0 reg loss: 0.1001560446\n",
      "====> Epoch: 192 Average eval loss: 0.0181563646\n",
      "====> Epoch: 193 Average train loss: 0.0212416946\n",
      "====> Epoch: 193 Average L0 reg loss: 0.1001532699\n",
      "====> Epoch: 193 Average eval loss: 0.0182014350\n",
      "====> Epoch: 194 Average train loss: 0.0221375075\n",
      "====> Epoch: 194 Average L0 reg loss: 0.1001505973\n",
      "====> Epoch: 194 Average eval loss: 0.0181693546\n",
      "====> Epoch: 195 Average train loss: 0.0283540232\n",
      "====> Epoch: 195 Average L0 reg loss: 0.1001487808\n",
      "====> Epoch: 195 Average eval loss: 0.0181694087\n",
      "====> Epoch: 196 Average train loss: 0.0195337075\n",
      "====> Epoch: 196 Average L0 reg loss: 0.1001465466\n",
      "====> Epoch: 196 Average eval loss: 0.0181557778\n",
      "====> Epoch: 197 Average train loss: 0.0208232668\n",
      "====> Epoch: 197 Average L0 reg loss: 0.1001441147\n",
      "====> Epoch: 197 Average eval loss: 0.0181520060\n",
      "====> Epoch: 198 Average train loss: 0.0235690394\n",
      "====> Epoch: 198 Average L0 reg loss: 0.1001411901\n",
      "====> Epoch: 198 Average eval loss: 0.0181479324\n",
      "====> Epoch: 199 Average train loss: 0.0210712792\n",
      "====> Epoch: 199 Average L0 reg loss: 0.1001394456\n",
      "====> Epoch: 199 Average eval loss: 0.0181483161\n",
      "====> Epoch: 200 Average train loss: 0.0226313033\n",
      "====> Epoch: 200 Average L0 reg loss: 0.1001375233\n",
      "====> Epoch: 200 Average eval loss: 0.0182060469\n",
      "====> Epoch: 201 Average train loss: 0.0234110073\n",
      "====> Epoch: 201 Average L0 reg loss: 0.1001362956\n",
      "====> Epoch: 201 Average eval loss: 0.0181617234\n",
      "====> Epoch: 202 Average train loss: 0.0229936037\n",
      "====> Epoch: 202 Average L0 reg loss: 0.1001340645\n",
      "====> Epoch: 202 Average eval loss: 0.0181594342\n",
      "====> Epoch: 203 Average train loss: 0.0215255822\n",
      "====> Epoch: 203 Average L0 reg loss: 0.1001330990\n",
      "====> Epoch: 203 Average eval loss: 0.0181518253\n",
      "====> Epoch: 204 Average train loss: 0.0224237435\n",
      "====> Epoch: 204 Average L0 reg loss: 0.1001311362\n",
      "====> Epoch: 204 Average eval loss: 0.0181588344\n",
      "====> Epoch: 205 Average train loss: 0.0206270253\n",
      "====> Epoch: 205 Average L0 reg loss: 0.1001295338\n",
      "====> Epoch: 205 Average eval loss: 0.0181488935\n",
      "====> Epoch: 206 Average train loss: 0.0203896857\n",
      "====> Epoch: 206 Average L0 reg loss: 0.1001277878\n",
      "====> Epoch: 206 Average eval loss: 0.0181530342\n",
      "====> Epoch: 207 Average train loss: 0.0192074264\n",
      "====> Epoch: 207 Average L0 reg loss: 0.1001268447\n",
      "====> Epoch: 207 Average eval loss: 0.0181575194\n",
      "====> Epoch: 208 Average train loss: 0.0192969831\n",
      "====> Epoch: 208 Average L0 reg loss: 0.1001263921\n",
      "====> Epoch: 208 Average eval loss: 0.0181634072\n",
      "====> Epoch: 209 Average train loss: 0.0231595557\n",
      "====> Epoch: 209 Average L0 reg loss: 0.1001255227\n",
      "====> Epoch: 209 Average eval loss: 0.0181470867\n",
      "====> Epoch: 210 Average train loss: 0.0198997373\n",
      "====> Epoch: 210 Average L0 reg loss: 0.1001236302\n",
      "====> Epoch: 210 Average eval loss: 0.0181554612\n",
      "====> Epoch: 211 Average train loss: 0.0193752074\n",
      "====> Epoch: 211 Average L0 reg loss: 0.1001221789\n",
      "====> Epoch: 211 Average eval loss: 0.0181526896\n",
      "====> Epoch: 212 Average train loss: 0.0219670321\n",
      "====> Epoch: 212 Average L0 reg loss: 0.1001207951\n",
      "====> Epoch: 212 Average eval loss: 0.0181496534\n",
      "====> Epoch: 213 Average train loss: 0.0219008864\n",
      "====> Epoch: 213 Average L0 reg loss: 0.1001197426\n",
      "====> Epoch: 213 Average eval loss: 0.0181548260\n",
      "====> Epoch: 214 Average train loss: 0.0210504108\n",
      "====> Epoch: 214 Average L0 reg loss: 0.1001185728\n",
      "====> Epoch: 214 Average eval loss: 0.0181556642\n",
      "====> Epoch: 215 Average train loss: 0.0192693670\n",
      "====> Epoch: 215 Average L0 reg loss: 0.1001172408\n",
      "====> Epoch: 215 Average eval loss: 0.0181515813\n",
      "====> Epoch: 216 Average train loss: 0.0226568438\n",
      "====> Epoch: 216 Average L0 reg loss: 0.1001143472\n",
      "====> Epoch: 216 Average eval loss: 0.0181507282\n",
      "====> Epoch: 217 Average train loss: 0.0210151601\n",
      "====> Epoch: 217 Average L0 reg loss: 0.1001129495\n",
      "====> Epoch: 217 Average eval loss: 0.0181467310\n",
      "====> Epoch: 218 Average train loss: 0.0206501199\n",
      "====> Epoch: 218 Average L0 reg loss: 0.1001119806\n",
      "====> Epoch: 218 Average eval loss: 0.0181597471\n",
      "====> Epoch: 219 Average train loss: 0.0210878787\n",
      "====> Epoch: 219 Average L0 reg loss: 0.1001113151\n",
      "====> Epoch: 219 Average eval loss: 0.0181581751\n",
      "====> Epoch: 220 Average train loss: 0.0195747113\n",
      "====> Epoch: 220 Average L0 reg loss: 0.1001101714\n",
      "====> Epoch: 220 Average eval loss: 0.0181617402\n",
      "====> Epoch: 221 Average train loss: 0.0212386479\n",
      "====> Epoch: 221 Average L0 reg loss: 0.1001085883\n",
      "====> Epoch: 221 Average eval loss: 0.0181477331\n",
      "====> Epoch: 222 Average train loss: 0.0191879082\n",
      "====> Epoch: 222 Average L0 reg loss: 0.1001079568\n",
      "====> Epoch: 222 Average eval loss: 0.0181509610\n",
      "====> Epoch: 223 Average train loss: 0.0230008754\n",
      "====> Epoch: 223 Average L0 reg loss: 0.1001071526\n",
      "====> Epoch: 223 Average eval loss: 0.0181500092\n",
      "====> Epoch: 224 Average train loss: 0.0215907264\n",
      "====> Epoch: 224 Average L0 reg loss: 0.1001061924\n",
      "====> Epoch: 224 Average eval loss: 0.0181747973\n",
      "====> Epoch: 225 Average train loss: 0.0202971473\n",
      "====> Epoch: 225 Average L0 reg loss: 0.1001043332\n",
      "====> Epoch: 225 Average eval loss: 0.0181498453\n",
      "====> Epoch: 226 Average train loss: 0.0207476533\n",
      "====> Epoch: 226 Average L0 reg loss: 0.1001027977\n",
      "====> Epoch: 226 Average eval loss: 0.0181522425\n",
      "====> Epoch: 227 Average train loss: 0.0198472612\n",
      "====> Epoch: 227 Average L0 reg loss: 0.1001010055\n",
      "====> Epoch: 227 Average eval loss: 0.0181680936\n",
      "====> Epoch: 228 Average train loss: 0.0227549967\n",
      "====> Epoch: 228 Average L0 reg loss: 0.1000995245\n",
      "====> Epoch: 228 Average eval loss: 0.0181528907\n",
      "====> Epoch: 229 Average train loss: 0.0219092381\n",
      "====> Epoch: 229 Average L0 reg loss: 0.1000979136\n",
      "====> Epoch: 229 Average eval loss: 0.0181489382\n",
      "====> Epoch: 230 Average train loss: 0.0196701710\n",
      "====> Epoch: 230 Average L0 reg loss: 0.1000961292\n",
      "====> Epoch: 230 Average eval loss: 0.0181606282\n",
      "====> Epoch: 231 Average train loss: 0.0189493438\n",
      "====> Epoch: 231 Average L0 reg loss: 0.1000939111\n",
      "====> Epoch: 231 Average eval loss: 0.0181488190\n",
      "====> Epoch: 232 Average train loss: 0.0215107981\n",
      "====> Epoch: 232 Average L0 reg loss: 0.1000917931\n",
      "====> Epoch: 232 Average eval loss: 0.0181745570\n",
      "====> Epoch: 233 Average train loss: 0.0195430978\n",
      "====> Epoch: 233 Average L0 reg loss: 0.1000897539\n",
      "====> Epoch: 233 Average eval loss: 0.0181455184\n",
      "====> Epoch: 234 Average train loss: 0.0232580423\n",
      "====> Epoch: 234 Average L0 reg loss: 0.1000875362\n",
      "====> Epoch: 234 Average eval loss: 0.0181575976\n",
      "====> Epoch: 235 Average train loss: 0.0210469643\n",
      "====> Epoch: 235 Average L0 reg loss: 0.1000863897\n",
      "====> Epoch: 235 Average eval loss: 0.0181508958\n",
      "====> Epoch: 236 Average train loss: 0.0196094579\n",
      "====> Epoch: 236 Average L0 reg loss: 0.1000852851\n",
      "====> Epoch: 236 Average eval loss: 0.0181512740\n",
      "====> Epoch: 237 Average train loss: 0.0229376356\n",
      "====> Epoch: 237 Average L0 reg loss: 0.1000848518\n",
      "====> Epoch: 237 Average eval loss: 0.0182893816\n",
      "====> Epoch: 238 Average train loss: 0.0217210305\n",
      "====> Epoch: 238 Average L0 reg loss: 0.1000843252\n",
      "====> Epoch: 238 Average eval loss: 0.0181585606\n",
      "====> Epoch: 239 Average train loss: 0.0222051236\n",
      "====> Epoch: 239 Average L0 reg loss: 0.1000837204\n",
      "====> Epoch: 239 Average eval loss: 0.0181497317\n",
      "====> Epoch: 240 Average train loss: 0.0193417623\n",
      "====> Epoch: 240 Average L0 reg loss: 0.1000831217\n",
      "====> Epoch: 240 Average eval loss: 0.0181498919\n",
      "====> Epoch: 241 Average train loss: 0.0222114285\n",
      "====> Epoch: 241 Average L0 reg loss: 0.1000821385\n",
      "====> Epoch: 241 Average eval loss: 0.0181462951\n",
      "====> Epoch: 242 Average train loss: 0.0213276104\n",
      "====> Epoch: 242 Average L0 reg loss: 0.1000809159\n",
      "====> Epoch: 242 Average eval loss: 0.0181485228\n",
      "====> Epoch: 243 Average train loss: 0.0196258541\n",
      "====> Epoch: 243 Average L0 reg loss: 0.1000804839\n",
      "====> Epoch: 243 Average eval loss: 0.0181487352\n",
      "====> Epoch: 244 Average train loss: 0.0190919784\n",
      "====> Epoch: 244 Average L0 reg loss: 0.1000799841\n",
      "====> Epoch: 244 Average eval loss: 0.0181637723\n",
      "====> Epoch: 245 Average train loss: 0.0197417810\n",
      "====> Epoch: 245 Average L0 reg loss: 0.1000791716\n",
      "====> Epoch: 245 Average eval loss: 0.0181511212\n",
      "====> Epoch: 246 Average train loss: 0.0214715322\n",
      "====> Epoch: 246 Average L0 reg loss: 0.1000781836\n",
      "====> Epoch: 246 Average eval loss: 0.0181564670\n",
      "====> Epoch: 247 Average train loss: 0.0213639638\n",
      "====> Epoch: 247 Average L0 reg loss: 0.1000772791\n",
      "====> Epoch: 247 Average eval loss: 0.0181538705\n",
      "====> Epoch: 248 Average train loss: 0.0202432235\n",
      "====> Epoch: 248 Average L0 reg loss: 0.1000763305\n",
      "====> Epoch: 248 Average eval loss: 0.0181515608\n",
      "====> Epoch: 249 Average train loss: 0.0200057902\n",
      "====> Epoch: 249 Average L0 reg loss: 0.1000758652\n",
      "====> Epoch: 249 Average eval loss: 0.0181518998\n",
      "Best testing error L0 SINDy is 0.018144018948078156 and it was found at epoch 158\n",
      "x0 = s1, x1 = s2, x2 = s3, x3 = u\n",
      "s1 is equal to:\n",
      "['x0' 'x1 x2' 'x0^3' 'x0 x1^2']\n",
      "tensor([[1., 1., 1., 1.]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[ 0.2890, -0.0491,  0.6870,  0.6660]], device='cuda:0',\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "x0 = s1, x1 = s2, x2 = s3, x3 = u\n",
      "s2 is equal to:\n",
      "x1\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0.9765]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "x0 = s1, x1 = s2, x2 = s3, x3 = u\n",
      "s3 is equal to:\n",
      "['x2' 'x0^2 x1' 'x0^2 x2' 'x1^3' 'x1^2 x2']\n",
      "tensor([[1., 1., 1., 1., 1.]], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([[0.4571, 0.7180, 0.5395, 0.7374, 0.5434]], device='cuda:0',\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T08:16:27.475876Z",
     "start_time": "2024-05-23T08:16:26.927441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# creating the plots\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Training and Evaluation Metrics')\n",
    "\n",
    "data_train = {'FCNN': metrics_fcnn[0], 'SparseFCNN': metrics_sparsefcnn[0], 'L0SINDy': metrics_l0sindy[0]}\n",
    "methods_train = list(data_train.keys())\n",
    "values_train = list(data_train.values())\n",
    "\n",
    "# creating the bar plot\n",
    "ax1.bar(methods_train, values_train, color='maroon', width=0.4)\n",
    "\n",
    "data_eval = {'FCNN': metrics_fcnn[2], 'SparseFCNN': metrics_sparsefcnn[2], 'L0SINDy': metrics_l0sindy[2]}\n",
    "methods_eval = list(data_eval.keys())\n",
    "values_eval = list(data_eval.values())\n",
    "\n",
    "ax2.bar(methods_eval, values_eval, color='blue', width=0.4)\n",
    "\n",
    "save_dir = \"figures\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "fig.savefig('figures/LearningDynamics.png', dpi=300)"
   ],
   "id": "5b52985fbff205cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAHNCAYAAADxHhq4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABTAElEQVR4nO3de1wU5eI/8M8C7q6AoIKyaigcJdG8oCiIl9QTCUklaYZ2EQ21zDtpiSEX7YT3u0Va3s6JI1FKpkYR6jdPIiZC5jUtlRIWb8EqJCQ8vz/8MTmyXFZBFubzfr3mpTvzzMwzs7sPn5nZeUYlhBAgIiIiUgiLuq4AERER0cPE8ENERESKwvBDREREisLwQ0RERIrC8ENERESKwvBDREREisLwQ0RERIrC8ENERESKwvBDREREisLwQ2SCsWPHwsXF5b7mjYqKgkqlqtkKmZkLFy5ApVJh8+bNdV2VCrm4uGDs2LF1su76sH8etgf5ThHdL4YfahBUKlW1hv3799d1VQnA/v37K32ftm3bVtdVfCBxcXFYuXJlXVdDZuzYsVCpVLCzs8Off/5ZbvrZs2el/b906VKTl19YWIioqCh+x6hesKrrChDVhH//+9+y11u3bkVycnK58Z06dXqg9WzYsAGlpaX3NW94eDjmzJnzQOtvaKZNm4bevXuXG+/j41MHtak5cXFxOH78OGbMmCEb365dO/z5559o1KhRndTLysoKhYWF+PLLL/HCCy/Ipn3yySfQarW4devWfS27sLAQ0dHRAIBBgwZVe74H+U4R3S+GH2oQXn75ZdnrQ4cOITk5udz4exUWFsLa2rra63mQP1pWVlawsuJX7m4DBgzA888/X9fVeGhUKhW0Wm2drV+j0aBfv37473//Wy78xMXFISAgAJ9//vlDqUtBQQFsbGzqLAiSsvGyFynGoEGD0KVLF6Snp+Pxxx+HtbU15s6dCwD44osvEBAQgNatW0Oj0aB9+/ZYsGABSkpKZMu49/cJZb/hWLp0KdavX4/27dtDo9Ggd+/e+OGHH2TzGvvNj0qlwpQpU5CYmIguXbpAo9HgscceQ1JSUrn679+/H7169YJWq0X79u3x4YcfVvt3RAcOHMDIkSPRtm1baDQaODs7Y+bMmeUuf4wdOxa2tra4dOkSAgMDYWtrixYtWmDWrFnl9kVeXh7Gjh0Le3t7NG3aFMHBwcjLy6uyLqbo0qULBg8eXG58aWkp2rRpIwtOS5cuRd++feHg4IDGjRvD09MTn332WZXrqGgfbt68GSqVChcuXJDGVedzMmjQIOzevRsXL16ULiOVfWYq+s3P3r17MWDAANjY2KBp06YYNmwYTp06ZbSe586dw9ixY9G0aVPY29tj3LhxKCwsrHI7y7z44ov46quvZO/VDz/8gLNnz+LFF180Ok9eXh5mzJgBZ2dnaDQadOjQAYsWLZLO2Fy4cAEtWrQAAERHR0vbHRUVBeDvz9Uvv/yCoUOHokmTJnjppZekaff+5qe0tBSrVq1C165dodVq0aJFC/j7++PIkSNSmeTkZPTv3x9NmzaFra0tOnbsKH2fiarCw1BSlGvXruGpp57CqFGj8PLLL8PJyQnAnT90tra2CA0Nha2tLfbu3YuIiAgYDAYsWbKkyuXGxcXhxo0beO2116BSqbB48WIMHz4cv/76a5VHtv/73/+wfft2vPHGG2jSpAlWr16NESNGICsrCw4ODgCAjIwM+Pv7o1WrVoiOjkZJSQnmz58v/cGpSkJCAgoLCzFp0iQ4ODjg8OHDWLNmDX7//XckJCTIypaUlMDPzw/e3t5YunQpvv32Wyxbtgzt27fHpEmTAABCCAwbNgz/+9//8Prrr6NTp07YsWMHgoODq1WfMjdu3MDVq1fLjXdwcIBKpUJQUBCioqKg1+uh0+lk+yw7OxujRo2Sxq1atQrPPvssXnrpJRQXF2Pbtm0YOXIkdu3ahYCAAJPqVZHqfE7eeecd5Ofn4/fff8eKFSsAALa2thUu89tvv8VTTz2Ff/zjH4iKisKff/6JNWvWoF+/fjh69Gi5YPDCCy/A1dUVMTExOHr0KD766CO0bNkSixYtqtY2DB8+HK+//jq2b9+OV199FcCdz6+7uzt69uxZrnxhYSEGDhyIS5cu4bXXXkPbtm1x8OBBhIWFIScnBytXrkSLFi3wwQcfYNKkSXjuuecwfPhwAEC3bt2k5dy+fRt+fn7o378/li5dWukZ15CQEGzevBlPPfUUxo8fj9u3b+PAgQM4dOgQevXqhRMnTuDpp59Gt27dMH/+fGg0Gpw7dw7ff/99tfYBEQRRAzR58mRx78d74MCBAoCIjY0tV76wsLDcuNdee01YW1uLW7duSeOCg4NFu3btpNfnz58XAISDg4O4fv26NP6LL74QAMSXX34pjYuMjCxXJwBCrVaLc+fOSeN+/PFHAUCsWbNGGvfMM88Ia2trcenSJWnc2bNnhZWVVbllGmNs+2JiYoRKpRIXL16UbR8AMX/+fFnZHj16CE9PT+l1YmKiACAWL14sjbt9+7YYMGCAACA2bdpUaX327dsnAFQ45OTkCCGEOHPmTLl9IYQQb7zxhrC1tZVt173bWFxcLLp06SL++c9/ysa3a9dOBAcHS6+NvS9CCLFp0yYBQJw/f77CdQhh/HMSEBAg+5yUKfu83L1/PDw8RMuWLcW1a9ekcT/++KOwsLAQY8aMKVfPV199VbbM5557Tjg4OJRb172Cg4OFjY2NEEKI559/XjzxxBNCCCFKSkqETqcT0dHRUv2WLFkizbdgwQJhY2Mjfv75Z9ny5syZIywtLUVWVpYQQogrV64IACIyMtLougGIOXPmGJ12977au3evACCmTZtWrmxpaakQQogVK1YIAOLKlStVbjeRMbzsRYqi0Wgwbty4cuMbN24s/b/sbMSAAQNQWFiI06dPV7ncoKAgNGvWTHo9YMAAAMCvv/5a5by+vr5o37699Lpbt26ws7OT5i0pKcG3336LwMBAtG7dWirXoUMHPPXUU1UuH5BvX0FBAa5evYq+fftCCIGMjIxy5V9//XXZ6wEDBsi2Zc+ePbCyspLOBAGApaUlpk6dWq36lImIiEBycnK5oXnz5gCARx99FB4eHoiPj5fmKSkpwWeffYZnnnlGtl13//+PP/5Afn4+BgwYgKNHj5pUp8o86OfkXjk5OcjMzMTYsWOlbQbufAaefPJJ7Nmzp9w8xt6ba9euwWAwVHu9L774Ivbv3w+9Xo+9e/dCr9dXeMkrISEBAwYMQLNmzXD16lVp8PX1RUlJCb777rtqr/fuz0tFPv/8c6hUKkRGRpabVnZ5smnTpgDuXIbkj6XpfvCyFylKmzZtoFary40/ceIEwsPDsXfv3nJ/RPLz86tcbtu2bWWvy4LQH3/8YfK8ZfOXzXv58mX8+eef6NChQ7lyxsYZk5WVhYiICOzcubNcne7dvrLfWFRUHwC4ePEiWrVqVe5yTseOHatVnzJdu3aFr69vpWWCgoIwd+5cXLp0CW3atMH+/ftx+fJlBAUFycrt2rUL7777LjIzM1FUVCSNr8m+lR70c3KvixcvAjC+3zp16oSvv/5a+mFwmco+a3Z2dtVab9nvbuLj45GZmYnevXujQ4cOst83lTl79iyOHTtW4SXWy5cvV2udVlZWeOSRR6os98svv6B169ayMHivoKAgfPTRRxg/fjzmzJmDJ554AsOHD8fzzz8PCwse01PVGH5IUe4+ci+Tl5eHgQMHws7ODvPnz0f79u2h1Wpx9OhRvP3229U6srS0tDQ6XghRq/NWR0lJCZ588klcv34db7/9Ntzd3WFjY4NLly5h7Nix5bavovrUlaCgIISFhSEhIQEzZszAp59+Cnt7e/j7+0tlDhw4gGeffRaPP/443n//fbRq1QqNGjXCpk2bEBcXV+nyKwpHxn7g/aCfk5pQE58XjUaD4cOHY8uWLfj111+lHyYbU1paiieffBJvvfWW0emPPvpotddZU8GkcePG+O6777Bv3z7s3r0bSUlJiI+Pxz//+U988803ZvcZJvPD8EOKt3//fly7dg3bt2/H448/Lo0/f/58Hdbqby1btoRWq8W5c+fKTTM27l4//fQTfv75Z2zZsgVjxoyRxicnJ993ndq1a4eUlBTcvHlTdvbnzJkz973Miri6usLLywvx8fGYMmUKtm/fjsDAQGg0GqnM559/Dq1Wi6+//lo2ftOmTVUuv+zMSV5ennQ5Bfj7rEwZUz4n1T3b1K5dOwDG99vp06fh6OgoO+tTk1588UVs3LgRFhYWsh+O36t9+/a4efNmlWfoauoMW/v27fH111/j+vXrlZ79sbCwwBNPPIEnnngCy5cvx3vvvYd33nkH+/btq7KuRDw/SIpXdpR495FzcXEx3n///bqqkoylpSV8fX2RmJiI7Oxsafy5c+fw1VdfVWt+QL59QgisWrXqvus0dOhQ3L59Gx988IE0rqSkBGvWrLnvZVYmKCgIhw4dwsaNG3H16tVyl7wsLS2hUqlkZ2suXLiAxMTEKpdd9nuru3+7UlBQgC1btpRbB1C9z4mNjU21LoO1atUKHh4e2LJli+zW8+PHj+Obb77B0KFDq1zG/Ro8eDAWLFiAtWvXyu6ku9cLL7yA1NRUfP311+Wm5eXl4fbt2wAg3b31oN0djBgxAkIIqcPEu5Xt++vXr5eb5uHhAQCyS55EFeGZH1K8vn37olmzZggODsa0adOgUqnw73//u8YuO9WEqKgofPPNN+jXrx8mTZqEkpISrF27Fl26dEFmZmal87q7u6N9+/aYNWsWLl26BDs7O3z++efV+j1SRZ555hn069cPc+bMwYULF9C5c2ds377d5N+9HDhwwGiPwt26dZPdJv3CCy9g1qxZmDVrFpo3b17uyD4gIADLly+Hv78/XnzxRVy+fBnr1q1Dhw4dcOzYsUrrMGTIELRt2xYhISGYPXs2LC0tsXHjRrRo0QJZWVlSOVM+J56enoiPj0doaCh69+4NW1tbPPPMM0bXv2TJEjz11FPw8fFBSEiIdKu7vb19pZejHpSFhQXCw8OrLDd79mzs3LkTTz/9NMaOHQtPT08UFBTgp59+wmeffYYLFy7A0dERjRs3RufOnREfH49HH30UzZs3R5cuXdClSxeT6jV48GC88sorWL16Nc6ePQt/f3+UlpbiwIEDGDx4MKZMmYL58+fju+++Q0BAANq1a4fLly/j/fffxyOPPIL+/fvf7y4hJambm8yIaldFt7o/9thjRst///33ok+fPqJx48aidevW4q233hJff/21ACD27dsnlavoVve7bw0ug3tu+63oVvfJkyeXm/fe27GFECIlJUX06NFDqNVq0b59e/HRRx+JN998U2i12gr2wt9OnjwpfH19ha2trXB0dBQTJkyQbqm/+7bru2+Hvpuxul+7dk288sorws7OTtjb24tXXnlFZGRk1Mit7sZul+7Xr58AIMaPH290mR9//LFwc3MTGo1GuLu7i02bNhmtt7F9m56eLry9vYVarRZt27YVy5cvN3qre3U/Jzdv3hQvvviiaNq0qQAgfWaM3eouhBDffvut6Nevn2jcuLGws7MTzzzzjDh58qSsTNm23Ht7t7F6GlPRe3u3ij7PN27cEGFhYaJDhw5CrVYLR0dH0bdvX7F06VJRXFwslTt48KDw9PQUarVa9j5Wtu57v1NC3Ok2YcmSJcLd3V2o1WrRokUL8dRTT4n09HQhxJ3vwrBhw0Tr1q2FWq0WrVu3FqNHjy53Oz5RRVRCmNHhLRGZJDAwECdOnMDZs2fruipERPUGf/NDVE/c+yiKs2fPYs+ePSY9RJKIiACe+SGqJ1q1aoWxY8fiH//4By5evIgPPvgARUVFyMjIgJubW11Xj4io3uAPnonqCX9/f/z3v/+FXq+HRqOBj48P3nvvPQYfIiIT8cwPERERKQp/80NERESKwvBDREREisLwQ0RERIrC8ENERESKwvBDREREisLwQ0RERIrC8ENERESKwvBDREREisLwQ0RERIrC8ENERESKwvBDREREisLwQ0RERIrC8ENERESKwvBDREREisLwQ0RERIrC8ENERESKwvBDREREisLwQ0RERIrC8ENERESKwvBDREREisLwQ0RERIrC8ENERESKwvBDREREisLwQ0RERIrC8ENERESKwvBDREREisLwQ0RERIrC8ENERESKYlXXFTAnpaWlyM7ORpMmTaBSqeq6OkSKI4TAjRs30Lp1a1hY1J9jM7YdRHXL1LaD4ecu2dnZcHZ2rutqECneb7/9hkceeaSuq1FtbDuIzEN12w6Gn7s0adIEwJ2dZ2dnV8e1IVIeg8EAZ2dn6btYX7DtIKpbprYdDD93KTtdbWdnxwaMqA7Vt0tHbDuIzEN12476c1GdiIiIqAYw/BAREZGiMPwQERGRojD8EBERkaIw/BAREZGiMPwQERGRojD8EBERkaIw/BAREZGiMPwQERGRojD8EBERkaIw/BAREZGiMPwQERGRojD8EBERkaIw/BAREZGiWNV1BYiodkWrVHVdBQBApBB1XQUiMoGZNB2ojaaDZ36IiIhIURh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFEYfojooVi3bh1cXFyg1Wrh7e2Nw4cPV1o+ISEB7u7u0Gq16Nq1K/bs2SObvn37dgwZMgQODg5QqVTIzMyUTb9w4QJUKpXRISEhQSpnbPq2bdtqbLuJyPww/BBRrTsOIDQ0FJGRkTh69Ci6d+8OPz8/XL582Wj5tLQ0jB49GiEhIcjIyEBgYCACAwNx/PhxqUxBQQH69++PRYsWGV2Gs7MzcnJyZEN0dDRsbW3x1FNPycpu2rRJVi4wMLCmNp2IzJBKCPY5X8ZgMMDe3h75+fmws7Or6+oQ1QhzeLzFBgCBkydj7dq1AIDS0lI4Oztj6tSpmDNnjlSu7Dv43HPPobi4GLt27ZKm9enTBx4eHoiNjZUt+8KFC3B1dUVGRgY8PDwqrUePHj3Qs2dPfPzxx9I4lUqFHTt2PFDgYdtBDZEZNB0Aqvd4C1O/gzzzQ0S16jaAbAC+vr7SOAsLC/j6+iI1NdXoPD/88IOsPAD4+flVWL460tPTkZmZiZCQkHLTJk+eDEdHR3h5eWHjxo2o6piwqKgIBoNBNhBR/cEHmxJRrSoEIAA4OTnJxjs5OeH06dNG58nNzTVaXq/X33c9Pv74Y3Tq1Al9+/aVjZ8/fz7++c9/wtraGt988w3eeOMN3Lx5E9OmTatwWTExMYiOjr7vuhBR3WL4IaIG788//0RcXBzmzZtXbtrd43r06IGCggIsWbKk0vATFhaG0NBQ6bXBYICzs3PNVpqIag0vexFRrbIGoMKdszl3y83NhU6nMzqPk5OTSeWr8tlnn6GwsBBjxoypsqy3tzd+//13FBUVVVhGo9HAzs5ONhBR/cHwQ0S1ygpAawApKSnSuNLSUqSkpMDHx8foPL1795aVB4Dk5OQKy1fl448/xrPPPosWLVpUWTYzMxPNmjWDRqO5r3URkfnjZS8iqnU+ADZs2IBevXrBy8sLK1euREFBAcaNGwcAGDNmDNq0aYOwsDAAwKRJkzB06FAsW7YMAQEB2LZtG44cOYL169dLy7x+/TqysrKQnZ0NADhz5gwAQKfTyc4QnTt3Dt999125foIA4Msvv0Rubi769OkDrVaL5ORkvPfee5g1a1Zt7QoiMgMMP0RU67oAGLh0KSIiIqDX6+Hh4YGkpCTpR81ZWVmwsPj7RLS3tzfi4uIQHh6OuXPnws3NDYmJiejSpYtUZufOnVJ4AoBRo0YBACIjIxEVFSWN37hxIx555BEMGTKkXL0aNWqEdevWYebMmRBCoEOHDli+fDkmTJhQw3uAiMwJ+/m5C/vqoIbIHPr5AYDIajQ19fU7WF/rTVQZM2k62M8PERER0YNi+CEiIiJFYfghIiIiRbmv8MOnMxMREVF9ZXL4iY+PN+npzAcPHuTTmYmIiMhsmHy3l7e3N3r37l3l05nLBAUFoaCgoF48nZl3bFBDxLu9al99rTdRZcyk6aj7u72Ki4uRnp5u0tOZU1NTzfbpzHwyMxERkfKY1Mnh1atXUVJSYtLTmfV6vdk+nZlPZiYiIlKeetfDc00+nZlPZiYiIlIeky57OTo6wtLS0qSnLet0OrN9OjOfzExERKQ8JoUftVoNT09Pk57O7OPjw6czExERkdkw+bJXaGgogoODq3w6c0xMDABg+vTpGDhwIJ/OTERERGbB5PATFBSEK1euVPvpzH379uXTmYmIiMhs8Knud2FfHdQQsZ+f2ldf601UGTNpOuq+nx8iIiKi+o7hh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIHop169bBxcUFWq0W3t7eOHz4cKXlExIS4O7uDq1Wi65du2LPnj2y6du3b8eQIUPg4OAAlUqFzMzMcssYNGgQVCqVbHj99ddlZbKyshAQEABra2u0bNkSs2fPxu3btx94e4nIfDH8EFGtOw4gNDQUkZGROHr0KLp37w4/Pz9cvnzZaPm0tDSMHj0aISEhyMjIQGBgIAIDA3H8+HGpTEFBAfr3749FixZVuu4JEyYgJydHGhYvXixNKykpQUBAAIqLi3Hw4EFs2bIFmzdvRkRERI1sNxGZJ5UQQtR1JcyFwWCAvb098vPzYWdnV9fVIaoR0SpVXVcBGwAETp6MtWvXAgBKS0vh7OyMqVOnYs6cOVK5su/gc889h+LiYuzatUua1qdPH3h4eCA2Nla27AsXLsDV1RUZGRnw8PCQTRs0aBA8PDywcuVKo/X66quv8PTTTyM7OxtOTk4AgNjYWLz99tu4cuUK1Gp1tbaPbQc1RGbQdAAAqpNSTP0O8swPEdWq2wCyAfj6+krjLCws4Ovri9TUVKPz/PDDD7LyAODn51dh+cp88skncHR0RJcuXRAWFobCwkJpWmpqKrp27SoFn7L1GAwGnDhxosJlFhUVwWAwyAYiqj+s6roCRNSwFQIQgCxg4P+/Pn36tNF5cnNzjZbX6/UmrfvFF19Eu3bt0Lp1axw7dgxvv/02zpw5g+3btwMA9Hq90fWUTatITEwMoqOjTaoLEZkPhh8iarAmTpwo/b9r165o1aoVnnjiCfzyyy9o3779fS83LCwMoaGh0muDwQBnZ+cHqisRPTy87EVEtcoagAp3zubcLTc3Fzqdzug8Tk5OJpWvLm9vbwDAuXPnAAA6nc7oesqmVUSj0cDOzk42EFH9wfBDRLXKCkBrACkpKdK40tJSpKSkwMfHx+g8vXv3lpUHgOTk5ArLV1fZ7fCtWrUCAPj4+OCnn36S3XWWnJwMOzs7dO7c+YHWRUTmi5e9iKjW+QDYsGEDevXqBS8vL6xcuRIFBQUYN24cAGDMmDFo06YNwsLCAACTJk3C0KFDsWzZMgQEBGDbtm04cuQI1q9fLy3z+vXryMrKQnZ2NgDgzJkzAO6csdHpdPjll18QFxeHoUOHwsHBAceOHcPMmTPx+OOPo1u3bgCAIUOGoHPnznjllVewePFi6PV6hIeHY/LkydBoNA9xDxHRw8QzP0RU67oAWLp0KSIiIuDh4YHMzEwkJSVJPy7OyspCTk6OVN7b2xtxcXFYv349unfvjs8++wyJiYno0qWLVGbnzp3o0aMHAgICAACjRo1Cjx49pFvh1Wo1vv32WwwZMgTu7u548803MWLECHz55ZfSMiwtLbFr1y5YWlrCx8cHL7/8MsaMGYP58+c/hL1CRHWF/fzchX11UENkDv38AEBkNZqa+vodrK/1JqqMmTQd7OeHiIiI6EHdV/jhM3qIiIiovjI5/MTHx5v0jJ6DBw/yGT1ERERkNkz+zY+3tzd69+5d5TN6ygQFBaGgoKBePKOH1+2pIeJvfmpffa03UWXMpOmo+9/8FBcXIz093aRn9KSmppr1M3qIiIhIWUzq5+fq1asoKSkx6Rk9FT07xxye0VNUVISioiLpNR9OSERE1PDVm04Oa+MZPXw4IRERkfKYdNnL0dERlpaWJj1zp6Jn55jDM3rCwsKQn58vDb/99tsD1YmIiIjMn0nhR61Ww9PT06Rn9Pj4+JjtM3r4cEIiIiLlMfmyV2hoKIKDg6t8Rk9MTAwAYPr06Rg4cCCf0UNERERmweTwExQUhCtXriAiIgJ6vR4eHh7lntFjYfH3CaW+ffsiLi4O4eHhmDt3Ltzc3Iw+o6csPAF3ntEDAJGRkYiKipKe0VMWtJydnTFixAiEh4dL85Q9o2fSpEnw8fGBjY0NgoOD+YweIiIikuGzve7CvjqoIWI/P7WvvtabqDJm0nTUfT8/RERERPUdww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPET0U69atg4uLC7RaLby9vXH48OFKyyckJMDd3R1arRZdu3bFnj17ZNO3b9+OIUOGwMHBASqVCpmZmbLp169fx9SpU9GxY0c0btwYbdu2xbRp05Cfny8rp1Kpyg3btm2rkW0mIvPE8ENEte44gNDQUERGRuLo0aPo3r07/Pz8cPnyZaPl09LSMHr0aISEhCAjIwOBgYEIDAzE8ePHpTIFBQXo378/Fi1aZHQZ2dnZyM7OxtKlS3H8+HFs3rwZSUlJCAkJKVd206ZNyMnJkYbAwMCa2GwiMlMqIYSo60qYC4PBAHt7e+Tn58POzq6uq0NUI6JVqrquAjYACJw8GWvXrgUAlJaWwtnZGVOnTsWcOXOkcmXfweeeew7FxcXYtWuXNK1Pnz7w8PBAbGysbNkXLlyAq6srMjIy4OHhUWk9EhIS8PLLL6OgoABWVlYA7pz52bFjxwMFHrYd1BCZQdMBAKhOSjH1O8gzP0RUq24DyAbg6+srjbOwsICvry9SU1ONzvPDDz/IygOAn59fheWrq6xhLAs+ZSZPngxHR0d4eXlh48aNqOqYsKioCAaDQTYQUf1hVXURIqL7VwhAAHBycpKNd3JywunTp43Ok5uba7S8Xq+/73pcvXoVCxYswMSJE2Xj58+fj3/+85+wtrbGN998gzfeeAM3b97EtGnTKlxWTEwMoqOj77suRFS3GH6IqMEzGAwICAhA586dERUVJZs2b9486f89evRAQUEBlixZUmn4CQsLQ2hoqGz5zs7ONV5vIqodvOxFRLXKGoAKd87m3C03Nxc6nc7oPE5OTiaVr8yNGzfg7++PJk2aYMeOHWjUqFGl5b29vfH777+jqKiowjIajQZ2dnaygYjqD4YfIqpVVgBaA0hJSZHGlZaWIiUlBT4+Pkbn6d27t6w8ACQnJ1dYviIGgwFDhgyBWq3Gzp07odVqq5wnMzMTzZo1g0ajMWldRFR/8LIXEdU6HwAbNmxAr1694OXlhZUrV6KgoADjxo0DAIwZMwZt2rRBWFgYAGDSpEkYOnQoli1bhoCAAGzbtg1HjhzB+vXrpWVev34dWVlZyM7OBgCcOXMGAKDT6aDT6aTgU1hYiP/85z+yHya3aNEClpaW+PLLL5Gbm4s+ffpAq9UiOTkZ7733HmbNmvUQ9w4RPWwMP0RU67oAGLh0KSIiIqDX6+Hh4YGkpCTpR81ZWVmwsPj7RLS3tzfi4uIQHh6OuXPnws3NDYmJiejSpYtUZufOnVJ4AoBRo0YBACIjIxEVFYWjR48iLS0NANChQwdZfc6fPw8XFxc0atQI69atw8yZMyGEQIcOHbB8+XJMmDChtnYFEZmB++rnZ926dViyZAn0ej26d++ONWvWwMvLq8LyCQkJmDdvHi5cuAA3NzcsWrQIQ4cOlaZv374dsbGxSE9Px/Xr18v113H9+nVERkbim2++QVZWFlq0aIHAwEAsWLAA9vb2f2+MkU4J/vvf/0qNYlXYVwc1RObQzw8ARFajqamv38H6Wm+iyphJ02Ee/fzEx8eb1FPrwYMH2VMrERERmQ2Tz/x4e3ujd+/eVfbUWiYoKAgFBQX1oqdWHr1RQ8QzP7WvvtabqDJm0nTU/Zmf4uJipKenm9RTa2pqqtn21MpeWomIiJTHpB88X716FSUlJSb11KrX6822p1b20kpERKQ89e5ur5rsqZW9tBIRESmPSZe9HB0dYWlpaVLPqzqdzmx7amUvrURERMpjUvhRq9Xw9PQ0qadWHx8f9tRKREREZsPky16hoaEIDg6usqfWmJgYAMD06dMxcOBA9tRKREREZsHk8BMUFIQrV65Uu6fWvn37sqdWIiIiMhv31cNzQ8W+OqghYj8/ta++1puoMmbSdNR9Pz9ERERE9R3DDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPET0U69atg4uLC7RaLby9vXH48OFKyyckJMDd3R1arRZdu3bFnj17ZNO3b9+OIUOGwMHBASqVCpmZmeWWcevWLUyePBkODg6wtbXFiBEjkJubKyuTlZWFgIAAWFtbo2XLlpg9ezZu3779wNtLROaL4YeIat1xAKGhoYiMjMTRo0fRvXt3+Pn54fLly0bLp6WlYfTo0QgJCUFGRgYCAwMRGBiI48ePS2UKCgrQv39/LFq0qML1zpw5E19++SUSEhLwf//3f8jOzsbw4cOl6SUlJQgICEBxcTEOHjyILVu2YPPmzYiIiKixbSci86MSQoi6roS5MBgMsLe3R35+Puzs7Oq6OkQ1IlqlqusqYAOAwMmTsXbtWgBAaWkpnJ2dMXXqVMyZM0cqV/YdfO6551BcXIxdu3ZJ0/r06QMPDw/ExsbKln3hwgW4uroiIyMDHh4e0vj8/Hy0aNECcXFxeP755wEAp0+fRqdOnZCamoo+ffrgq6++wtNPP43s7Gw4OTkBAGJjY/H222/jypUrUKvV1do+th11yww+4gCAhvbXtD7tV1O/gzzzQ0S16jaAbAC+vr7SOAsLC/j6+iI1NdXoPD/88IOsPAD4+flVWN6Y9PR0/PXXX7LluLu7o23bttJyUlNT0bVrVyn4lK3HYDDgxIkTFS67qKgIBoNBNhBR/cHwQ0S1qhCAAGQBA///tV6vNzpPbm6uSeWN0ev1UKvVaNq0aYXL0ev1RtdTNq0iMTExsLe3lwZnZ+dq14uI6h7DDxGRicLCwpCfny8Nv/32W11XiYhMYFXXFSCihs0agAood5dVbm4udDqd0XmcnJxMKm+MTqdDcXEx8vLyZGd/7l6OTqcrd9dZ2XorW5dGo4FGo6l2XYjIvPDMDxHVKisArQGkpKRI40pLS5GSkgIfHx+j8/Tu3VtWHgCSk5MrLG+Mp6cnGjVqJFvOmTNnkJWVJS3Hx8cHP/30k+yus+TkZNjZ2aFz587VXhcR1S/3FX7YXwcRmcIHwIYNG7BlyxacOnUKkyZNQkFBAcaNGwcAGDNmDMLCwqTykyZNQlJSEpYtW4bTp08jKioKR44cwZQpU6Qy169fR2ZmJk6ePAngTrDJzMyUfqtjb2+PkJAQhIaGYt++fUhPT8e4cePg4+ODPn36AACGDBmCzp0745VXXsGPP/6Ir7/+GuHh4Zg8eTLP7BA1YCZf9oqPj0doaChiY2Ph7e2NlStXws/PD2fOnEHLli3LlT948CBGjx6NmJgYPP3004iLi0NgYCCOHj2KLl26APi7v44XXngBEyZMMLremTNnYvfu3UhISIC9vT2mTJmC4cOH4/vvvwfwd38dOp0OBw8eRE5ODsaMGYNGjRrhvffeM3UzqY6Yw23ZABDZ0O5ZrWNdAAxcuhQRERHQ6/Xw8PBAUlKS9OPirKwsWFj8fSzm7e2NuLg4hIeHY+7cuXBzc0NiYqLUZgDAzp07pfAEAKNGjQIAREZGIioqCgCwYsUKWFhYYMSIESgqKoKfnx/ef/99aR5LS0vs2rULkyZNgo+PD2xsbBAcHIz58+fX4t4gorpmcj8/3t7e6N27d5X9dZQJCgpCQUFBveivg3111D2Gn5pXn/Zpff0O1td6NxRm8hFnPz+1pM77+SkuLkZ6erpJ/XWkpqaadX8dREREpCwmXfa6evUqSkpKjPaLcfr0aaPzVNSPhjn011FUVISioiLpNTsqIyIiavgUfbcXOyojIiJSHpPCj6OjIywtLU3qf0On09Vofx0VLaei9ZRNM4YdlRERESmPSeFHrVbD09PTpP46fHx8zLa/Do1GAzs7O9lAREREDZvJt7qHhoYiODgYvXr1gpeXF1auXFmuv442bdogJiYGADB9+nQMHDgQy5YtQ0BAALZt24YjR45g/fr10jKvX7+OrKwsZGdnA7gTbIA7Z2x0Op2sv47mzZvDzs4OU6dOrbC/jsWLF0Ov17O/DiIiIirH5PATFBSEK1euVLu/jr59+7K/DiIiIjIbJvfz05Cxr466V5/6pKkv6tM+ra/fwfpa74bCTD7i7OenltR5Pz9ERERE9R3DDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPET0069atg4uLC7RaLby9vXH48OFKyyckJMDd3R1arRZdu3bFnj17ZNOFEIiIiECrVq3QuHFj+Pr64uzZs9L0/fv3Q6VSGR1++OEHAMCFCxeMTj906FDN7wAiMgsMP0T0UMTHxyM0NBSRkZE4evQounfvDj8/P1y+fNlo+YMHD2L06NEICQlBRkYGAgMDERgYiOPHj0tlFi9ejNWrVyM2NhZpaWmwsbGBn58fbt26BQDo27cvcnJyZMP48ePh6uqKXr16ydb37bffysp5enrW3s4gojrF8ENED8Xy5csxYcIEjBs3Dp07d0ZsbCysra2xceNGo+VXrVoFf39/zJ49G506dcKCBQvQs2dPrF27FsCdsz4rV65EeHg4hg0bhm7dumHr1q3Izs5GYmIiAECtVkOn00mDg4MDvvjiC4wbNw4qlUq2PgcHB1nZRo0a1er+IKK6w/BDRLXuNoD09HT4+vpK4ywsLODr64vU1FSj86SmpsrKA4Cfn59U/vz589Dr9bIy9vb28Pb2rnCZO3fuxLVr1zBu3Lhy05599lm0bNkS/fv3x86dOyvdnqKiIhgMBtlARPUHww8R1bpCACUlJXBycpKNd3Jygl6vNzqPXq+vtHzZv6Ys8+OPP4afnx8eeeQRaZytrS2WLVuGhIQE7N69G/3790dgYGClASgmJgb29vbS4OzsXGFZIjI/VnVdASKih+H333/H119/jU8//VQ23tHREaGhodLr3r17Izs7G0uWLMGzzz5rdFlhYWGyeQwGAwMQUT1yX2d+eMcGEZnCGoClpSVyc3Nl43Nzc6HT6YzOo9PpKi1f9m91l7lp0yY4ODhUGGju5u3tjXPnzlU4XaPRwM7OTjYQUf1hcvjhHRtEZCorAJ6enkhJSZHGlZaWIiUlBT4+Pkbn8fHxkZUHgOTkZKm8q6srdDqdrIzBYEBaWlq5ZQohsGnTJowZM6ZaP2TOzMxEq1atqrt5RFTPmHzZ6+47NgAgNjYWu3fvxsaNGzFnzpxy5e++YwMAFixYgOTkZKxduxaxsbHl7tgAgK1bt8LJyQmJiYkYNWqUdMdGmb/++gtffPEFpk6dWuEdG0RkXkJDQxEcHIxevXrBy8sLK1euREFBgdSWjBkzBo6OjlL56dOnY+DAgVi2bBkCAgKwbds2HDlyBOvXrwcAqFQqzJgxA++++y7c3Nzg6uqKefPmoXXr1ggMDJSte+/evTh//jzGjx9frl5btmyBWq1Gjx49AADbt2/Hxo0b8dFHH9XSniCiumZS+CkuLkZ6ejrCwsKkcdW5Y+Pua+PAnTs2ym5FreqOjVGjRpVbZlV3bNy6dQuPPvoo3nrrrUpPcRcVFaGoqEh6zTs2iGpPUFAQrly5goiICOj1enh4eCApKUn6wXJWVhZKSkqk8n379kVcXBzCw8Mxd+5cuLm5ITExEV26dJHKvPXWWygoKMDEiRORl5eH/v37IykpCVqtVrbujz/+GH379oW7u7vRui1YsAAXL16ElZUV3N3dER8fj+eff74W9gIRmQOTws/Vq1crvGPj9OnTRud52Hds9OvXDxYWFvj8888RGBiIxMTECgNQTEwMoqOjK9liIqpJU6ZMwZQpU4xO279/PwwGA+Li4qRxI0eOxMiRIytcnkqlwvz58zF//vxK13v3Mu8VHByM4ODgKmpORA1Jvbvbi3dsEBER0YMw6QfPjo6OvGODiIiI6jWTwo9areYdG0RERFSvmXzZqzp3bLRp0wYxMTEAeMcGERERmReTw0917tiwsPj7hBLv2CAiIiJzohJCiLquhLkwGAywt7dHfn4+f/9TR6Lv6beprkQ2oK9Ffdqn9fU7WF/r3VCYyUccDajZAFC/9qup30E+2JSIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiIiIFIXhh4iIiBSF4YeIiIgUheGHiB6adevWwcXFBVqtFt7e3jh8+HCl5RMSEuDu7g6tVouuXbtiz549sulCCERERKBVq1Zo3LgxfH19cfbsWVkZFxcXqFQq2bBw4UJZmWPHjmHAgAHQarVwdnbG4sWLa2aDicgsMfwQ0UMRHx+P0NBQREZG4ujRo+jevTv8/Pxw+fJlo+UPHjyI0aNHIyQkBBkZGQgMDERgYCCOHz8ulVm8eDFWr16N2NhYpKWlwcbGBn5+frh165ZsWfPnz0dOTo40TJ06VZpmMBgwZMgQtGvXDunp6ViyZAmioqKwfv362tkRRFTnGH6I6KFYvnw5JkyYgHHjxqFz586IjY2FtbU1Nm7caLT8qlWr4O/vj9mzZ6NTp05YsGABevbsibVr1wK4c9Zn5cqVCA8Px7Bhw9CtWzds3boV2dnZSExMlC2rSZMm0Ol00mBjYyNN++STT1BcXIyNGzfisccew6hRozBt2jQsX7681vYFEdUthh8iqnW3AaSnp8PX11caZ2FhAV9fX6SmphqdJzU1VVYeAPz8/KTy58+fh16vl5Wxt7eHt7d3uWUuXLgQDg4O6NGjB5YsWYLbt2/L1vP4449DrVbL1nPmzBn88ccfRutWVFQEg8EgG4io/riv8MPr9kRkikIAJSUlcHJyko13cnKCXq83Oo9er6+0fNm/VS1z2rRp2LZtG/bt24fXXnsN7733Ht56660q13P3Ou4VExMDe3t7aXB2dq5o04nIDJkcfnjdnojqk9DQUAwaNAjdunXD66+/jmXLlmHNmjUoKiq672WGhYUhPz9fGn777bcarDER1TaTww+v2xORqawBWFpaIjc3VzY+NzcXOp3O6Dw6na7S8mX/mrJMAPD29sbt27dx4cKFStdz9zrupdFoYGdnJxuIqP4wKfwUFxfzuj0RmcwKgKenJ1JSUqRxpaWlSElJgY+Pj9F5fHx8ZOUBIDk5WSrv6uoKnU4nK2MwGJCWllbhMgEgMzMTFhYWaNmypbSe7777Dn/99ZdsPR07dkSzZs1M3lYiMn9WphS+evVqhdftT58+bXSemrxu37NnTzRv3hwHDx5EWFgYcnJypDM7er0erq6u5ZZRNs1YIxYTE4Po6Ogqt5uIHlxoaCiCg4PRq1cveHl5YeXKlSgoKMC4ceMAAGPGjIGjo6NUfvr06Rg4cCCWLVuGgIAAbNu2DUeOHJEuZatUKsyYMQPvvvsu3Nzc4Orqinnz5qF169YIDAwEcOegKC0tDYMHD0aTJk2QmpqKmTNn4uWXX5bahBdffBHR0dEICQnB22+/jePHj2PVqlVYsWLFw91BRPTQmBR+6lJoaKj0/27dukGtVuO1115DTEwMNBrNfS0zLCxMtlyDwcAfLhLVkqCgIFy5cgURERHQ6/Xw8PBAUlKSdJCSlZWFkpISqXzfvn0RFxeH8PBwzJ07F25ubkhMTESXLl2kMm+99RYKCgowceJE5OXloX///khKSoJWqwVw5/LUtm3bEBUVhaKiIri6umLmzJmy7729vT2++eYbTJ48GZ6ennB0dERERAQmTpz4kPYMET1sJoUfR0fHWr1u36pVK1kZDw+PCuty93X7jh073vd1+/sNTkRkuilTpmDKlClGp+3fvx8GgwFxcXHSuJEjR2LkyJEVLk+lUmH+/PmYP3++0ek9e/bEoUOHqqxXt27dcODAgSrLEVHDYNJvftRqNa/bExERUb1m8t1eoaGh2LBhA7Zs2YJTp05h0qRJ5a7bh4WFSeWnT5+OpKQkLFu2DKdPn0ZUVBSOHDkiHf3dfd1+586d+OmnnzBmzJhy1+1XrlyJH3/8Eb/++is++eQTo9ft1Wo1QkJCcOLECcTHx2PVqlWy09tEREREJv/mpzrX7S0s/s5UvG5PRERE5kQlhBB1XQlzYTAYYG9vj/z8fPbbUUeiVaq6rgIAILIBfS3q0z6tr9/B+lrvhsJMPuJoQM0GgPq1X039DvLZXkRERKQoDD9ERESkKAw/REREpCgMP0RERKQoDD9ERESkKAw/REREpCgMP0RERKQoDD9ERESkKAw/REREpCgMP0RERKQoDD9ERESkKAw/REREpCgMP0RERKQoDD9ERESkKAw/REREpCgMP0RERKQoDD9ERESkKAw/REREpCgMP0RERKQoDD9ERESkKAw/REREpCgMP0RERKQoDD9ERESkKAw/REREpCgMP0RERKQoDD9ERESkKAw/REREpCgMP0RERKQoDD9ERESkKAw/REREpCgMP0RERKQoDD9ERESkKAw/REREpCgMP0RERKQoDD9E9NCsW7cOLi4u0Gq18Pb2xuHDhystn5CQAHd3d2i1WnTt2hV79uyRTRdCICIiAq1atULjxo3h6+uLs2fPStMvXLiAkJAQuLq6onHjxmjfvj0iIyNRXFwsK6NSqcoNhw4dqtmNJyKzwfBDRA9FfHw8QkNDERkZiaNHj6J79+7w8/PD5cuXjZY/ePAgRo8ejZCQEGRkZCAwMBCBgYE4fvy4VGbx4sVYvXo1YmNjkZaWBhsbG/j5+eHWrVsAgNOnT6O0tBQffvghTpw4gRUrViA2NhZz584tt75vv/0WOTk50uDp6Vk7O4KI6tx9hR8evRGRqZYvX44JEyZg3Lhx6Ny5M2JjY2FtbY2NGzcaLb9q1Sr4+/tj9uzZ6NSpExYsWICePXti7dq1AO60GytXrkR4eDiGDRuGbt26YevWrcjOzkZiYiIAwN/fH5s2bcKQIUPwj3/8A88++yxmzZqF7du3l1ufg4MDdDqdNDRq1KjW9gUR1S2Tww+P3ojIVLcBpKenw9fXVxpnYWEBX19fpKamGp0nNTVVVh4A/Pz8pPLnz5+HXq+XlbG3t4e3t3eFywSA/Px8NG/evNz4Z599Fi1btkT//v2xc+fOSrenqKgIBoNBNhBR/WFy+OHRGxGZqhBASUkJnJycZOOdnJyg1+uNzqPX6ystX/avKcs8d+4c1qxZg9dee00aZ2tri2XLliEhIQG7d+9G//79ERgYWGkAiomJgb29vTQ4OztXWJaIzI9J4ae4uJhHb0RUL126dAn+/v4YOXIkJkyYII13dHREaGgovL290bt3byxcuBAvv/wylixZUuGywsLCkJ+fLw2//fbbw9gEIqohJoWfq1ev8uiNiExmDcDS0hK5ubmy8bm5udDpdEbn0el0lZYv+7c6y8zOzsbgwYPRt29frF+/vsr6ent749y5cxVO12g0sLOzkw1EVH/Uu7u9ePRGVP9YAfD09ERKSoo0rrS0FCkpKfDx8TE6j4+Pj6w8ACQnJ0vlXV1dodPpZGUMBgPS0tJky7x06RIGDRoET09PbNq0CRYWVTd7mZmZaNWqlSmbSET1iJUphR0dHWv16O3uxiY3NxceHh6y+e7n6C05ObnC6RqNBhqNpsrlENGDCw0NRXBwMHr16gUvLy+sXLkSBQUFGDduHABgzJgxcHR0lMpPnz4dAwcOxLJlyxAQEIBt27bhyJEj0ndfpVJhxowZePfdd+Hm5gZXV1fMmzcPrVu3RmBgIIC/g0+7du2wdOlSXLlyRVp+WduzZcsWqNVq9OjRAwCwfft2bNy4ER999NHD2C1EVAdMCj9qtVo6eitrXMqO3qZMmWJ0nrKjtxkzZkjjKjp6Kws7ZUdvkyZNkua5dOkSBg8ezKM3onoqKCgIV65cQUREBPR6PTw8PJCUlCRd8s7KykJJSYlUvm/fvoiLi0N4eDjmzp0LNzc3JCYmokuXLlKZt956CwUFBZg4cSLy8vLQv39/JCUlQavVArjT1pw7dw7nzp3DI488IquPEEL6/4IFC3Dx4kVYWVnB3d0d8fHxeP7552tzdxBRHVKJu1uAaoiPj0dwcDA+/PBD6ejt008/xenTp+Hk5IQxY8agTZs2iImJAXDnVveBAwdi4cKF0tHbe++9h6NHj0qN2KJFi7Bw4UJs2bJFOno7duwYTp48Ca1WKzt627JlCywtLaX6VHb0Nm/ePHz00UfSkWVVDAYD7O3tkZ+fz2v4dSRaparrKgAAIk37Wpi1+rRP6+t3sL7Wu6Ewk484GlCzAaB+7VdTv4MmnfkBqnf0dvdZGR69ERERkTkx+cxPQ8ajt7pXn85S1Bf1aZ/W1+9gfa13Q2EmH3Ge+akltXHmp97d7UVERET0IBh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFEYfoiIiEhRGH6IiIhIURh+iIiISFHuK/ysW7cOLi4u0Gq18Pb2xuHDhystn5CQAHd3d2i1WnTt2hV79uyRTRdCICIiAq1atULjxo3h6+uLs2fPyspcv34dL730Euzs7NC0aVOEhITg5s2bsjLHjh3DgAEDoNVq4ezsjMWLF9/P5lVLtEplFgNRfcK2g4jMgcnhJz4+HqGhoYiMjMTRo0fRvXt3+Pn54fLly0bLHzx4EKNHj0ZISAgyMjIQGBiIwMBAHD9+XCqzePFirF69GrGxsUhLS4ONjQ38/Pxw69YtqcxLL72EEydOIDk5Gbt27cJ3332HiRMnStMNBgOGDBmCdu3aIT09HUuWLEFUVBTWr19v6iYSUS1g23GHSmUeA5GSqYQQwpQZvL290bt3b6xduxYAUFpaCmdnZ0ydOhVz5swpVz4oKAgFBQXYtWuXNK5Pnz7w8PBAbGwshBBo3bo13nzzTcyaNQsAkJ+fDycnJ2zevBmjRo3CqVOn0LlzZ/zwww/o1asXACApKQlDhw7F77//jtatW+ODDz7AO++8A71eD7VaDQCYM2cOEhMTcfr06Wptm8FggL29PfLz82FnZ1dpWXM56xJp2ttn9rhfa5657NM9Xl5Vth13fwcnTJjQINsOM3k70IA+4tyntaQ+7VdTvoMAYGVKBYqLi5Geno6wsDBpnIWFBXx9fZGammp0ntTUVISGhsrG+fn5ITExEQBw/vx56PV6+Pr6StPt7e3h7e2N1NRUjBo1CqmpqWjatKnUeAGAr68vLCwskJaWhueeew6pqal4/PHHpcarbD2LFi3CH3/8gWbNmpWrW1FREYqKiqTX+fn5AO7sxKrcqrLEw1GdutYn3K81zxz2aQmA9PR0TJ8+XbZvBw4ciAMHDuCNN94A8Pd+F0I02LbDXNSjqtYb3Ke1ozr79e62ozpMCj9Xr15FSUkJnJycZOOdnJwqPELS6/VGy+v1eml62bjKyrRs2VJecSsrNG/eXFbG1dW13DLKphlrwGJiYhAdHV1uvLOzs9FtMUcL7e3rugoNEvdrLSgpwUsvvWR0kv09+/vGjRtsO2oZP+I1j/u0dpiyX2/cuFGuPTHGpPDT0ISFhcmOLEtLS3H9+nU4ODhAVcvn+wwGA5ydnfHbb79V6xQdVQ/3a82riX2ak5MDd3d3JCcnw8vLSxo/b948fP/999i7dy+AO0dtN27cQOvWrWuk7rWFbUfDwn1aOx7mfjW17TAp/Dg6OsLS0hK5ubmy8bm5udDpdEbn0el0lZYv+zc3NxetWrWSlfHw8JDK3PujyNu3b+P69euy5Rhbz93ruJdGo4FGo5GNa9q0qdGytcXOzo5ftlrA/VrzHmSfarVaWFpa4ubNm7Jl5OXloU2bNrJxZUdtbDsqx894zeM+rR0Pa79W54xPGZPu9lKr1fD09ERKSoo0rrS0FCkpKfDx8TE6j4+Pj6w8ACQnJ0vlXV1dodPpZGUMBgPS0tKkMj4+PsjLy0N6erpUZu/evSgtLYW3t7dU5rvvvsNff/0lW0/Hjh2NnrYmooeHbQcRmRVhom3btgmNRiM2b94sTp48KSZOnCiaNm0q9Hq9EEKIV155RcyZM0cq//333wsrKyuxdOlScerUKREZGSkaNWokfvrpJ6nMwoULRdOmTcUXX3whjh07JoYNGyZcXV3Fn3/+KZXx9/cXPXr0EGlpaeJ///ufcHNzE6NHj5am5+XlCScnJ/HKK6+I48ePi23btglra2vx4YcfmrqJD0V+fr4AIPLz8+u6Kg0K92vNq6l9yrajZvAzXvO4T2uHOe9Xk8OPEEKsWbNGtG3bVqjVauHl5SUOHTokTRs4cKAIDg6Wlf/000/Fo48+KtRqtXjsscfE7t27ZdNLS0vFvHnzhJOTk9BoNOKJJ54QZ86ckZW5du2aGD16tLC1tRV2dnZi3Lhx4saNG7IyP/74o+jfv7/QaDSiTZs2YuHChfezeQ/FrVu3RGRkpLh161ZdV6VB4X6teTW5T9l2PDh+xmse92ntMOf9anI/P0RERET1GZ/tRURERIrC8ENERESKwvBDREREisLwQ0RERIrC8HMfxo4dC5VKVW44d+4cgDtd4k+dOhX/+Mc/oNFo4OzsjGeeeUbWH4mLiwtUKhUOHTokW/aMGTMwaNAg6XVUVBRUKhVef/11WbnMzEyoVCpcuHCh1rbTmCtXrmDSpElo27YtNBoNdDod/Pz88P333z/UetwPY+9Z//79ZWX27duHoUOHwsHBAdbW1ujcuTPefPNNXLp0CQCwf/9+qFQqPPbYYygpKZHN27RpU2zevFl6Xd33+H6MHTsWgYGBRqfdunULkydPhoODA2xtbTFixIhynfjt2LEDffr0gb29PZo0aYLHHnsMM2bMkKZv3rxZ1mnf5s2boVKp4O/vL1tOXl4eVCoV9u/fL427e//a2NjAzc0NY8eOlfW1o1RKbTvYbphHuwGw7SjD8HOf/P39kZOTIxtcXV1x4cIFeHp6Yu/evViyZAl++uknJCUlYfDgwZg8ebJsGVqtFm+//XaV69Jqtfj4449x9uzZ2tqcahsxYgQyMjKwZcsW/Pzzz9i5cycGDRqEa9eu1do6i4uLa2xZmzZtkr1nO3fulKZ9+OGH8PX1hU6nw+eff46TJ08iNjYW+fn5WLZsmWw5v/76K7Zu3Vrl+qr7HtekmTNn4ssvv0RCQgL+7//+D9nZ2Rg+fLg0PSUlBUFBQRgxYgQOHz6M9PR0/Otf/5J18meMlZUVvv32W+zbt6/KOpTt5xMnTmDdunW4efMmvL29q7XPGjolth1sN+4w53YDUFjbUdf32tdHwcHBYtiwYUanPfXUU6JNmzbi5s2b5ab98ccf0v/btWsnpk2bJtRqtazvkunTp4uBAwdKryMjI0X37t3Fk08+KUaOHCmNz8jIEADE+fPnH3Rzqu2PP/4QAMT+/fsrLANAvP/++8Lf319otVrh6uoqEhISZGXeeust4ebmJho3bixcXV1FeHi4KC4ulqaXbfOGDRuEi4uLUKlUQgghEhISRJcuXYRWqxXNmzcXTzzxhGw/b9iwQbi7uwuNRiM6duwo1q1bV65uO3bsMFrv3377TajVajFjxowKt10IIfbt2ycAiNmzZwtnZ2dZ/xX29vZi06ZN0uvqvsf3o6LPYF5enmjUqJFsn586dUoAEKmpqdL6Bw0aVOnyN23aJOzt7cu9njBhgvDy8pLGl30m9u3bJ42raD+PGTNGNGnSRFy/fl3cvHlTNGnSpNxnY8eOHcLa2loYDIZK61dfKbHtYLthPu2GEGw7yvDMTw26fv06kpKSMHnyZNjY2JSbfu+zf1xdXfH6668jLCwMpaWllS574cKF+Pzzz3HkyJGarLJJbG1tYWtri8TERBQVFVVYbt68eRgxYgR+/PFHvPTSSxg1ahROnTolTW/SpAk2b96MkydPYtWqVdiwYQNWrFghW8a5c+fw+eefY/v27cjMzEROTg5Gjx6NV199FadOncL+/fsxfPhwiP/fTdUnn3yCiIgI/Otf/8KpU6fw3nvvYd68ediyZUu1ti0hIQHFxcV46623jE6/972bMWMGbt++jTVr1lS6XFPe45qQnp6Ov/76C76+vtI4d3d3tG3bFqmpqQDuPK/qxIkTOH78uMnLj4qKwk8//YTPPvvM5HlnzpyJGzduIDk5GTY2Nhg1ahQ2bdokK7Np0yY8//zzaNKkicnLr88actvBduNv5tpuAMprOxh+7tOuXbukL7WtrS1GjhyJc+fOQQgBd3f3ai8nPDwc58+fxyeffFJpuZ49e+KFF16ok1OhZaysrLB582Zs2bIFTZs2Rb9+/TB37lwcO3ZMVm7kyJEYP348Hn30USxYsAC9evWSfdnDw8PRt29fuLi44JlnnsGsWbPw6aefypZRXFyMrVu3okePHujWrRtycnJw+/ZtDB8+HC4uLujatSveeOMN2NraAgAiIyOxbNkyDB8+HK6urhg+fDhmzpyJDz/8ULbc0aNHy963xMREAMDZs2dhZ2cne0BmZaytrREZGYmYmBjk5+dXWra673FN0Ov1UKvV5RpdJycn6PV6AMDUqVPRu3dvdO3aFS4uLhg1ahQ2btxY6R+mMq1bt8b06dPxzjvv4Pbt2ybVrex7UfZbk/Hjx+Prr79GTk4OAODy5cvYs2cPXn31VZOWW98ore1gu/E3c203AOW1HQw/92nw4MHIzMyUhtWrV0tHE6Zo0aIFZs2ahYiIiCqvUb/77rs4cOAAvvnmm/ut9gMbMWIEsrOzsXPnTvj7+2P//v3o2bOn7Ad79z6o0sfHR3YEFx8fj379+kGn08HW1hbh4eHIysqSzdOuXTu0aNFCet29e3c88cQT6Nq1K0aOHIkNGzbgjz/+AAAUFBTgl19+QUhIiKyBevfdd/HLL7/IlrtixQrZ+/bkk08CAIQQUKlUJu2LkJAQODg4YNGiRZWWM+U9fhhsbGywe/dunDt3DuHh4bC1tcWbb74JLy8vFBYWVjn/22+/jStXrmDjxo0mrbfs+1G2n728vPDYY49JR9n/+c9/0K5dOzz++OMmblH9osS2g+3G3+pruwE0rLaD4ec+2djYoEOHDtLQqlUruLm5QaVS4fTp0yYtKzQ0FH/++Sfef//9Ssu1b98eEyZMwJw5c+6rsawpWq0WTz75JObNm4eDBw9i7NixiIyMrNa8qampeOmllzB06FDs2rULGRkZeOedd8p9ue899W9paYnk5GR89dVX6Ny5M9asWYOOHTvi/PnzuHnzJgBgw4YNsgbq+PHj5e6Y0Ol0svetbD2PPvoo8vPzpSOJ6rCyssK//vUvrFq1CtnZ2ZWWre57/KB0Oh2Ki4uRl5cnG5+bmwudTicb1759e4wfPx4fffQRjh49ipMnTyI+Pr7KdTRt2hRhYWGIjo6uVoNXpuwPmaurqzRu/Pjx0h/ATZs2Ydy4cSb/MalvlNp2sN24wxzbDUB5bQfDTw1q3rw5/Pz8sG7dOhQUFJSbfu+HqoytrS3mzZuHf/3rX7hx40al64iIiMDPP/+Mbdu21USVa0Tnzp1l23tvw3Ho0CF06tQJAHDw4EG0a9cO77zzDnr16gU3NzdcvHixWutRqVTo168foqOjkZGRAbVajR07dsDJyQmtW7fGr7/+KmugOnToIPuyVOb555+HWq3G4sWLjU6v6L0bOXIkHnvsMURHR1e6fFPe4wfh6emJRo0ayW6NPnPmDLKyssodWd/NxcUF1tbWRj+3xkydOhUWFhZYtWpVteu2cuVK2NnZyX5T8PLLL+PixYtYvXo1Tp48ieDg4GovryFRYtvBdsN82g1AeW2HlUmlqUrr1q1Dv3794OXlhfnz56Nbt264ffs2kpOT8cEHH8hO495t4sSJWLFiBeLi4uDt7V3h8p2cnBAaGoolS5bU1iZU6Nq1axg5ciReffVVdOvWDU2aNMGRI0ewePFiDBs2TCqXkJCAXr16oX///vjkk09w+PBhfPzxxwAANzc3ZGVlYdu2bejduzd2796NHTt2VLnutLQ0pKSkYMiQIWjZsiXS0tJw5coVqXGMjo7GtGnTYG9vD39/fxQVFeHIkSP4448/EBoaWuXynZ2dsWLFCkyZMgUGgwFjxoyBi4sLfv/9d2zduhW2trblblsts3DhQvj5+VW5juq+x9WVn5+PzMxM2TgHBweEhIQgNDQUzZs3h52dHaZOnQofHx/06dMHwJ0fHhYWFmLo0KFo164d8vLysHr1avz111/S6fyqaLVaREdHl7sFu0xeXh70ej2Kiorw888/48MPP0RiYiK2bt0q+01Bs2bNMHz4cMyePRtDhgzBI488cl/7oiFoqG0H2w3zajcAth0AeKv7/ajsdlUhhMjOzhaTJ08W7dq1E2q1WrRp00Y8++yzslv62rVrJ1asWCGbLy4uTgAwervq3fLz84Wjo+NDv9X91q1bYs6cOaJnz57C3t5eWFtbi44dO4rw8HBRWFgohLhzq+K6devEk08+KTQajXBxcRHx8fGy5cyePVs4ODgIW1tbERQUJFasWCG7NdLYNp88eVL4+fmJFi1aCI1GIx599FGxZs0aWZlPPvlEeHh4CLVaLZo1ayYef/xxsX37dmk6KrlltUxycrLw8/MTzZo1E1qtVri7u4tZs2aJ7OxsIcTft6zefeuxEEIMGTJEACh3y2p13uP7ERwcLACUG0JCQsSff/4p3njjDdGsWTNhbW0tnnvuOZGTkyPNu3fvXjFixAjh7Ows1Gq1cHJyEv7+/uLAgQNSmYpuV73b7du3RefOnY3erlo2aLVa0b59exEcHCzS09ONbktKSooAID799NMH2if1gRLbDrYb5tNuCMG2o4zq/6+QqEaoVCrs2LGjwh5Eie7173//GzNnzkR2djbUanVdV4fqANsNuh8P0nbwshcR1YnCwkLk5ORg4cKFeO211xh8iKhaaqLt4A+eiahOLF68GO7u7tDpdAgLC6vr6hBRPVETbQcvexEREZGi8MwPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpCsMPERERKQrDDxERESkKww8REREpyv8DoxNlZL+ylJoAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
