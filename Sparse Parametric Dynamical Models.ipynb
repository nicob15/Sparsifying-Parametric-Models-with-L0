{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Learning Sparse Dynamical Models\n",
    "\n",
    "We consider the case of learning the dynamics of a pendulum. In particular, we aim to learn the transition function $T:\\mathcal{S}\\times \\mathcal{A}\\rightarrow \\mathcal{S}$, predicting the next state $\\hat{\\mathbf{s}}_{t+1}$ from the current state $\\mathbf{s}_t$ and action $\\mathbf{a}_t$: $$\\hat{\\mathbf{s}}_{t+1}=T(\\mathbf{s}_t, \\mathbf{a}_t).$$ We will approximate $T(\\mathbf{s}_t, \\mathbf{a}_t)$ using:\n",
    "* a fully-connected neural network (fcnn_model),\n",
    "* a fully-connected neural network sparsified by the L$_0$ regularization (sparsefcnn_model), and\n",
    "* a SINDy-like architecture again sparsified by the L$_0$ regularization (l0sindy_model)."
   ],
   "id": "7b7ca3e2b76dfbb6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-15T16:57:45.759395Z",
     "start_time": "2024-07-15T16:57:45.049504Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds\n",
    "seed = 23524\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We first create a training set composed of 1000 episodes of 200 steps each. The actions are sampled from a random policy.",
   "id": "8ddbdef861816cc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "render = False\n",
    "if render:\n",
    "    env = gym.make('Pendulum-v1', g=9.81, render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make('Pendulum-v1', g=9.81)\n",
    "max_episodes = 1000\n",
    "max_steps = 200\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "buf_dim = int(max_episodes*max_steps)\n",
    "\n",
    "training_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=buf_dim)\n",
    "\n",
    "# create training set\n",
    "for episode in range(max_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    for steps in range(max_steps+1):\n",
    "        action = env.action_space.sample()\n",
    "        next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        training_buffer.store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "print(\"Finished creating the training set\")"
   ],
   "id": "f30d49f053f6de6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Secondly, we create the test set that we will use to evaluate the models.",
   "id": "8273bd38694c4609"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T16:57:57.229204Z",
     "start_time": "2024-07-15T16:57:56.185910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create test set\n",
    "max_episodes_test = 100\n",
    "buf_dim = int(max_episodes*max_steps)\n",
    "\n",
    "testing_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=buf_dim)\n",
    "\n",
    "# create training set\n",
    "for episode in range(max_episodes_test):\n",
    "    observation, _ = env.reset()\n",
    "    for steps in range(max_steps + 1):\n",
    "        action = env.action_space.sample()\n",
    "        next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        testing_buffer.store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "print(\"Finished creating the test set\")"
   ],
   "id": "5c45b1ee713bfd84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating the test set\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After creating train and test set, we are now ready to train the three models. We utilize the same learning rate, batch size, and number of traning epochs.",
   "id": "1dd2318732f7de44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T16:57:57.484008Z",
     "start_time": "2024-07-15T16:57:57.230083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.models import FCNN, SparseFCNN, L0SINDy_dynamics\n",
    "from utils.trainer import train_eval_dynamics_model\n",
    "import torch\n",
    "\n",
    "# number of hidden units used by the fcnn_model and the sparsefcnn_model\n",
    "h_dim = 128\n",
    "\n",
    "# shared hyperparameter of the experiment\n",
    "lr = 3e-4\n",
    "batch_size = 256\n",
    "num_epochs = 500"
   ],
   "id": "5b9d333e476e4824",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train and evaluate the fcnn_model.",
   "id": "74aea84006ae75c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fcnn_model = FCNN(input_dim=obs_dim+act_dim, output_dim=obs_dim, h_dim=h_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    fcnn_model = fcnn_model.cuda()\n",
    "\n",
    "optimizer_fcnn = torch.optim.Adam([\n",
    "    {'params': fcnn_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_fcnn = train_eval_dynamics_model(fcnn_model, optimizer_fcnn, training_buffer, testing_buffer, batch_size, num_epochs)\n",
    "print(\"Best testing error FCNN is {} and it was found at epoch {}\".format(metrics_fcnn[2], metrics_fcnn[3]))"
   ],
   "id": "2cac37c5b681ba1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train and evaluate the sparsefcnn_model.",
   "id": "9cfbed8187fd361a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T17:19:26.803905Z",
     "start_time": "2024-07-15T17:03:42.980625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reg_coefficient = 0.000005\n",
    "sparsefcnn_model = SparseFCNN(input_dim=obs_dim+act_dim, output_dim=obs_dim, h_dim=h_dim, lambda_coeff=reg_coefficient)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    sparsefcnn_model = sparsefcnn_model.cuda()\n",
    "\n",
    "optimizer_sparsefcnn = torch.optim.Adam([\n",
    "    {'params': sparsefcnn_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_sparsefcnn = train_eval_dynamics_model(sparsefcnn_model, optimizer_sparsefcnn, training_buffer, testing_buffer,\n",
    "                                               batch_size, num_epochs, l0=True)\n",
    "print(\"Best testing error sparse FCNN is {} and it was found at epoch {}\".format(metrics_sparsefcnn[2], metrics_sparsefcnn[3]))"
   ],
   "id": "5f6094f05663b753",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0Dense(4 -> 128, droprate_init=0.5, lamba=5e-06, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False)\n",
      "L0Dense(128 -> 128, droprate_init=0.5, lamba=5e-06, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False)\n",
      "L0Dense(128 -> 3, droprate_init=0.5, lamba=5e-06, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/Documents/Sparsifying-Parametric-Models-with-L0/utils/l0_layer.py:51: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.weights, mode='fan_out')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average train loss: 1.9357626689\n",
      "====> Epoch: 0 Average L0 reg loss: 0.0718839753\n",
      "====> Epoch: 0 Average eval loss: 0.3897535205\n",
      "====> Epoch: 1 Average train loss: 1.5835513903\n",
      "====> Epoch: 1 Average L0 reg loss: 0.0718958646\n",
      "====> Epoch: 1 Average eval loss: 0.3284494877\n",
      "====> Epoch: 2 Average train loss: 1.5679122517\n",
      "====> Epoch: 2 Average L0 reg loss: 0.0719184796\n",
      "====> Epoch: 2 Average eval loss: 0.2783397436\n",
      "====> Epoch: 3 Average train loss: 1.2588246811\n",
      "====> Epoch: 3 Average L0 reg loss: 0.0719482836\n",
      "====> Epoch: 3 Average eval loss: 0.2147325277\n",
      "====> Epoch: 4 Average train loss: 1.2533747192\n",
      "====> Epoch: 4 Average L0 reg loss: 0.0719848296\n",
      "====> Epoch: 4 Average eval loss: 0.2362017483\n",
      "====> Epoch: 5 Average train loss: 1.2129035863\n",
      "====> Epoch: 5 Average L0 reg loss: 0.0720251836\n",
      "====> Epoch: 5 Average eval loss: 0.2104419321\n",
      "====> Epoch: 6 Average train loss: 1.1388231519\n",
      "====> Epoch: 6 Average L0 reg loss: 0.0720646028\n",
      "====> Epoch: 6 Average eval loss: 0.1744805723\n",
      "====> Epoch: 7 Average train loss: 0.9722704003\n",
      "====> Epoch: 7 Average L0 reg loss: 0.0721219668\n",
      "====> Epoch: 7 Average eval loss: 0.1466002017\n",
      "====> Epoch: 8 Average train loss: 0.9516997010\n",
      "====> Epoch: 8 Average L0 reg loss: 0.0721860405\n",
      "====> Epoch: 8 Average eval loss: 0.1777614206\n",
      "====> Epoch: 9 Average train loss: 0.8712240051\n",
      "====> Epoch: 9 Average L0 reg loss: 0.0722426077\n",
      "====> Epoch: 9 Average eval loss: 0.1385658383\n",
      "====> Epoch: 10 Average train loss: 0.7926336592\n",
      "====> Epoch: 10 Average L0 reg loss: 0.0723039222\n",
      "====> Epoch: 10 Average eval loss: 0.1187121570\n",
      "====> Epoch: 11 Average train loss: 0.7845877286\n",
      "====> Epoch: 11 Average L0 reg loss: 0.0723748990\n",
      "====> Epoch: 11 Average eval loss: 0.1205156446\n",
      "====> Epoch: 12 Average train loss: 0.7638918010\n",
      "====> Epoch: 12 Average L0 reg loss: 0.0724394274\n",
      "====> Epoch: 12 Average eval loss: 0.0931822434\n",
      "====> Epoch: 13 Average train loss: 0.6478528088\n",
      "====> Epoch: 13 Average L0 reg loss: 0.0725128007\n",
      "====> Epoch: 13 Average eval loss: 0.0850953683\n",
      "====> Epoch: 14 Average train loss: 0.6740678011\n",
      "====> Epoch: 14 Average L0 reg loss: 0.0725762572\n",
      "====> Epoch: 14 Average eval loss: 0.0742495432\n",
      "====> Epoch: 15 Average train loss: 0.6490232304\n",
      "====> Epoch: 15 Average L0 reg loss: 0.0726579948\n",
      "====> Epoch: 15 Average eval loss: 0.0686615258\n",
      "====> Epoch: 16 Average train loss: 0.5738156411\n",
      "====> Epoch: 16 Average L0 reg loss: 0.0727356295\n",
      "====> Epoch: 16 Average eval loss: 0.0617654286\n",
      "====> Epoch: 17 Average train loss: 0.5432966157\n",
      "====> Epoch: 17 Average L0 reg loss: 0.0728042206\n",
      "====> Epoch: 17 Average eval loss: 0.0506822653\n",
      "====> Epoch: 18 Average train loss: 0.5160946090\n",
      "====> Epoch: 18 Average L0 reg loss: 0.0728901701\n",
      "====> Epoch: 18 Average eval loss: 0.0495565385\n",
      "====> Epoch: 19 Average train loss: 0.5081769180\n",
      "====> Epoch: 19 Average L0 reg loss: 0.0729692088\n",
      "====> Epoch: 19 Average eval loss: 0.0527811460\n",
      "====> Epoch: 20 Average train loss: 0.5133263719\n",
      "====> Epoch: 20 Average L0 reg loss: 0.0730469542\n",
      "====> Epoch: 20 Average eval loss: 0.0447499268\n",
      "====> Epoch: 21 Average train loss: 0.3959978410\n",
      "====> Epoch: 21 Average L0 reg loss: 0.0731305780\n",
      "====> Epoch: 21 Average eval loss: 0.0390515514\n",
      "====> Epoch: 22 Average train loss: 0.3604710821\n",
      "====> Epoch: 22 Average L0 reg loss: 0.0732114648\n",
      "====> Epoch: 22 Average eval loss: 0.0539122298\n",
      "====> Epoch: 23 Average train loss: 0.3631969221\n",
      "====> Epoch: 23 Average L0 reg loss: 0.0732962225\n",
      "====> Epoch: 23 Average eval loss: 0.0434270836\n",
      "====> Epoch: 24 Average train loss: 0.3548908037\n",
      "====> Epoch: 24 Average L0 reg loss: 0.0733780726\n",
      "====> Epoch: 24 Average eval loss: 0.0725547448\n",
      "====> Epoch: 25 Average train loss: 0.2998655468\n",
      "====> Epoch: 25 Average L0 reg loss: 0.0734640843\n",
      "====> Epoch: 25 Average eval loss: 0.0436823405\n",
      "====> Epoch: 26 Average train loss: 0.3579177803\n",
      "====> Epoch: 26 Average L0 reg loss: 0.0735503854\n",
      "====> Epoch: 26 Average eval loss: 0.0479908139\n",
      "====> Epoch: 27 Average train loss: 0.3510213983\n",
      "====> Epoch: 27 Average L0 reg loss: 0.0736309141\n",
      "====> Epoch: 27 Average eval loss: 0.0503776930\n",
      "====> Epoch: 28 Average train loss: 0.2545780226\n",
      "====> Epoch: 28 Average L0 reg loss: 0.0737074161\n",
      "====> Epoch: 28 Average eval loss: 0.0408462584\n",
      "====> Epoch: 29 Average train loss: 0.2924916383\n",
      "====> Epoch: 29 Average L0 reg loss: 0.0737892584\n",
      "====> Epoch: 29 Average eval loss: 0.0299119800\n",
      "====> Epoch: 30 Average train loss: 0.3019840285\n",
      "====> Epoch: 30 Average L0 reg loss: 0.0738612565\n",
      "====> Epoch: 30 Average eval loss: 0.0318900831\n",
      "====> Epoch: 31 Average train loss: 0.2512205552\n",
      "====> Epoch: 31 Average L0 reg loss: 0.0739445197\n",
      "====> Epoch: 31 Average eval loss: 0.0431853272\n",
      "====> Epoch: 32 Average train loss: 0.2662451833\n",
      "====> Epoch: 32 Average L0 reg loss: 0.0740182849\n",
      "====> Epoch: 32 Average eval loss: 0.0847169608\n",
      "====> Epoch: 33 Average train loss: 0.2163797854\n",
      "====> Epoch: 33 Average L0 reg loss: 0.0740955263\n",
      "====> Epoch: 33 Average eval loss: 0.0859812647\n",
      "====> Epoch: 34 Average train loss: 0.2514397305\n",
      "====> Epoch: 34 Average L0 reg loss: 0.0741790786\n",
      "====> Epoch: 34 Average eval loss: 0.0219699908\n",
      "====> Epoch: 35 Average train loss: 0.1861721838\n",
      "====> Epoch: 35 Average L0 reg loss: 0.0742566368\n",
      "====> Epoch: 35 Average eval loss: 0.0293267742\n",
      "====> Epoch: 36 Average train loss: 0.2334687298\n",
      "====> Epoch: 36 Average L0 reg loss: 0.0743288862\n",
      "====> Epoch: 36 Average eval loss: 0.0196575671\n",
      "====> Epoch: 37 Average train loss: 0.1951414516\n",
      "====> Epoch: 37 Average L0 reg loss: 0.0744077161\n",
      "====> Epoch: 37 Average eval loss: 0.0655133873\n",
      "====> Epoch: 38 Average train loss: 0.2008591245\n",
      "====> Epoch: 38 Average L0 reg loss: 0.0744796733\n",
      "====> Epoch: 38 Average eval loss: 0.0692889094\n",
      "====> Epoch: 39 Average train loss: 0.1711041319\n",
      "====> Epoch: 39 Average L0 reg loss: 0.0745555612\n",
      "====> Epoch: 39 Average eval loss: 0.0517710336\n",
      "====> Epoch: 40 Average train loss: 0.1505103016\n",
      "====> Epoch: 40 Average L0 reg loss: 0.0746302665\n",
      "====> Epoch: 40 Average eval loss: 0.0542639084\n",
      "====> Epoch: 41 Average train loss: 0.1783818886\n",
      "====> Epoch: 41 Average L0 reg loss: 0.0747029054\n",
      "====> Epoch: 41 Average eval loss: 0.0484661944\n",
      "====> Epoch: 42 Average train loss: 0.1618582785\n",
      "====> Epoch: 42 Average L0 reg loss: 0.0747706132\n",
      "====> Epoch: 42 Average eval loss: 0.0328091234\n",
      "====> Epoch: 43 Average train loss: 0.1560380609\n",
      "====> Epoch: 43 Average L0 reg loss: 0.0748298418\n",
      "====> Epoch: 43 Average eval loss: 0.0767257139\n",
      "====> Epoch: 44 Average train loss: 0.1433965422\n",
      "====> Epoch: 44 Average L0 reg loss: 0.0748891854\n",
      "====> Epoch: 44 Average eval loss: 0.0380857661\n",
      "====> Epoch: 45 Average train loss: 0.1419862149\n",
      "====> Epoch: 45 Average L0 reg loss: 0.0749648262\n",
      "====> Epoch: 45 Average eval loss: 0.0157508105\n",
      "====> Epoch: 46 Average train loss: 0.1494047852\n",
      "====> Epoch: 46 Average L0 reg loss: 0.0750298789\n",
      "====> Epoch: 46 Average eval loss: 0.0405735746\n",
      "====> Epoch: 47 Average train loss: 0.1195017628\n",
      "====> Epoch: 47 Average L0 reg loss: 0.0750973968\n",
      "====> Epoch: 47 Average eval loss: 0.0199101008\n",
      "====> Epoch: 48 Average train loss: 0.1391216551\n",
      "====> Epoch: 48 Average L0 reg loss: 0.0751529086\n",
      "====> Epoch: 48 Average eval loss: 0.1238357797\n",
      "====> Epoch: 49 Average train loss: 0.1355656657\n",
      "====> Epoch: 49 Average L0 reg loss: 0.0752080034\n",
      "====> Epoch: 49 Average eval loss: 0.0911185220\n",
      "====> Epoch: 50 Average train loss: 0.1311597783\n",
      "====> Epoch: 50 Average L0 reg loss: 0.0752594177\n",
      "====> Epoch: 50 Average eval loss: 0.0473264158\n",
      "====> Epoch: 51 Average train loss: 0.1257916313\n",
      "====> Epoch: 51 Average L0 reg loss: 0.0753112330\n",
      "====> Epoch: 51 Average eval loss: 0.0240498260\n",
      "====> Epoch: 52 Average train loss: 0.1316349171\n",
      "====> Epoch: 52 Average L0 reg loss: 0.0753675617\n",
      "====> Epoch: 52 Average eval loss: 0.0481930114\n",
      "====> Epoch: 53 Average train loss: 0.1269506589\n",
      "====> Epoch: 53 Average L0 reg loss: 0.0754244789\n",
      "====> Epoch: 53 Average eval loss: 0.0458080880\n",
      "====> Epoch: 54 Average train loss: 0.1220489476\n",
      "====> Epoch: 54 Average L0 reg loss: 0.0754770931\n",
      "====> Epoch: 54 Average eval loss: 0.0481881239\n",
      "====> Epoch: 55 Average train loss: 0.1216449507\n",
      "====> Epoch: 55 Average L0 reg loss: 0.0755270459\n",
      "====> Epoch: 55 Average eval loss: 0.0810142457\n",
      "====> Epoch: 56 Average train loss: 0.0901809670\n",
      "====> Epoch: 56 Average L0 reg loss: 0.0755810619\n",
      "====> Epoch: 56 Average eval loss: 0.0668685362\n",
      "====> Epoch: 57 Average train loss: 0.0841728431\n",
      "====> Epoch: 57 Average L0 reg loss: 0.0756285364\n",
      "====> Epoch: 57 Average eval loss: 0.0776109472\n",
      "====> Epoch: 58 Average train loss: 0.0866019438\n",
      "====> Epoch: 58 Average L0 reg loss: 0.0756635879\n",
      "====> Epoch: 58 Average eval loss: 0.0350225493\n",
      "====> Epoch: 59 Average train loss: 0.0832215487\n",
      "====> Epoch: 59 Average L0 reg loss: 0.0756967732\n",
      "====> Epoch: 59 Average eval loss: 0.0543911904\n",
      "====> Epoch: 60 Average train loss: 0.1163816358\n",
      "====> Epoch: 60 Average L0 reg loss: 0.0757276001\n",
      "====> Epoch: 60 Average eval loss: 0.0674696863\n",
      "====> Epoch: 61 Average train loss: 0.1053422944\n",
      "====> Epoch: 61 Average L0 reg loss: 0.0757591621\n",
      "====> Epoch: 61 Average eval loss: 0.0548679456\n",
      "====> Epoch: 62 Average train loss: 0.0845832283\n",
      "====> Epoch: 62 Average L0 reg loss: 0.0757940390\n",
      "====> Epoch: 62 Average eval loss: 0.1095330715\n",
      "====> Epoch: 63 Average train loss: 0.0990851445\n",
      "====> Epoch: 63 Average L0 reg loss: 0.0758334710\n",
      "====> Epoch: 63 Average eval loss: 0.0651956797\n",
      "====> Epoch: 64 Average train loss: 0.0832901402\n",
      "====> Epoch: 64 Average L0 reg loss: 0.0758603040\n",
      "====> Epoch: 64 Average eval loss: 0.0886900946\n",
      "====> Epoch: 65 Average train loss: 0.0795608924\n",
      "====> Epoch: 65 Average L0 reg loss: 0.0758908093\n",
      "====> Epoch: 65 Average eval loss: 0.0830572248\n",
      "====> Epoch: 66 Average train loss: 0.0753313967\n",
      "====> Epoch: 66 Average L0 reg loss: 0.0759113252\n",
      "====> Epoch: 66 Average eval loss: 0.0611592792\n",
      "====> Epoch: 67 Average train loss: 0.0809806704\n",
      "====> Epoch: 67 Average L0 reg loss: 0.0759280221\n",
      "====> Epoch: 67 Average eval loss: 0.0938189700\n",
      "====> Epoch: 68 Average train loss: 0.0864056180\n",
      "====> Epoch: 68 Average L0 reg loss: 0.0759356132\n",
      "====> Epoch: 68 Average eval loss: 0.1131773740\n",
      "====> Epoch: 69 Average train loss: 0.0825544976\n",
      "====> Epoch: 69 Average L0 reg loss: 0.0759443771\n",
      "====> Epoch: 69 Average eval loss: 0.0431946442\n",
      "====> Epoch: 70 Average train loss: 0.0934806936\n",
      "====> Epoch: 70 Average L0 reg loss: 0.0759632234\n",
      "====> Epoch: 70 Average eval loss: 0.0645478815\n",
      "====> Epoch: 71 Average train loss: 0.0706557055\n",
      "====> Epoch: 71 Average L0 reg loss: 0.0759839605\n",
      "====> Epoch: 71 Average eval loss: 0.1028566435\n",
      "====> Epoch: 72 Average train loss: 0.0700421093\n",
      "====> Epoch: 72 Average L0 reg loss: 0.0759970256\n",
      "====> Epoch: 72 Average eval loss: 0.1621777415\n",
      "====> Epoch: 73 Average train loss: 0.0609313681\n",
      "====> Epoch: 73 Average L0 reg loss: 0.0760059103\n",
      "====> Epoch: 73 Average eval loss: 0.0920598954\n",
      "====> Epoch: 74 Average train loss: 0.0622975719\n",
      "====> Epoch: 74 Average L0 reg loss: 0.0760207097\n",
      "====> Epoch: 74 Average eval loss: 0.0365609117\n",
      "====> Epoch: 75 Average train loss: 0.0617552746\n",
      "====> Epoch: 75 Average L0 reg loss: 0.0760242292\n",
      "====> Epoch: 75 Average eval loss: 0.0610053949\n",
      "====> Epoch: 76 Average train loss: 0.0618226099\n",
      "====> Epoch: 76 Average L0 reg loss: 0.0760295297\n",
      "====> Epoch: 76 Average eval loss: 0.1288809925\n",
      "====> Epoch: 77 Average train loss: 0.0493814243\n",
      "====> Epoch: 77 Average L0 reg loss: 0.0760384096\n",
      "====> Epoch: 77 Average eval loss: 0.0906434953\n",
      "====> Epoch: 78 Average train loss: 0.0701663429\n",
      "====> Epoch: 78 Average L0 reg loss: 0.0760334697\n",
      "====> Epoch: 78 Average eval loss: 0.0969151035\n",
      "====> Epoch: 79 Average train loss: 0.0794326105\n",
      "====> Epoch: 79 Average L0 reg loss: 0.0760198239\n",
      "====> Epoch: 79 Average eval loss: 0.0960011706\n",
      "====> Epoch: 80 Average train loss: 0.0568973416\n",
      "====> Epoch: 80 Average L0 reg loss: 0.0760184244\n",
      "====> Epoch: 80 Average eval loss: 0.0888905898\n",
      "====> Epoch: 81 Average train loss: 0.0521679703\n",
      "====> Epoch: 81 Average L0 reg loss: 0.0760118338\n",
      "====> Epoch: 81 Average eval loss: 0.0896544307\n",
      "====> Epoch: 82 Average train loss: 0.0495392673\n",
      "====> Epoch: 82 Average L0 reg loss: 0.0759990392\n",
      "====> Epoch: 82 Average eval loss: 0.0480015315\n",
      "====> Epoch: 83 Average train loss: 0.0582631684\n",
      "====> Epoch: 83 Average L0 reg loss: 0.0759714370\n",
      "====> Epoch: 83 Average eval loss: 0.1572357118\n",
      "====> Epoch: 84 Average train loss: 0.0594850231\n",
      "====> Epoch: 84 Average L0 reg loss: 0.0759463334\n",
      "====> Epoch: 84 Average eval loss: 0.0705495775\n",
      "====> Epoch: 85 Average train loss: 0.0576817801\n",
      "====> Epoch: 85 Average L0 reg loss: 0.0759155052\n",
      "====> Epoch: 85 Average eval loss: 0.0615918338\n",
      "====> Epoch: 86 Average train loss: 0.0462137037\n",
      "====> Epoch: 86 Average L0 reg loss: 0.0758781138\n",
      "====> Epoch: 86 Average eval loss: 0.0535737686\n",
      "====> Epoch: 87 Average train loss: 0.0500605615\n",
      "====> Epoch: 87 Average L0 reg loss: 0.0758566071\n",
      "====> Epoch: 87 Average eval loss: 0.1240949705\n",
      "====> Epoch: 88 Average train loss: 0.0547061317\n",
      "====> Epoch: 88 Average L0 reg loss: 0.0758251772\n",
      "====> Epoch: 88 Average eval loss: 0.0535516441\n",
      "====> Epoch: 89 Average train loss: 0.0477403981\n",
      "====> Epoch: 89 Average L0 reg loss: 0.0757843842\n",
      "====> Epoch: 89 Average eval loss: 0.1519540250\n",
      "====> Epoch: 90 Average train loss: 0.0412324609\n",
      "====> Epoch: 90 Average L0 reg loss: 0.0757502951\n",
      "====> Epoch: 90 Average eval loss: 0.1132308096\n",
      "====> Epoch: 91 Average train loss: 0.0427308456\n",
      "====> Epoch: 91 Average L0 reg loss: 0.0757127973\n",
      "====> Epoch: 91 Average eval loss: 0.0867454261\n",
      "====> Epoch: 92 Average train loss: 0.0489181294\n",
      "====> Epoch: 92 Average L0 reg loss: 0.0756734588\n",
      "====> Epoch: 92 Average eval loss: 0.1134202629\n",
      "====> Epoch: 93 Average train loss: 0.0488965055\n",
      "====> Epoch: 93 Average L0 reg loss: 0.0756351178\n",
      "====> Epoch: 93 Average eval loss: 0.0857295245\n",
      "====> Epoch: 94 Average train loss: 0.0430402889\n",
      "====> Epoch: 94 Average L0 reg loss: 0.0755891884\n",
      "====> Epoch: 94 Average eval loss: 0.1092492118\n",
      "====> Epoch: 95 Average train loss: 0.0402671953\n",
      "====> Epoch: 95 Average L0 reg loss: 0.0755343011\n",
      "====> Epoch: 95 Average eval loss: 0.0820967630\n",
      "====> Epoch: 96 Average train loss: 0.0367518799\n",
      "====> Epoch: 96 Average L0 reg loss: 0.0754746459\n",
      "====> Epoch: 96 Average eval loss: 0.1635044664\n",
      "====> Epoch: 97 Average train loss: 0.0303366464\n",
      "====> Epoch: 97 Average L0 reg loss: 0.0754093556\n",
      "====> Epoch: 97 Average eval loss: 0.0533141755\n",
      "====> Epoch: 98 Average train loss: 0.0340563389\n",
      "====> Epoch: 98 Average L0 reg loss: 0.0753273136\n",
      "====> Epoch: 98 Average eval loss: 0.0901759192\n",
      "====> Epoch: 99 Average train loss: 0.0341418430\n",
      "====> Epoch: 99 Average L0 reg loss: 0.0752256980\n",
      "====> Epoch: 99 Average eval loss: 0.1019644067\n",
      "====> Epoch: 100 Average train loss: 0.0476444925\n",
      "====> Epoch: 100 Average L0 reg loss: 0.0751211064\n",
      "====> Epoch: 100 Average eval loss: 0.0795845613\n",
      "====> Epoch: 101 Average train loss: 0.0404083951\n",
      "====> Epoch: 101 Average L0 reg loss: 0.0750136826\n",
      "====> Epoch: 101 Average eval loss: 0.0807457492\n",
      "====> Epoch: 102 Average train loss: 0.0404806671\n",
      "====> Epoch: 102 Average L0 reg loss: 0.0748926143\n",
      "====> Epoch: 102 Average eval loss: 0.0965571776\n",
      "====> Epoch: 103 Average train loss: 0.0535393263\n",
      "====> Epoch: 103 Average L0 reg loss: 0.0747692966\n",
      "====> Epoch: 103 Average eval loss: 0.0549886897\n",
      "====> Epoch: 104 Average train loss: 0.0449511555\n",
      "====> Epoch: 104 Average L0 reg loss: 0.0746523143\n",
      "====> Epoch: 104 Average eval loss: 0.1235864460\n",
      "====> Epoch: 105 Average train loss: 0.0305696278\n",
      "====> Epoch: 105 Average L0 reg loss: 0.0745345851\n",
      "====> Epoch: 105 Average eval loss: 0.0796982720\n",
      "====> Epoch: 106 Average train loss: 0.0437293512\n",
      "====> Epoch: 106 Average L0 reg loss: 0.0743931600\n",
      "====> Epoch: 106 Average eval loss: 0.0820318535\n",
      "====> Epoch: 107 Average train loss: 0.0461179059\n",
      "====> Epoch: 107 Average L0 reg loss: 0.0742417809\n",
      "====> Epoch: 107 Average eval loss: 0.1102203727\n",
      "====> Epoch: 108 Average train loss: 0.0422153916\n",
      "====> Epoch: 108 Average L0 reg loss: 0.0740830694\n",
      "====> Epoch: 108 Average eval loss: 0.0600836016\n",
      "====> Epoch: 109 Average train loss: 0.0322782671\n",
      "====> Epoch: 109 Average L0 reg loss: 0.0739018196\n",
      "====> Epoch: 109 Average eval loss: 0.0626586899\n",
      "====> Epoch: 110 Average train loss: 0.0372986512\n",
      "====> Epoch: 110 Average L0 reg loss: 0.0737151909\n",
      "====> Epoch: 110 Average eval loss: 0.0763616860\n",
      "====> Epoch: 111 Average train loss: 0.0342032403\n",
      "====> Epoch: 111 Average L0 reg loss: 0.0735120231\n",
      "====> Epoch: 111 Average eval loss: 0.0572903268\n",
      "====> Epoch: 112 Average train loss: 0.0252774908\n",
      "====> Epoch: 112 Average L0 reg loss: 0.0732925348\n",
      "====> Epoch: 112 Average eval loss: 0.0622211173\n",
      "====> Epoch: 113 Average train loss: 0.0349780138\n",
      "====> Epoch: 113 Average L0 reg loss: 0.0730604510\n",
      "====> Epoch: 113 Average eval loss: 0.0555578507\n",
      "====> Epoch: 114 Average train loss: 0.0393880036\n",
      "====> Epoch: 114 Average L0 reg loss: 0.0728106219\n",
      "====> Epoch: 114 Average eval loss: 0.0926628932\n",
      "====> Epoch: 115 Average train loss: 0.0262682716\n",
      "====> Epoch: 115 Average L0 reg loss: 0.0725354069\n",
      "====> Epoch: 115 Average eval loss: 0.0407598317\n",
      "====> Epoch: 116 Average train loss: 0.0270875713\n",
      "====> Epoch: 116 Average L0 reg loss: 0.0722444639\n",
      "====> Epoch: 116 Average eval loss: 0.0712545291\n",
      "====> Epoch: 117 Average train loss: 0.0225414189\n",
      "====> Epoch: 117 Average L0 reg loss: 0.0719388677\n",
      "====> Epoch: 117 Average eval loss: 0.0421494618\n",
      "====> Epoch: 118 Average train loss: 0.0324756160\n",
      "====> Epoch: 118 Average L0 reg loss: 0.0716143752\n",
      "====> Epoch: 118 Average eval loss: 0.0777893215\n",
      "====> Epoch: 119 Average train loss: 0.0386322091\n",
      "====> Epoch: 119 Average L0 reg loss: 0.0712653679\n",
      "====> Epoch: 119 Average eval loss: 0.0372811668\n",
      "====> Epoch: 120 Average train loss: 0.0277348337\n",
      "====> Epoch: 120 Average L0 reg loss: 0.0708987377\n",
      "====> Epoch: 120 Average eval loss: 0.0687810555\n",
      "====> Epoch: 121 Average train loss: 0.0318396747\n",
      "====> Epoch: 121 Average L0 reg loss: 0.0705025651\n",
      "====> Epoch: 121 Average eval loss: 0.0844568238\n",
      "====> Epoch: 122 Average train loss: 0.0298883941\n",
      "====> Epoch: 122 Average L0 reg loss: 0.0700731280\n",
      "====> Epoch: 122 Average eval loss: 0.0905802026\n",
      "====> Epoch: 123 Average train loss: 0.0301590636\n",
      "====> Epoch: 123 Average L0 reg loss: 0.0695943376\n",
      "====> Epoch: 123 Average eval loss: 0.0402303971\n",
      "====> Epoch: 124 Average train loss: 0.0267797945\n",
      "====> Epoch: 124 Average L0 reg loss: 0.0690908998\n",
      "====> Epoch: 124 Average eval loss: 0.0407236740\n",
      "====> Epoch: 125 Average train loss: 0.0372281175\n",
      "====> Epoch: 125 Average L0 reg loss: 0.0685389948\n",
      "====> Epoch: 125 Average eval loss: 0.0478468686\n",
      "====> Epoch: 126 Average train loss: 0.0195175729\n",
      "====> Epoch: 126 Average L0 reg loss: 0.0679629238\n",
      "====> Epoch: 126 Average eval loss: 0.0553014129\n",
      "====> Epoch: 127 Average train loss: 0.0237873136\n",
      "====> Epoch: 127 Average L0 reg loss: 0.0673417686\n",
      "====> Epoch: 127 Average eval loss: 0.0646620765\n",
      "====> Epoch: 128 Average train loss: 0.0351263138\n",
      "====> Epoch: 128 Average L0 reg loss: 0.0666857568\n",
      "====> Epoch: 128 Average eval loss: 0.0622476563\n",
      "====> Epoch: 129 Average train loss: 0.0271115024\n",
      "====> Epoch: 129 Average L0 reg loss: 0.0660017282\n",
      "====> Epoch: 129 Average eval loss: 0.0668857992\n",
      "====> Epoch: 130 Average train loss: 0.0208443103\n",
      "====> Epoch: 130 Average L0 reg loss: 0.0653001107\n",
      "====> Epoch: 130 Average eval loss: 0.0250572022\n",
      "====> Epoch: 131 Average train loss: 0.0264689503\n",
      "====> Epoch: 131 Average L0 reg loss: 0.0645502151\n",
      "====> Epoch: 131 Average eval loss: 0.0480857827\n",
      "====> Epoch: 132 Average train loss: 0.0280744589\n",
      "====> Epoch: 132 Average L0 reg loss: 0.0637532579\n",
      "====> Epoch: 132 Average eval loss: 0.0274020303\n",
      "====> Epoch: 133 Average train loss: 0.0236323744\n",
      "====> Epoch: 133 Average L0 reg loss: 0.0629362125\n",
      "====> Epoch: 133 Average eval loss: 0.0195914954\n",
      "====> Epoch: 134 Average train loss: 0.0241121412\n",
      "====> Epoch: 134 Average L0 reg loss: 0.0620797456\n",
      "====> Epoch: 134 Average eval loss: 0.0370715745\n",
      "====> Epoch: 135 Average train loss: 0.0202720238\n",
      "====> Epoch: 135 Average L0 reg loss: 0.0612099237\n",
      "====> Epoch: 135 Average eval loss: 0.0504880548\n",
      "====> Epoch: 136 Average train loss: 0.0195200509\n",
      "====> Epoch: 136 Average L0 reg loss: 0.0602926387\n",
      "====> Epoch: 136 Average eval loss: 0.0423343107\n",
      "====> Epoch: 137 Average train loss: 0.0197244148\n",
      "====> Epoch: 137 Average L0 reg loss: 0.0593168614\n",
      "====> Epoch: 137 Average eval loss: 0.0495544821\n",
      "====> Epoch: 138 Average train loss: 0.0201776586\n",
      "====> Epoch: 138 Average L0 reg loss: 0.0583111990\n",
      "====> Epoch: 138 Average eval loss: 0.0555431172\n",
      "====> Epoch: 139 Average train loss: 0.0159891759\n",
      "====> Epoch: 139 Average L0 reg loss: 0.0572727596\n",
      "====> Epoch: 139 Average eval loss: 0.0350884832\n",
      "====> Epoch: 140 Average train loss: 0.0199391908\n",
      "====> Epoch: 140 Average L0 reg loss: 0.0561834453\n",
      "====> Epoch: 140 Average eval loss: 0.0448672846\n",
      "====> Epoch: 141 Average train loss: 0.0302401582\n",
      "====> Epoch: 141 Average L0 reg loss: 0.0550766808\n",
      "====> Epoch: 141 Average eval loss: 0.0232588388\n",
      "====> Epoch: 142 Average train loss: 0.0231328477\n",
      "====> Epoch: 142 Average L0 reg loss: 0.0539746285\n",
      "====> Epoch: 142 Average eval loss: 0.0584228076\n",
      "====> Epoch: 143 Average train loss: 0.0240547300\n",
      "====> Epoch: 143 Average L0 reg loss: 0.0528633411\n",
      "====> Epoch: 143 Average eval loss: 0.0435937755\n",
      "====> Epoch: 144 Average train loss: 0.0170336115\n",
      "====> Epoch: 144 Average L0 reg loss: 0.0517720885\n",
      "====> Epoch: 144 Average eval loss: 0.0297559090\n",
      "====> Epoch: 145 Average train loss: 0.0215235641\n",
      "====> Epoch: 145 Average L0 reg loss: 0.0506914568\n",
      "====> Epoch: 145 Average eval loss: 0.0278159324\n",
      "====> Epoch: 146 Average train loss: 0.0249658441\n",
      "====> Epoch: 146 Average L0 reg loss: 0.0496425586\n",
      "====> Epoch: 146 Average eval loss: 0.0948845297\n",
      "====> Epoch: 147 Average train loss: 0.0128012215\n",
      "====> Epoch: 147 Average L0 reg loss: 0.0486254136\n",
      "====> Epoch: 147 Average eval loss: 0.0255044214\n",
      "====> Epoch: 148 Average train loss: 0.0125170305\n",
      "====> Epoch: 148 Average L0 reg loss: 0.0476222987\n",
      "====> Epoch: 148 Average eval loss: 0.0432406813\n",
      "====> Epoch: 149 Average train loss: 0.0333610019\n",
      "====> Epoch: 149 Average L0 reg loss: 0.0466440718\n",
      "====> Epoch: 149 Average eval loss: 0.0563296489\n",
      "====> Epoch: 150 Average train loss: 0.0251545903\n",
      "====> Epoch: 150 Average L0 reg loss: 0.0456988838\n",
      "====> Epoch: 150 Average eval loss: 0.0339952521\n",
      "====> Epoch: 151 Average train loss: 0.0211316807\n",
      "====> Epoch: 151 Average L0 reg loss: 0.0447864456\n",
      "====> Epoch: 151 Average eval loss: 0.0166583862\n",
      "====> Epoch: 152 Average train loss: 0.0221722908\n",
      "====> Epoch: 152 Average L0 reg loss: 0.0439338840\n",
      "====> Epoch: 152 Average eval loss: 0.0192204304\n",
      "====> Epoch: 153 Average train loss: 0.0159558291\n",
      "====> Epoch: 153 Average L0 reg loss: 0.0431333929\n",
      "====> Epoch: 153 Average eval loss: 0.0382944494\n",
      "====> Epoch: 154 Average train loss: 0.0184996002\n",
      "====> Epoch: 154 Average L0 reg loss: 0.0423612655\n",
      "====> Epoch: 154 Average eval loss: 0.0174036417\n",
      "====> Epoch: 155 Average train loss: 0.0135460090\n",
      "====> Epoch: 155 Average L0 reg loss: 0.0416278594\n",
      "====> Epoch: 155 Average eval loss: 0.0199323408\n",
      "====> Epoch: 156 Average train loss: 0.0100878064\n",
      "====> Epoch: 156 Average L0 reg loss: 0.0409187117\n",
      "====> Epoch: 156 Average eval loss: 0.0178747773\n",
      "====> Epoch: 157 Average train loss: 0.0132488643\n",
      "====> Epoch: 157 Average L0 reg loss: 0.0402169631\n",
      "====> Epoch: 157 Average eval loss: 0.0177150518\n",
      "====> Epoch: 158 Average train loss: 0.0153514537\n",
      "====> Epoch: 158 Average L0 reg loss: 0.0395267250\n",
      "====> Epoch: 158 Average eval loss: 0.0312177688\n",
      "====> Epoch: 159 Average train loss: 0.0128724988\n",
      "====> Epoch: 159 Average L0 reg loss: 0.0388419397\n",
      "====> Epoch: 159 Average eval loss: 0.0297662374\n",
      "====> Epoch: 160 Average train loss: 0.0181105762\n",
      "====> Epoch: 160 Average L0 reg loss: 0.0381699224\n",
      "====> Epoch: 160 Average eval loss: 0.0246327017\n",
      "====> Epoch: 161 Average train loss: 0.0149599258\n",
      "====> Epoch: 161 Average L0 reg loss: 0.0375136029\n",
      "====> Epoch: 161 Average eval loss: 0.0318977386\n",
      "====> Epoch: 162 Average train loss: 0.0087898544\n",
      "====> Epoch: 162 Average L0 reg loss: 0.0368657497\n",
      "====> Epoch: 162 Average eval loss: 0.0158165898\n",
      "====> Epoch: 163 Average train loss: 0.0128847888\n",
      "====> Epoch: 163 Average L0 reg loss: 0.0362238838\n",
      "====> Epoch: 163 Average eval loss: 0.0472738072\n",
      "====> Epoch: 164 Average train loss: 0.0099478615\n",
      "====> Epoch: 164 Average L0 reg loss: 0.0356079536\n",
      "====> Epoch: 164 Average eval loss: 0.0314294100\n",
      "====> Epoch: 165 Average train loss: 0.0097315699\n",
      "====> Epoch: 165 Average L0 reg loss: 0.0350194803\n",
      "====> Epoch: 165 Average eval loss: 0.0238704830\n",
      "====> Epoch: 166 Average train loss: 0.0168791562\n",
      "====> Epoch: 166 Average L0 reg loss: 0.0344603844\n",
      "====> Epoch: 166 Average eval loss: 0.0195707232\n",
      "====> Epoch: 167 Average train loss: 0.0102863450\n",
      "====> Epoch: 167 Average L0 reg loss: 0.0339242845\n",
      "====> Epoch: 167 Average eval loss: 0.0490978099\n",
      "====> Epoch: 168 Average train loss: 0.0147354160\n",
      "====> Epoch: 168 Average L0 reg loss: 0.0334077076\n",
      "====> Epoch: 168 Average eval loss: 0.0152148912\n",
      "====> Epoch: 169 Average train loss: 0.0124880346\n",
      "====> Epoch: 169 Average L0 reg loss: 0.0329272892\n",
      "====> Epoch: 169 Average eval loss: 0.0161384791\n",
      "====> Epoch: 170 Average train loss: 0.0108258961\n",
      "====> Epoch: 170 Average L0 reg loss: 0.0324602600\n",
      "====> Epoch: 170 Average eval loss: 0.0113959797\n",
      "====> Epoch: 171 Average train loss: 0.0133571343\n",
      "====> Epoch: 171 Average L0 reg loss: 0.0320067046\n",
      "====> Epoch: 171 Average eval loss: 0.0142196342\n",
      "====> Epoch: 172 Average train loss: 0.0074938872\n",
      "====> Epoch: 172 Average L0 reg loss: 0.0315809143\n",
      "====> Epoch: 172 Average eval loss: 0.0238162670\n",
      "====> Epoch: 173 Average train loss: 0.0182499785\n",
      "====> Epoch: 173 Average L0 reg loss: 0.0311759003\n",
      "====> Epoch: 173 Average eval loss: 0.0232295617\n",
      "====> Epoch: 174 Average train loss: 0.0069748968\n",
      "====> Epoch: 174 Average L0 reg loss: 0.0307974949\n",
      "====> Epoch: 174 Average eval loss: 0.0238514692\n",
      "====> Epoch: 175 Average train loss: 0.0067429761\n",
      "====> Epoch: 175 Average L0 reg loss: 0.0304368689\n",
      "====> Epoch: 175 Average eval loss: 0.0229050685\n",
      "====> Epoch: 176 Average train loss: 0.0073794416\n",
      "====> Epoch: 176 Average L0 reg loss: 0.0300872519\n",
      "====> Epoch: 176 Average eval loss: 0.0121610351\n",
      "====> Epoch: 177 Average train loss: 0.0100470958\n",
      "====> Epoch: 177 Average L0 reg loss: 0.0297561183\n",
      "====> Epoch: 177 Average eval loss: 0.0084913485\n",
      "====> Epoch: 178 Average train loss: 0.0069308958\n",
      "====> Epoch: 178 Average L0 reg loss: 0.0294411195\n",
      "====> Epoch: 178 Average eval loss: 0.0138610601\n",
      "====> Epoch: 179 Average train loss: 0.0081575932\n",
      "====> Epoch: 179 Average L0 reg loss: 0.0291503349\n",
      "====> Epoch: 179 Average eval loss: 0.0139343990\n",
      "====> Epoch: 180 Average train loss: 0.0132126626\n",
      "====> Epoch: 180 Average L0 reg loss: 0.0288838813\n",
      "====> Epoch: 180 Average eval loss: 0.0246298816\n",
      "====> Epoch: 181 Average train loss: 0.0119161837\n",
      "====> Epoch: 181 Average L0 reg loss: 0.0286280048\n",
      "====> Epoch: 181 Average eval loss: 0.0346556008\n",
      "====> Epoch: 182 Average train loss: 0.0064293209\n",
      "====> Epoch: 182 Average L0 reg loss: 0.0283855098\n",
      "====> Epoch: 182 Average eval loss: 0.0194056761\n",
      "====> Epoch: 183 Average train loss: 0.0098806799\n",
      "====> Epoch: 183 Average L0 reg loss: 0.0281532835\n",
      "====> Epoch: 183 Average eval loss: 0.0105721261\n",
      "====> Epoch: 184 Average train loss: 0.0134490233\n",
      "====> Epoch: 184 Average L0 reg loss: 0.0279336302\n",
      "====> Epoch: 184 Average eval loss: 0.0082154069\n",
      "====> Epoch: 185 Average train loss: 0.0101318127\n",
      "====> Epoch: 185 Average L0 reg loss: 0.0277132818\n",
      "====> Epoch: 185 Average eval loss: 0.0160718039\n",
      "====> Epoch: 186 Average train loss: 0.0100071878\n",
      "====> Epoch: 186 Average L0 reg loss: 0.0274980316\n",
      "====> Epoch: 186 Average eval loss: 0.0064350525\n",
      "====> Epoch: 187 Average train loss: 0.0062925440\n",
      "====> Epoch: 187 Average L0 reg loss: 0.0272840788\n",
      "====> Epoch: 187 Average eval loss: 0.0044692731\n",
      "====> Epoch: 188 Average train loss: 0.0053213434\n",
      "====> Epoch: 188 Average L0 reg loss: 0.0270712157\n",
      "====> Epoch: 188 Average eval loss: 0.0145875188\n",
      "====> Epoch: 189 Average train loss: 0.0104116548\n",
      "====> Epoch: 189 Average L0 reg loss: 0.0268612063\n",
      "====> Epoch: 189 Average eval loss: 0.0058104303\n",
      "====> Epoch: 190 Average train loss: 0.0113426198\n",
      "====> Epoch: 190 Average L0 reg loss: 0.0266555938\n",
      "====> Epoch: 190 Average eval loss: 0.0155140813\n",
      "====> Epoch: 191 Average train loss: 0.0056790219\n",
      "====> Epoch: 191 Average L0 reg loss: 0.0264513325\n",
      "====> Epoch: 191 Average eval loss: 0.0074749654\n",
      "====> Epoch: 192 Average train loss: 0.0103006020\n",
      "====> Epoch: 192 Average L0 reg loss: 0.0262500594\n",
      "====> Epoch: 192 Average eval loss: 0.0067442646\n",
      "====> Epoch: 193 Average train loss: 0.0057807488\n",
      "====> Epoch: 193 Average L0 reg loss: 0.0260592259\n",
      "====> Epoch: 193 Average eval loss: 0.0076412349\n",
      "====> Epoch: 194 Average train loss: 0.0110050622\n",
      "====> Epoch: 194 Average L0 reg loss: 0.0258721266\n",
      "====> Epoch: 194 Average eval loss: 0.0072531211\n",
      "====> Epoch: 195 Average train loss: 0.0105478130\n",
      "====> Epoch: 195 Average L0 reg loss: 0.0256894622\n",
      "====> Epoch: 195 Average eval loss: 0.0095321415\n",
      "====> Epoch: 196 Average train loss: 0.0070605460\n",
      "====> Epoch: 196 Average L0 reg loss: 0.0255056363\n",
      "====> Epoch: 196 Average eval loss: 0.0090639228\n",
      "====> Epoch: 197 Average train loss: 0.0053382847\n",
      "====> Epoch: 197 Average L0 reg loss: 0.0253330293\n",
      "====> Epoch: 197 Average eval loss: 0.0063084769\n",
      "====> Epoch: 198 Average train loss: 0.0132441508\n",
      "====> Epoch: 198 Average L0 reg loss: 0.0251562809\n",
      "====> Epoch: 198 Average eval loss: 0.0082564931\n",
      "====> Epoch: 199 Average train loss: 0.0118055082\n",
      "====> Epoch: 199 Average L0 reg loss: 0.0249785871\n",
      "====> Epoch: 199 Average eval loss: 0.0051523135\n",
      "====> Epoch: 200 Average train loss: 0.0067560644\n",
      "====> Epoch: 200 Average L0 reg loss: 0.0247985702\n",
      "====> Epoch: 200 Average eval loss: 0.0038650632\n",
      "====> Epoch: 201 Average train loss: 0.0081131967\n",
      "====> Epoch: 201 Average L0 reg loss: 0.0246230164\n",
      "====> Epoch: 201 Average eval loss: 0.0059180264\n",
      "====> Epoch: 202 Average train loss: 0.0155884873\n",
      "====> Epoch: 202 Average L0 reg loss: 0.0244514315\n",
      "====> Epoch: 202 Average eval loss: 0.0029014209\n",
      "====> Epoch: 203 Average train loss: 0.0090179267\n",
      "====> Epoch: 203 Average L0 reg loss: 0.0242903576\n",
      "====> Epoch: 203 Average eval loss: 0.0068312408\n",
      "====> Epoch: 204 Average train loss: 0.0144769909\n",
      "====> Epoch: 204 Average L0 reg loss: 0.0241418681\n",
      "====> Epoch: 204 Average eval loss: 0.0048250216\n",
      "====> Epoch: 205 Average train loss: 0.0049673600\n",
      "====> Epoch: 205 Average L0 reg loss: 0.0240061128\n",
      "====> Epoch: 205 Average eval loss: 0.0033582458\n",
      "====> Epoch: 206 Average train loss: 0.0046963318\n",
      "====> Epoch: 206 Average L0 reg loss: 0.0238821023\n",
      "====> Epoch: 206 Average eval loss: 0.0021121674\n",
      "====> Epoch: 207 Average train loss: 0.0082242264\n",
      "====> Epoch: 207 Average L0 reg loss: 0.0237676733\n",
      "====> Epoch: 207 Average eval loss: 0.0100563746\n",
      "====> Epoch: 208 Average train loss: 0.0041338217\n",
      "====> Epoch: 208 Average L0 reg loss: 0.0236613984\n",
      "====> Epoch: 208 Average eval loss: 0.0035734971\n",
      "====> Epoch: 209 Average train loss: 0.0094477409\n",
      "====> Epoch: 209 Average L0 reg loss: 0.0235653306\n",
      "====> Epoch: 209 Average eval loss: 0.0047973217\n",
      "====> Epoch: 210 Average train loss: 0.0097984752\n",
      "====> Epoch: 210 Average L0 reg loss: 0.0234832240\n",
      "====> Epoch: 210 Average eval loss: 0.0052793967\n",
      "====> Epoch: 211 Average train loss: 0.0081527221\n",
      "====> Epoch: 211 Average L0 reg loss: 0.0234115496\n",
      "====> Epoch: 211 Average eval loss: 0.0108076865\n",
      "====> Epoch: 212 Average train loss: 0.0062195412\n",
      "====> Epoch: 212 Average L0 reg loss: 0.0233488410\n",
      "====> Epoch: 212 Average eval loss: 0.0043780711\n",
      "====> Epoch: 213 Average train loss: 0.0071254335\n",
      "====> Epoch: 213 Average L0 reg loss: 0.0232941592\n",
      "====> Epoch: 213 Average eval loss: 0.0072767017\n",
      "====> Epoch: 214 Average train loss: 0.0064090030\n",
      "====> Epoch: 214 Average L0 reg loss: 0.0232468117\n",
      "====> Epoch: 214 Average eval loss: 0.0047304383\n",
      "====> Epoch: 215 Average train loss: 0.0039206118\n",
      "====> Epoch: 215 Average L0 reg loss: 0.0232046196\n",
      "====> Epoch: 215 Average eval loss: 0.0059592030\n",
      "====> Epoch: 216 Average train loss: 0.0048526343\n",
      "====> Epoch: 216 Average L0 reg loss: 0.0231674965\n",
      "====> Epoch: 216 Average eval loss: 0.0078755608\n",
      "====> Epoch: 217 Average train loss: 0.0076715503\n",
      "====> Epoch: 217 Average L0 reg loss: 0.0231352643\n",
      "====> Epoch: 217 Average eval loss: 0.0031546517\n",
      "====> Epoch: 218 Average train loss: 0.0093915396\n",
      "====> Epoch: 218 Average L0 reg loss: 0.0231076333\n",
      "====> Epoch: 218 Average eval loss: 0.0020693096\n",
      "====> Epoch: 219 Average train loss: 0.0041898778\n",
      "====> Epoch: 219 Average L0 reg loss: 0.0230838209\n",
      "====> Epoch: 219 Average eval loss: 0.0025932561\n",
      "====> Epoch: 220 Average train loss: 0.0034860825\n",
      "====> Epoch: 220 Average L0 reg loss: 0.0230634967\n",
      "====> Epoch: 220 Average eval loss: 0.0031760072\n",
      "====> Epoch: 221 Average train loss: 0.0058974116\n",
      "====> Epoch: 221 Average L0 reg loss: 0.0230464031\n",
      "====> Epoch: 221 Average eval loss: 0.0021889315\n",
      "====> Epoch: 222 Average train loss: 0.0077866411\n",
      "====> Epoch: 222 Average L0 reg loss: 0.0230316903\n",
      "====> Epoch: 222 Average eval loss: 0.0021139211\n",
      "====> Epoch: 223 Average train loss: 0.0111620329\n",
      "====> Epoch: 223 Average L0 reg loss: 0.0230183872\n",
      "====> Epoch: 223 Average eval loss: 0.0031542524\n",
      "====> Epoch: 224 Average train loss: 0.0040695726\n",
      "====> Epoch: 224 Average L0 reg loss: 0.0230062623\n",
      "====> Epoch: 224 Average eval loss: 0.0023675293\n",
      "====> Epoch: 225 Average train loss: 0.0040796088\n",
      "====> Epoch: 225 Average L0 reg loss: 0.0229949650\n",
      "====> Epoch: 225 Average eval loss: 0.0056874189\n",
      "====> Epoch: 226 Average train loss: 0.0137440686\n",
      "====> Epoch: 226 Average L0 reg loss: 0.0229837886\n",
      "====> Epoch: 226 Average eval loss: 0.0015086045\n",
      "====> Epoch: 227 Average train loss: 0.0030056983\n",
      "====> Epoch: 227 Average L0 reg loss: 0.0229728956\n",
      "====> Epoch: 227 Average eval loss: 0.0073994501\n",
      "====> Epoch: 228 Average train loss: 0.0034804929\n",
      "====> Epoch: 228 Average L0 reg loss: 0.0229625443\n",
      "====> Epoch: 228 Average eval loss: 0.0013437223\n",
      "====> Epoch: 229 Average train loss: 0.0028236739\n",
      "====> Epoch: 229 Average L0 reg loss: 0.0229526016\n",
      "====> Epoch: 229 Average eval loss: 0.0044602887\n",
      "====> Epoch: 230 Average train loss: 0.0029589311\n",
      "====> Epoch: 230 Average L0 reg loss: 0.0229421015\n",
      "====> Epoch: 230 Average eval loss: 0.0021696233\n",
      "====> Epoch: 231 Average train loss: 0.0026636194\n",
      "====> Epoch: 231 Average L0 reg loss: 0.0229305134\n",
      "====> Epoch: 231 Average eval loss: 0.0019900848\n",
      "====> Epoch: 232 Average train loss: 0.0031198382\n",
      "====> Epoch: 232 Average L0 reg loss: 0.0229173705\n",
      "====> Epoch: 232 Average eval loss: 0.0034181529\n",
      "====> Epoch: 233 Average train loss: 0.0025507911\n",
      "====> Epoch: 233 Average L0 reg loss: 0.0229038443\n",
      "====> Epoch: 233 Average eval loss: 0.0023762528\n",
      "====> Epoch: 234 Average train loss: 0.0041324367\n",
      "====> Epoch: 234 Average L0 reg loss: 0.0228881434\n",
      "====> Epoch: 234 Average eval loss: 0.0008318138\n",
      "====> Epoch: 235 Average train loss: 0.0029670275\n",
      "====> Epoch: 235 Average L0 reg loss: 0.0228697736\n",
      "====> Epoch: 235 Average eval loss: 0.0025176611\n",
      "====> Epoch: 236 Average train loss: 0.0056563063\n",
      "====> Epoch: 236 Average L0 reg loss: 0.0228478781\n",
      "====> Epoch: 236 Average eval loss: 0.0057495618\n",
      "====> Epoch: 237 Average train loss: 0.0032133509\n",
      "====> Epoch: 237 Average L0 reg loss: 0.0228231430\n",
      "====> Epoch: 237 Average eval loss: 0.0033417356\n",
      "====> Epoch: 238 Average train loss: 0.0047822141\n",
      "====> Epoch: 238 Average L0 reg loss: 0.0227944514\n",
      "====> Epoch: 238 Average eval loss: 0.0008243602\n",
      "====> Epoch: 239 Average train loss: 0.0027922742\n",
      "====> Epoch: 239 Average L0 reg loss: 0.0227608709\n",
      "====> Epoch: 239 Average eval loss: 0.0012000870\n",
      "====> Epoch: 240 Average train loss: 0.0038125353\n",
      "====> Epoch: 240 Average L0 reg loss: 0.0227225609\n",
      "====> Epoch: 240 Average eval loss: 0.0019550181\n",
      "====> Epoch: 241 Average train loss: 0.0025977223\n",
      "====> Epoch: 241 Average L0 reg loss: 0.0226807319\n",
      "====> Epoch: 241 Average eval loss: 0.0010554373\n",
      "====> Epoch: 242 Average train loss: 0.0028860016\n",
      "====> Epoch: 242 Average L0 reg loss: 0.0226371186\n",
      "====> Epoch: 242 Average eval loss: 0.0024936155\n",
      "====> Epoch: 243 Average train loss: 0.0036406604\n",
      "====> Epoch: 243 Average L0 reg loss: 0.0225929229\n",
      "====> Epoch: 243 Average eval loss: 0.0008225456\n",
      "====> Epoch: 244 Average train loss: 0.0018862447\n",
      "====> Epoch: 244 Average L0 reg loss: 0.0225498730\n",
      "====> Epoch: 244 Average eval loss: 0.0012862647\n",
      "====> Epoch: 245 Average train loss: 0.0030038204\n",
      "====> Epoch: 245 Average L0 reg loss: 0.0225074532\n",
      "====> Epoch: 245 Average eval loss: 0.0015382665\n",
      "====> Epoch: 246 Average train loss: 0.0018920210\n",
      "====> Epoch: 246 Average L0 reg loss: 0.0224672591\n",
      "====> Epoch: 246 Average eval loss: 0.0007276451\n",
      "====> Epoch: 247 Average train loss: 0.0021665119\n",
      "====> Epoch: 247 Average L0 reg loss: 0.0224285673\n",
      "====> Epoch: 247 Average eval loss: 0.0021065958\n",
      "====> Epoch: 248 Average train loss: 0.0027518469\n",
      "====> Epoch: 248 Average L0 reg loss: 0.0223907850\n",
      "====> Epoch: 248 Average eval loss: 0.0018474017\n",
      "====> Epoch: 249 Average train loss: 0.0037838613\n",
      "====> Epoch: 249 Average L0 reg loss: 0.0223529558\n",
      "====> Epoch: 249 Average eval loss: 0.0007069121\n",
      "====> Epoch: 250 Average train loss: 0.0039602377\n",
      "====> Epoch: 250 Average L0 reg loss: 0.0223145675\n",
      "====> Epoch: 250 Average eval loss: 0.0025694321\n",
      "====> Epoch: 251 Average train loss: 0.0024301621\n",
      "====> Epoch: 251 Average L0 reg loss: 0.0222751764\n",
      "====> Epoch: 251 Average eval loss: 0.0012664421\n",
      "====> Epoch: 252 Average train loss: 0.0095142141\n",
      "====> Epoch: 252 Average L0 reg loss: 0.0222332246\n",
      "====> Epoch: 252 Average eval loss: 0.0004399790\n",
      "====> Epoch: 253 Average train loss: 0.0092937791\n",
      "====> Epoch: 253 Average L0 reg loss: 0.0221907742\n",
      "====> Epoch: 253 Average eval loss: 0.0010603943\n",
      "====> Epoch: 254 Average train loss: 0.0030854371\n",
      "====> Epoch: 254 Average L0 reg loss: 0.0221461200\n",
      "====> Epoch: 254 Average eval loss: 0.0003933432\n",
      "====> Epoch: 255 Average train loss: 0.0030565680\n",
      "====> Epoch: 255 Average L0 reg loss: 0.0220966858\n",
      "====> Epoch: 255 Average eval loss: 0.0007847219\n",
      "====> Epoch: 256 Average train loss: 0.0053668716\n",
      "====> Epoch: 256 Average L0 reg loss: 0.0220443051\n",
      "====> Epoch: 256 Average eval loss: 0.0003705115\n",
      "====> Epoch: 257 Average train loss: 0.0027105423\n",
      "====> Epoch: 257 Average L0 reg loss: 0.0219901704\n",
      "====> Epoch: 257 Average eval loss: 0.0006423640\n",
      "====> Epoch: 258 Average train loss: 0.0037231770\n",
      "====> Epoch: 258 Average L0 reg loss: 0.0219354753\n",
      "====> Epoch: 258 Average eval loss: 0.0005752596\n",
      "====> Epoch: 259 Average train loss: 0.0038667240\n",
      "====> Epoch: 259 Average L0 reg loss: 0.0218814124\n",
      "====> Epoch: 259 Average eval loss: 0.0006686542\n",
      "====> Epoch: 260 Average train loss: 0.0037439269\n",
      "====> Epoch: 260 Average L0 reg loss: 0.0218281646\n",
      "====> Epoch: 260 Average eval loss: 0.0003088330\n",
      "====> Epoch: 261 Average train loss: 0.0029282383\n",
      "====> Epoch: 261 Average L0 reg loss: 0.0217775479\n",
      "====> Epoch: 261 Average eval loss: 0.0036382615\n",
      "====> Epoch: 262 Average train loss: 0.0040961704\n",
      "====> Epoch: 262 Average L0 reg loss: 0.0217290546\n",
      "====> Epoch: 262 Average eval loss: 0.0007709594\n",
      "====> Epoch: 263 Average train loss: 0.0059480457\n",
      "====> Epoch: 263 Average L0 reg loss: 0.0216829754\n",
      "====> Epoch: 263 Average eval loss: 0.0007246917\n",
      "====> Epoch: 264 Average train loss: 0.0048899663\n",
      "====> Epoch: 264 Average L0 reg loss: 0.0216406754\n",
      "====> Epoch: 264 Average eval loss: 0.0021869258\n",
      "====> Epoch: 265 Average train loss: 0.0021656142\n",
      "====> Epoch: 265 Average L0 reg loss: 0.0216011844\n",
      "====> Epoch: 265 Average eval loss: 0.0004124265\n",
      "====> Epoch: 266 Average train loss: 0.0030952563\n",
      "====> Epoch: 266 Average L0 reg loss: 0.0215637770\n",
      "====> Epoch: 266 Average eval loss: 0.0006991877\n",
      "====> Epoch: 267 Average train loss: 0.0024279547\n",
      "====> Epoch: 267 Average L0 reg loss: 0.0215279018\n",
      "====> Epoch: 267 Average eval loss: 0.0004518528\n",
      "====> Epoch: 268 Average train loss: 0.0059578422\n",
      "====> Epoch: 268 Average L0 reg loss: 0.0214929562\n",
      "====> Epoch: 268 Average eval loss: 0.0004054000\n",
      "====> Epoch: 269 Average train loss: 0.0037529465\n",
      "====> Epoch: 269 Average L0 reg loss: 0.0214589445\n",
      "====> Epoch: 269 Average eval loss: 0.0013240909\n",
      "====> Epoch: 270 Average train loss: 0.0025462786\n",
      "====> Epoch: 270 Average L0 reg loss: 0.0214241175\n",
      "====> Epoch: 270 Average eval loss: 0.0007742888\n",
      "====> Epoch: 271 Average train loss: 0.0035226089\n",
      "====> Epoch: 271 Average L0 reg loss: 0.0213871503\n",
      "====> Epoch: 271 Average eval loss: 0.0005710868\n",
      "====> Epoch: 272 Average train loss: 0.0039902677\n",
      "====> Epoch: 272 Average L0 reg loss: 0.0213465864\n",
      "====> Epoch: 272 Average eval loss: 0.0005273702\n",
      "====> Epoch: 273 Average train loss: 0.0064887988\n",
      "====> Epoch: 273 Average L0 reg loss: 0.0213029837\n",
      "====> Epoch: 273 Average eval loss: 0.0048755221\n",
      "====> Epoch: 274 Average train loss: 0.0079888334\n",
      "====> Epoch: 274 Average L0 reg loss: 0.0212561857\n",
      "====> Epoch: 274 Average eval loss: 0.0002869474\n",
      "====> Epoch: 275 Average train loss: 0.0018938645\n",
      "====> Epoch: 275 Average L0 reg loss: 0.0212056705\n",
      "====> Epoch: 275 Average eval loss: 0.0006923836\n",
      "====> Epoch: 276 Average train loss: 0.0020436569\n",
      "====> Epoch: 276 Average L0 reg loss: 0.0211513472\n",
      "====> Epoch: 276 Average eval loss: 0.0033481400\n",
      "====> Epoch: 277 Average train loss: 0.0057182710\n",
      "====> Epoch: 277 Average L0 reg loss: 0.0210944209\n",
      "====> Epoch: 277 Average eval loss: 0.0005060314\n",
      "====> Epoch: 278 Average train loss: 0.0073286242\n",
      "====> Epoch: 278 Average L0 reg loss: 0.0210363000\n",
      "====> Epoch: 278 Average eval loss: 0.0003695578\n",
      "====> Epoch: 279 Average train loss: 0.0015910398\n",
      "====> Epoch: 279 Average L0 reg loss: 0.0209777752\n",
      "====> Epoch: 279 Average eval loss: 0.0016672540\n",
      "====> Epoch: 280 Average train loss: 0.0021682951\n",
      "====> Epoch: 280 Average L0 reg loss: 0.0209202418\n",
      "====> Epoch: 280 Average eval loss: 0.0009732806\n",
      "====> Epoch: 281 Average train loss: 0.0022140123\n",
      "====> Epoch: 281 Average L0 reg loss: 0.0208652225\n",
      "====> Epoch: 281 Average eval loss: 0.0010537233\n",
      "====> Epoch: 282 Average train loss: 0.0042860871\n",
      "====> Epoch: 282 Average L0 reg loss: 0.0208128116\n",
      "====> Epoch: 282 Average eval loss: 0.0002907804\n",
      "====> Epoch: 283 Average train loss: 0.0012950942\n",
      "====> Epoch: 283 Average L0 reg loss: 0.0207648822\n",
      "====> Epoch: 283 Average eval loss: 0.0004552385\n",
      "====> Epoch: 284 Average train loss: 0.0018760390\n",
      "====> Epoch: 284 Average L0 reg loss: 0.0207205815\n",
      "====> Epoch: 284 Average eval loss: 0.0003073254\n",
      "====> Epoch: 285 Average train loss: 0.0018747252\n",
      "====> Epoch: 285 Average L0 reg loss: 0.0206796480\n",
      "====> Epoch: 285 Average eval loss: 0.0011520168\n",
      "====> Epoch: 286 Average train loss: 0.0014705901\n",
      "====> Epoch: 286 Average L0 reg loss: 0.0206416873\n",
      "====> Epoch: 286 Average eval loss: 0.0004743466\n",
      "====> Epoch: 287 Average train loss: 0.0077518408\n",
      "====> Epoch: 287 Average L0 reg loss: 0.0206058198\n",
      "====> Epoch: 287 Average eval loss: 0.0020862396\n",
      "====> Epoch: 288 Average train loss: 0.0011474773\n",
      "====> Epoch: 288 Average L0 reg loss: 0.0205725655\n",
      "====> Epoch: 288 Average eval loss: 0.0003223652\n",
      "====> Epoch: 289 Average train loss: 0.0022884644\n",
      "====> Epoch: 289 Average L0 reg loss: 0.0205416768\n",
      "====> Epoch: 289 Average eval loss: 0.0006927295\n",
      "====> Epoch: 290 Average train loss: 0.0054921861\n",
      "====> Epoch: 290 Average L0 reg loss: 0.0205133661\n",
      "====> Epoch: 290 Average eval loss: 0.0003848161\n",
      "====> Epoch: 291 Average train loss: 0.0017290051\n",
      "====> Epoch: 291 Average L0 reg loss: 0.0204887422\n",
      "====> Epoch: 291 Average eval loss: 0.0003408156\n",
      "====> Epoch: 292 Average train loss: 0.0061575686\n",
      "====> Epoch: 292 Average L0 reg loss: 0.0204646506\n",
      "====> Epoch: 292 Average eval loss: 0.0002299980\n",
      "====> Epoch: 293 Average train loss: 0.0012560692\n",
      "====> Epoch: 293 Average L0 reg loss: 0.0204412311\n",
      "====> Epoch: 293 Average eval loss: 0.0004999588\n",
      "====> Epoch: 294 Average train loss: 0.0029113288\n",
      "====> Epoch: 294 Average L0 reg loss: 0.0204182097\n",
      "====> Epoch: 294 Average eval loss: 0.0005314657\n",
      "====> Epoch: 295 Average train loss: 0.0095031825\n",
      "====> Epoch: 295 Average L0 reg loss: 0.0203963289\n",
      "====> Epoch: 295 Average eval loss: 0.0003359070\n",
      "====> Epoch: 296 Average train loss: 0.0011877398\n",
      "====> Epoch: 296 Average L0 reg loss: 0.0203756081\n",
      "====> Epoch: 296 Average eval loss: 0.0003247746\n",
      "====> Epoch: 297 Average train loss: 0.0064086477\n",
      "====> Epoch: 297 Average L0 reg loss: 0.0203555986\n",
      "====> Epoch: 297 Average eval loss: 0.0005942670\n",
      "====> Epoch: 298 Average train loss: 0.0046331799\n",
      "====> Epoch: 298 Average L0 reg loss: 0.0203363751\n",
      "====> Epoch: 298 Average eval loss: 0.0002467369\n",
      "====> Epoch: 299 Average train loss: 0.0053666977\n",
      "====> Epoch: 299 Average L0 reg loss: 0.0203186119\n",
      "====> Epoch: 299 Average eval loss: 0.0003310278\n",
      "====> Epoch: 300 Average train loss: 0.0085149391\n",
      "====> Epoch: 300 Average L0 reg loss: 0.0203015134\n",
      "====> Epoch: 300 Average eval loss: 0.0002403070\n",
      "====> Epoch: 301 Average train loss: 0.0016331315\n",
      "====> Epoch: 301 Average L0 reg loss: 0.0202855305\n",
      "====> Epoch: 301 Average eval loss: 0.0002570660\n",
      "====> Epoch: 302 Average train loss: 0.0041648875\n",
      "====> Epoch: 302 Average L0 reg loss: 0.0202701375\n",
      "====> Epoch: 302 Average eval loss: 0.0008621175\n",
      "====> Epoch: 303 Average train loss: 0.0057270499\n",
      "====> Epoch: 303 Average L0 reg loss: 0.0202547918\n",
      "====> Epoch: 303 Average eval loss: 0.0004787984\n",
      "====> Epoch: 304 Average train loss: 0.0037793931\n",
      "====> Epoch: 304 Average L0 reg loss: 0.0202397844\n",
      "====> Epoch: 304 Average eval loss: 0.0003712822\n",
      "====> Epoch: 305 Average train loss: 0.0012664736\n",
      "====> Epoch: 305 Average L0 reg loss: 0.0202254401\n",
      "====> Epoch: 305 Average eval loss: 0.0002219145\n",
      "====> Epoch: 306 Average train loss: 0.0028307755\n",
      "====> Epoch: 306 Average L0 reg loss: 0.0202116811\n",
      "====> Epoch: 306 Average eval loss: 0.0002307393\n",
      "====> Epoch: 307 Average train loss: 0.0033588067\n",
      "====> Epoch: 307 Average L0 reg loss: 0.0201978173\n",
      "====> Epoch: 307 Average eval loss: 0.0012604747\n",
      "====> Epoch: 308 Average train loss: 0.0048478914\n",
      "====> Epoch: 308 Average L0 reg loss: 0.0201851470\n",
      "====> Epoch: 308 Average eval loss: 0.0006617371\n",
      "====> Epoch: 309 Average train loss: 0.0056194728\n",
      "====> Epoch: 309 Average L0 reg loss: 0.0201739905\n",
      "====> Epoch: 309 Average eval loss: 0.0001985858\n",
      "====> Epoch: 310 Average train loss: 0.0015333297\n",
      "====> Epoch: 310 Average L0 reg loss: 0.0201625959\n",
      "====> Epoch: 310 Average eval loss: 0.0002046668\n",
      "====> Epoch: 311 Average train loss: 0.0014814618\n",
      "====> Epoch: 311 Average L0 reg loss: 0.0201497355\n",
      "====> Epoch: 311 Average eval loss: 0.0002230005\n",
      "====> Epoch: 312 Average train loss: 0.0011948068\n",
      "====> Epoch: 312 Average L0 reg loss: 0.0201353656\n",
      "====> Epoch: 312 Average eval loss: 0.0002649153\n",
      "====> Epoch: 313 Average train loss: 0.0043744795\n",
      "====> Epoch: 313 Average L0 reg loss: 0.0201187953\n",
      "====> Epoch: 313 Average eval loss: 0.0002325052\n",
      "====> Epoch: 314 Average train loss: 0.0014158003\n",
      "====> Epoch: 314 Average L0 reg loss: 0.0201014773\n",
      "====> Epoch: 314 Average eval loss: 0.0002862237\n",
      "====> Epoch: 315 Average train loss: 0.0010591091\n",
      "====> Epoch: 315 Average L0 reg loss: 0.0200823493\n",
      "====> Epoch: 315 Average eval loss: 0.0001923196\n",
      "====> Epoch: 316 Average train loss: 0.0012908604\n",
      "====> Epoch: 316 Average L0 reg loss: 0.0200592871\n",
      "====> Epoch: 316 Average eval loss: 0.0003908019\n",
      "====> Epoch: 317 Average train loss: 0.0015265572\n",
      "====> Epoch: 317 Average L0 reg loss: 0.0200324734\n",
      "====> Epoch: 317 Average eval loss: 0.0002233471\n",
      "====> Epoch: 318 Average train loss: 0.0013649931\n",
      "====> Epoch: 318 Average L0 reg loss: 0.0200024394\n",
      "====> Epoch: 318 Average eval loss: 0.0002230755\n",
      "====> Epoch: 319 Average train loss: 0.0013114656\n",
      "====> Epoch: 319 Average L0 reg loss: 0.0199691949\n",
      "====> Epoch: 319 Average eval loss: 0.0002578025\n",
      "====> Epoch: 320 Average train loss: 0.0015643359\n",
      "====> Epoch: 320 Average L0 reg loss: 0.0199335181\n",
      "====> Epoch: 320 Average eval loss: 0.0002126364\n",
      "====> Epoch: 321 Average train loss: 0.0046926851\n",
      "====> Epoch: 321 Average L0 reg loss: 0.0198928926\n",
      "====> Epoch: 321 Average eval loss: 0.0001795512\n",
      "====> Epoch: 322 Average train loss: 0.0074018497\n",
      "====> Epoch: 322 Average L0 reg loss: 0.0198473319\n",
      "====> Epoch: 322 Average eval loss: 0.0002784580\n",
      "====> Epoch: 323 Average train loss: 0.0033209634\n",
      "====> Epoch: 323 Average L0 reg loss: 0.0198003556\n",
      "====> Epoch: 323 Average eval loss: 0.0018835866\n",
      "====> Epoch: 324 Average train loss: 0.0019729270\n",
      "====> Epoch: 324 Average L0 reg loss: 0.0197519797\n",
      "====> Epoch: 324 Average eval loss: 0.0006214303\n",
      "====> Epoch: 325 Average train loss: 0.0012266990\n",
      "====> Epoch: 325 Average L0 reg loss: 0.0197045902\n",
      "====> Epoch: 325 Average eval loss: 0.0001855598\n",
      "====> Epoch: 326 Average train loss: 0.0009792143\n",
      "====> Epoch: 326 Average L0 reg loss: 0.0196593062\n",
      "====> Epoch: 326 Average eval loss: 0.0002177697\n",
      "====> Epoch: 327 Average train loss: 0.0064780803\n",
      "====> Epoch: 327 Average L0 reg loss: 0.0196151812\n",
      "====> Epoch: 327 Average eval loss: 0.0004721376\n",
      "====> Epoch: 328 Average train loss: 0.0012130834\n",
      "====> Epoch: 328 Average L0 reg loss: 0.0195730282\n",
      "====> Epoch: 328 Average eval loss: 0.0002151684\n",
      "====> Epoch: 329 Average train loss: 0.0012737795\n",
      "====> Epoch: 329 Average L0 reg loss: 0.0195340441\n",
      "====> Epoch: 329 Average eval loss: 0.0002563474\n",
      "====> Epoch: 330 Average train loss: 0.0077747839\n",
      "====> Epoch: 330 Average L0 reg loss: 0.0194987302\n",
      "====> Epoch: 330 Average eval loss: 0.0001987110\n",
      "====> Epoch: 331 Average train loss: 0.0010612016\n",
      "====> Epoch: 331 Average L0 reg loss: 0.0194677666\n",
      "====> Epoch: 331 Average eval loss: 0.0003914165\n",
      "====> Epoch: 332 Average train loss: 0.0010279368\n",
      "====> Epoch: 332 Average L0 reg loss: 0.0194394668\n",
      "====> Epoch: 332 Average eval loss: 0.0004681749\n",
      "====> Epoch: 333 Average train loss: 0.0012038295\n",
      "====> Epoch: 333 Average L0 reg loss: 0.0194130025\n",
      "====> Epoch: 333 Average eval loss: 0.0002211597\n",
      "====> Epoch: 334 Average train loss: 0.0011336139\n",
      "====> Epoch: 334 Average L0 reg loss: 0.0193888771\n",
      "====> Epoch: 334 Average eval loss: 0.0003846711\n",
      "====> Epoch: 335 Average train loss: 0.0016128273\n",
      "====> Epoch: 335 Average L0 reg loss: 0.0193670837\n",
      "====> Epoch: 335 Average eval loss: 0.0001692508\n",
      "====> Epoch: 336 Average train loss: 0.0014548075\n",
      "====> Epoch: 336 Average L0 reg loss: 0.0193466891\n",
      "====> Epoch: 336 Average eval loss: 0.0001820495\n",
      "====> Epoch: 337 Average train loss: 0.0010492367\n",
      "====> Epoch: 337 Average L0 reg loss: 0.0193278512\n",
      "====> Epoch: 337 Average eval loss: 0.0001741614\n",
      "====> Epoch: 338 Average train loss: 0.0007695890\n",
      "====> Epoch: 338 Average L0 reg loss: 0.0193107678\n",
      "====> Epoch: 338 Average eval loss: 0.0003734025\n",
      "====> Epoch: 339 Average train loss: 0.0018273734\n",
      "====> Epoch: 339 Average L0 reg loss: 0.0192945544\n",
      "====> Epoch: 339 Average eval loss: 0.0004561750\n",
      "====> Epoch: 340 Average train loss: 0.0007943294\n",
      "====> Epoch: 340 Average L0 reg loss: 0.0192795802\n",
      "====> Epoch: 340 Average eval loss: 0.0002423525\n",
      "====> Epoch: 341 Average train loss: 0.0024373260\n",
      "====> Epoch: 341 Average L0 reg loss: 0.0192654110\n",
      "====> Epoch: 341 Average eval loss: 0.0001539913\n",
      "====> Epoch: 342 Average train loss: 0.0011551611\n",
      "====> Epoch: 342 Average L0 reg loss: 0.0192521655\n",
      "====> Epoch: 342 Average eval loss: 0.0002791190\n",
      "====> Epoch: 343 Average train loss: 0.0011392114\n",
      "====> Epoch: 343 Average L0 reg loss: 0.0192399760\n",
      "====> Epoch: 343 Average eval loss: 0.0001653625\n",
      "====> Epoch: 344 Average train loss: 0.0085304098\n",
      "====> Epoch: 344 Average L0 reg loss: 0.0192287112\n",
      "====> Epoch: 344 Average eval loss: 0.0003482382\n",
      "====> Epoch: 345 Average train loss: 0.0007070701\n",
      "====> Epoch: 345 Average L0 reg loss: 0.0192181092\n",
      "====> Epoch: 345 Average eval loss: 0.0001653913\n",
      "====> Epoch: 346 Average train loss: 0.0011222350\n",
      "====> Epoch: 346 Average L0 reg loss: 0.0192079270\n",
      "====> Epoch: 346 Average eval loss: 0.0009460263\n",
      "====> Epoch: 347 Average train loss: 0.0012122367\n",
      "====> Epoch: 347 Average L0 reg loss: 0.0191986825\n",
      "====> Epoch: 347 Average eval loss: 0.0001523415\n",
      "====> Epoch: 348 Average train loss: 0.0007882288\n",
      "====> Epoch: 348 Average L0 reg loss: 0.0191899967\n",
      "====> Epoch: 348 Average eval loss: 0.0001899875\n",
      "====> Epoch: 349 Average train loss: 0.0052006188\n",
      "====> Epoch: 349 Average L0 reg loss: 0.0191816322\n",
      "====> Epoch: 349 Average eval loss: 0.0001804516\n",
      "====> Epoch: 350 Average train loss: 0.0011035671\n",
      "====> Epoch: 350 Average L0 reg loss: 0.0191740465\n",
      "====> Epoch: 350 Average eval loss: 0.0002213948\n",
      "====> Epoch: 351 Average train loss: 0.0014475452\n",
      "====> Epoch: 351 Average L0 reg loss: 0.0191670738\n",
      "====> Epoch: 351 Average eval loss: 0.0004891995\n",
      "====> Epoch: 352 Average train loss: 0.0025454376\n",
      "====> Epoch: 352 Average L0 reg loss: 0.0191604973\n",
      "====> Epoch: 352 Average eval loss: 0.0003035214\n",
      "====> Epoch: 353 Average train loss: 0.0015140966\n",
      "====> Epoch: 353 Average L0 reg loss: 0.0191545331\n",
      "====> Epoch: 353 Average eval loss: 0.0001837482\n",
      "====> Epoch: 354 Average train loss: 0.0006085456\n",
      "====> Epoch: 354 Average L0 reg loss: 0.0191494114\n",
      "====> Epoch: 354 Average eval loss: 0.0002994963\n",
      "====> Epoch: 355 Average train loss: 0.0110878289\n",
      "====> Epoch: 355 Average L0 reg loss: 0.0191446572\n",
      "====> Epoch: 355 Average eval loss: 0.0002066760\n",
      "====> Epoch: 356 Average train loss: 0.0009930588\n",
      "====> Epoch: 356 Average L0 reg loss: 0.0191405809\n",
      "====> Epoch: 356 Average eval loss: 0.0002588801\n",
      "====> Epoch: 357 Average train loss: 0.0011491753\n",
      "====> Epoch: 357 Average L0 reg loss: 0.0191370829\n",
      "====> Epoch: 357 Average eval loss: 0.0001436695\n",
      "====> Epoch: 358 Average train loss: 0.0015133057\n",
      "====> Epoch: 358 Average L0 reg loss: 0.0191339219\n",
      "====> Epoch: 358 Average eval loss: 0.0001761580\n",
      "====> Epoch: 359 Average train loss: 0.0008753839\n",
      "====> Epoch: 359 Average L0 reg loss: 0.0191309788\n",
      "====> Epoch: 359 Average eval loss: 0.0001639212\n",
      "====> Epoch: 360 Average train loss: 0.0047649460\n",
      "====> Epoch: 360 Average L0 reg loss: 0.0191279318\n",
      "====> Epoch: 360 Average eval loss: 0.0047731078\n",
      "====> Epoch: 361 Average train loss: 0.0009746027\n",
      "====> Epoch: 361 Average L0 reg loss: 0.0191248053\n",
      "====> Epoch: 361 Average eval loss: 0.0001328079\n",
      "====> Epoch: 362 Average train loss: 0.0007923713\n",
      "====> Epoch: 362 Average L0 reg loss: 0.0191216086\n",
      "====> Epoch: 362 Average eval loss: 0.0001804024\n",
      "====> Epoch: 363 Average train loss: 0.0008845095\n",
      "====> Epoch: 363 Average L0 reg loss: 0.0191186241\n",
      "====> Epoch: 363 Average eval loss: 0.0002393440\n",
      "====> Epoch: 364 Average train loss: 0.0008496391\n",
      "====> Epoch: 364 Average L0 reg loss: 0.0191158704\n",
      "====> Epoch: 364 Average eval loss: 0.0008564944\n",
      "====> Epoch: 365 Average train loss: 0.0011216419\n",
      "====> Epoch: 365 Average L0 reg loss: 0.0191131239\n",
      "====> Epoch: 365 Average eval loss: 0.0001963793\n",
      "====> Epoch: 366 Average train loss: 0.0009062049\n",
      "====> Epoch: 366 Average L0 reg loss: 0.0191102847\n",
      "====> Epoch: 366 Average eval loss: 0.0001940408\n",
      "====> Epoch: 367 Average train loss: 0.0006655293\n",
      "====> Epoch: 367 Average L0 reg loss: 0.0191073956\n",
      "====> Epoch: 367 Average eval loss: 0.0001315686\n",
      "====> Epoch: 368 Average train loss: 0.0011224800\n",
      "====> Epoch: 368 Average L0 reg loss: 0.0191045064\n",
      "====> Epoch: 368 Average eval loss: 0.0016083485\n",
      "====> Epoch: 369 Average train loss: 0.0028295755\n",
      "====> Epoch: 369 Average L0 reg loss: 0.0191016444\n",
      "====> Epoch: 369 Average eval loss: 0.0001650193\n",
      "====> Epoch: 370 Average train loss: 0.0036558823\n",
      "====> Epoch: 370 Average L0 reg loss: 0.0190991270\n",
      "====> Epoch: 370 Average eval loss: 0.0001737767\n",
      "====> Epoch: 371 Average train loss: 0.0009041711\n",
      "====> Epoch: 371 Average L0 reg loss: 0.0190966191\n",
      "====> Epoch: 371 Average eval loss: 0.0001544095\n",
      "====> Epoch: 372 Average train loss: 0.0004308463\n",
      "====> Epoch: 372 Average L0 reg loss: 0.0190939696\n",
      "====> Epoch: 372 Average eval loss: 0.0001560403\n",
      "====> Epoch: 373 Average train loss: 0.0010646998\n",
      "====> Epoch: 373 Average L0 reg loss: 0.0190912790\n",
      "====> Epoch: 373 Average eval loss: 0.0001457692\n",
      "====> Epoch: 374 Average train loss: 0.0010514816\n",
      "====> Epoch: 374 Average L0 reg loss: 0.0190885991\n",
      "====> Epoch: 374 Average eval loss: 0.0002522587\n",
      "====> Epoch: 375 Average train loss: 0.0017357121\n",
      "====> Epoch: 375 Average L0 reg loss: 0.0190858783\n",
      "====> Epoch: 375 Average eval loss: 0.0002136672\n",
      "====> Epoch: 376 Average train loss: 0.0038888866\n",
      "====> Epoch: 376 Average L0 reg loss: 0.0190831605\n",
      "====> Epoch: 376 Average eval loss: 0.0001312154\n",
      "====> Epoch: 377 Average train loss: 0.0007844368\n",
      "====> Epoch: 377 Average L0 reg loss: 0.0190806874\n",
      "====> Epoch: 377 Average eval loss: 0.0001318094\n",
      "====> Epoch: 378 Average train loss: 0.0005934603\n",
      "====> Epoch: 378 Average L0 reg loss: 0.0190785569\n",
      "====> Epoch: 378 Average eval loss: 0.0001398103\n",
      "====> Epoch: 379 Average train loss: 0.0006488578\n",
      "====> Epoch: 379 Average L0 reg loss: 0.0190765263\n",
      "====> Epoch: 379 Average eval loss: 0.0001636903\n",
      "====> Epoch: 380 Average train loss: 0.0072944242\n",
      "====> Epoch: 380 Average L0 reg loss: 0.0190745392\n",
      "====> Epoch: 380 Average eval loss: 0.0002269769\n",
      "====> Epoch: 381 Average train loss: 0.0008842006\n",
      "====> Epoch: 381 Average L0 reg loss: 0.0190726853\n",
      "====> Epoch: 381 Average eval loss: 0.0001313444\n",
      "====> Epoch: 382 Average train loss: 0.0005631782\n",
      "====> Epoch: 382 Average L0 reg loss: 0.0190709851\n",
      "====> Epoch: 382 Average eval loss: 0.0001604642\n",
      "====> Epoch: 383 Average train loss: 0.0007367770\n",
      "====> Epoch: 383 Average L0 reg loss: 0.0190693074\n",
      "====> Epoch: 383 Average eval loss: 0.0001591778\n",
      "====> Epoch: 384 Average train loss: 0.0076825596\n",
      "====> Epoch: 384 Average L0 reg loss: 0.0190676893\n",
      "====> Epoch: 384 Average eval loss: 0.0002096775\n",
      "====> Epoch: 385 Average train loss: 0.0006251141\n",
      "====> Epoch: 385 Average L0 reg loss: 0.0190662739\n",
      "====> Epoch: 385 Average eval loss: 0.0002320234\n",
      "====> Epoch: 386 Average train loss: 0.0011197127\n",
      "====> Epoch: 386 Average L0 reg loss: 0.0190648934\n",
      "====> Epoch: 386 Average eval loss: 0.0002283410\n",
      "====> Epoch: 387 Average train loss: 0.0010700137\n",
      "====> Epoch: 387 Average L0 reg loss: 0.0190635371\n",
      "====> Epoch: 387 Average eval loss: 0.0004200454\n",
      "====> Epoch: 388 Average train loss: 0.0014798191\n",
      "====> Epoch: 388 Average L0 reg loss: 0.0190622201\n",
      "====> Epoch: 388 Average eval loss: 0.0001279075\n",
      "====> Epoch: 389 Average train loss: 0.0011379185\n",
      "====> Epoch: 389 Average L0 reg loss: 0.0190609915\n",
      "====> Epoch: 389 Average eval loss: 0.0001876991\n",
      "====> Epoch: 390 Average train loss: 0.0004961034\n",
      "====> Epoch: 390 Average L0 reg loss: 0.0190598271\n",
      "====> Epoch: 390 Average eval loss: 0.0001610266\n",
      "====> Epoch: 391 Average train loss: 0.0033943863\n",
      "====> Epoch: 391 Average L0 reg loss: 0.0190587205\n",
      "====> Epoch: 391 Average eval loss: 0.0001892015\n",
      "====> Epoch: 392 Average train loss: 0.0004249912\n",
      "====> Epoch: 392 Average L0 reg loss: 0.0190577039\n",
      "====> Epoch: 392 Average eval loss: 0.0001995268\n",
      "====> Epoch: 393 Average train loss: 0.0006901888\n",
      "====> Epoch: 393 Average L0 reg loss: 0.0190567649\n",
      "====> Epoch: 393 Average eval loss: 0.0001339384\n",
      "====> Epoch: 394 Average train loss: 0.0008272899\n",
      "====> Epoch: 394 Average L0 reg loss: 0.0190559671\n",
      "====> Epoch: 394 Average eval loss: 0.0001520238\n",
      "====> Epoch: 395 Average train loss: 0.0004504691\n",
      "====> Epoch: 395 Average L0 reg loss: 0.0190552725\n",
      "====> Epoch: 395 Average eval loss: 0.0001283358\n",
      "====> Epoch: 396 Average train loss: 0.0008075916\n",
      "====> Epoch: 396 Average L0 reg loss: 0.0190544966\n",
      "====> Epoch: 396 Average eval loss: 0.0001192867\n",
      "====> Epoch: 397 Average train loss: 0.0008575778\n",
      "====> Epoch: 397 Average L0 reg loss: 0.0190536067\n",
      "====> Epoch: 397 Average eval loss: 0.0006175587\n",
      "====> Epoch: 398 Average train loss: 0.0058241628\n",
      "====> Epoch: 398 Average L0 reg loss: 0.0190527785\n",
      "====> Epoch: 398 Average eval loss: 0.0002552360\n",
      "====> Epoch: 399 Average train loss: 0.0049034577\n",
      "====> Epoch: 399 Average L0 reg loss: 0.0190519075\n",
      "====> Epoch: 399 Average eval loss: 0.0001209044\n",
      "====> Epoch: 400 Average train loss: 0.0007912695\n",
      "====> Epoch: 400 Average L0 reg loss: 0.0190509176\n",
      "====> Epoch: 400 Average eval loss: 0.0001247745\n",
      "====> Epoch: 401 Average train loss: 0.0005348060\n",
      "====> Epoch: 401 Average L0 reg loss: 0.0190497757\n",
      "====> Epoch: 401 Average eval loss: 0.0001245686\n",
      "====> Epoch: 402 Average train loss: 0.0082547867\n",
      "====> Epoch: 402 Average L0 reg loss: 0.0190485621\n",
      "====> Epoch: 402 Average eval loss: 0.0001653906\n",
      "====> Epoch: 403 Average train loss: 0.0060278919\n",
      "====> Epoch: 403 Average L0 reg loss: 0.0190473871\n",
      "====> Epoch: 403 Average eval loss: 0.0002193006\n",
      "====> Epoch: 404 Average train loss: 0.0006150423\n",
      "====> Epoch: 404 Average L0 reg loss: 0.0190461669\n",
      "====> Epoch: 404 Average eval loss: 0.0003486962\n",
      "====> Epoch: 405 Average train loss: 0.0018591724\n",
      "====> Epoch: 405 Average L0 reg loss: 0.0190448155\n",
      "====> Epoch: 405 Average eval loss: 0.0001296433\n",
      "====> Epoch: 406 Average train loss: 0.0009204623\n",
      "====> Epoch: 406 Average L0 reg loss: 0.0190434790\n",
      "====> Epoch: 406 Average eval loss: 0.0004655981\n",
      "====> Epoch: 407 Average train loss: 0.0008465171\n",
      "====> Epoch: 407 Average L0 reg loss: 0.0190421454\n",
      "====> Epoch: 407 Average eval loss: 0.0001968733\n",
      "====> Epoch: 408 Average train loss: 0.0007418672\n",
      "====> Epoch: 408 Average L0 reg loss: 0.0190408469\n",
      "====> Epoch: 408 Average eval loss: 0.0001185667\n",
      "====> Epoch: 409 Average train loss: 0.0007351267\n",
      "====> Epoch: 409 Average L0 reg loss: 0.0190395960\n",
      "====> Epoch: 409 Average eval loss: 0.0001143653\n",
      "====> Epoch: 410 Average train loss: 0.0007058985\n",
      "====> Epoch: 410 Average L0 reg loss: 0.0190382731\n",
      "====> Epoch: 410 Average eval loss: 0.0013987874\n",
      "====> Epoch: 411 Average train loss: 0.0006002989\n",
      "====> Epoch: 411 Average L0 reg loss: 0.0190368091\n",
      "====> Epoch: 411 Average eval loss: 0.0001181508\n",
      "====> Epoch: 412 Average train loss: 0.0038231102\n",
      "====> Epoch: 412 Average L0 reg loss: 0.0190353617\n",
      "====> Epoch: 412 Average eval loss: 0.0001125606\n",
      "====> Epoch: 413 Average train loss: 0.0008379322\n",
      "====> Epoch: 413 Average L0 reg loss: 0.0190337365\n",
      "====> Epoch: 413 Average eval loss: 0.0001262752\n",
      "====> Epoch: 414 Average train loss: 0.0003513168\n",
      "====> Epoch: 414 Average L0 reg loss: 0.0190317510\n",
      "====> Epoch: 414 Average eval loss: 0.0001128663\n",
      "====> Epoch: 415 Average train loss: 0.0039900727\n",
      "====> Epoch: 415 Average L0 reg loss: 0.0190295847\n",
      "====> Epoch: 415 Average eval loss: 0.0001897166\n",
      "====> Epoch: 416 Average train loss: 0.0006132212\n",
      "====> Epoch: 416 Average L0 reg loss: 0.0190278202\n",
      "====> Epoch: 416 Average eval loss: 0.0001132810\n",
      "====> Epoch: 417 Average train loss: 0.0038163253\n",
      "====> Epoch: 417 Average L0 reg loss: 0.0190256513\n",
      "====> Epoch: 417 Average eval loss: 0.0001374959\n",
      "====> Epoch: 418 Average train loss: 0.0020109910\n",
      "====> Epoch: 418 Average L0 reg loss: 0.0190230630\n",
      "====> Epoch: 418 Average eval loss: 0.0001158289\n",
      "====> Epoch: 419 Average train loss: 0.0004640293\n",
      "====> Epoch: 419 Average L0 reg loss: 0.0190198203\n",
      "====> Epoch: 419 Average eval loss: 0.0001747627\n",
      "====> Epoch: 420 Average train loss: 0.0007359041\n",
      "====> Epoch: 420 Average L0 reg loss: 0.0190153368\n",
      "====> Epoch: 420 Average eval loss: 0.0001195760\n",
      "====> Epoch: 421 Average train loss: 0.0038418860\n",
      "====> Epoch: 421 Average L0 reg loss: 0.0190096158\n",
      "====> Epoch: 421 Average eval loss: 0.0001754656\n",
      "====> Epoch: 422 Average train loss: 0.0005691494\n",
      "====> Epoch: 422 Average L0 reg loss: 0.0190020749\n",
      "====> Epoch: 422 Average eval loss: 0.0001644518\n",
      "====> Epoch: 423 Average train loss: 0.0014706253\n",
      "====> Epoch: 423 Average L0 reg loss: 0.0189919043\n",
      "====> Epoch: 423 Average eval loss: 0.0001197379\n",
      "====> Epoch: 424 Average train loss: 0.0008299811\n",
      "====> Epoch: 424 Average L0 reg loss: 0.0189790511\n",
      "====> Epoch: 424 Average eval loss: 0.0001866043\n",
      "====> Epoch: 425 Average train loss: 0.0005647220\n",
      "====> Epoch: 425 Average L0 reg loss: 0.0189625143\n",
      "====> Epoch: 425 Average eval loss: 0.0002077428\n",
      "====> Epoch: 426 Average train loss: 0.0005860905\n",
      "====> Epoch: 426 Average L0 reg loss: 0.0189418726\n",
      "====> Epoch: 426 Average eval loss: 0.0001160677\n",
      "====> Epoch: 427 Average train loss: 0.0011429083\n",
      "====> Epoch: 427 Average L0 reg loss: 0.0189166239\n",
      "====> Epoch: 427 Average eval loss: 0.0001125664\n",
      "====> Epoch: 428 Average train loss: 0.0006104462\n",
      "====> Epoch: 428 Average L0 reg loss: 0.0188871380\n",
      "====> Epoch: 428 Average eval loss: 0.0001135361\n",
      "====> Epoch: 429 Average train loss: 0.0014135684\n",
      "====> Epoch: 429 Average L0 reg loss: 0.0188536498\n",
      "====> Epoch: 429 Average eval loss: 0.0001144126\n",
      "====> Epoch: 430 Average train loss: 0.0006172434\n",
      "====> Epoch: 430 Average L0 reg loss: 0.0188166200\n",
      "====> Epoch: 430 Average eval loss: 0.0001163180\n",
      "====> Epoch: 431 Average train loss: 0.0023110706\n",
      "====> Epoch: 431 Average L0 reg loss: 0.0187769438\n",
      "====> Epoch: 431 Average eval loss: 0.0001096647\n",
      "====> Epoch: 432 Average train loss: 0.0005394186\n",
      "====> Epoch: 432 Average L0 reg loss: 0.0187368132\n",
      "====> Epoch: 432 Average eval loss: 0.0001712108\n",
      "====> Epoch: 433 Average train loss: 0.0033696280\n",
      "====> Epoch: 433 Average L0 reg loss: 0.0186969241\n",
      "====> Epoch: 433 Average eval loss: 0.0001183318\n",
      "====> Epoch: 434 Average train loss: 0.0005170381\n",
      "====> Epoch: 434 Average L0 reg loss: 0.0186581015\n",
      "====> Epoch: 434 Average eval loss: 0.0001291089\n",
      "====> Epoch: 435 Average train loss: 0.0058570306\n",
      "====> Epoch: 435 Average L0 reg loss: 0.0186214344\n",
      "====> Epoch: 435 Average eval loss: 0.0001751600\n",
      "====> Epoch: 436 Average train loss: 0.0004764334\n",
      "====> Epoch: 436 Average L0 reg loss: 0.0185878002\n",
      "====> Epoch: 436 Average eval loss: 0.0001468148\n",
      "====> Epoch: 437 Average train loss: 0.0004508023\n",
      "====> Epoch: 437 Average L0 reg loss: 0.0185577210\n",
      "====> Epoch: 437 Average eval loss: 0.0001102243\n",
      "====> Epoch: 438 Average train loss: 0.0064189357\n",
      "====> Epoch: 438 Average L0 reg loss: 0.0185310135\n",
      "====> Epoch: 438 Average eval loss: 0.0001108134\n",
      "====> Epoch: 439 Average train loss: 0.0005380397\n",
      "====> Epoch: 439 Average L0 reg loss: 0.0185075717\n",
      "====> Epoch: 439 Average eval loss: 0.0001907837\n",
      "====> Epoch: 440 Average train loss: 0.0005111454\n",
      "====> Epoch: 440 Average L0 reg loss: 0.0184871343\n",
      "====> Epoch: 440 Average eval loss: 0.0001076369\n",
      "====> Epoch: 441 Average train loss: 0.0007371830\n",
      "====> Epoch: 441 Average L0 reg loss: 0.0184693928\n",
      "====> Epoch: 441 Average eval loss: 0.0001310886\n",
      "====> Epoch: 442 Average train loss: 0.0003890933\n",
      "====> Epoch: 442 Average L0 reg loss: 0.0184540840\n",
      "====> Epoch: 442 Average eval loss: 0.0001138916\n",
      "====> Epoch: 443 Average train loss: 0.0005550685\n",
      "====> Epoch: 443 Average L0 reg loss: 0.0184409330\n",
      "====> Epoch: 443 Average eval loss: 0.0001293340\n",
      "====> Epoch: 444 Average train loss: 0.0002757539\n",
      "====> Epoch: 444 Average L0 reg loss: 0.0184297848\n",
      "====> Epoch: 444 Average eval loss: 0.0001227680\n",
      "====> Epoch: 445 Average train loss: 0.0006654411\n",
      "====> Epoch: 445 Average L0 reg loss: 0.0184202711\n",
      "====> Epoch: 445 Average eval loss: 0.0001251282\n",
      "====> Epoch: 446 Average train loss: 0.0004127512\n",
      "====> Epoch: 446 Average L0 reg loss: 0.0184121901\n",
      "====> Epoch: 446 Average eval loss: 0.0003891870\n",
      "====> Epoch: 447 Average train loss: 0.0070502585\n",
      "====> Epoch: 447 Average L0 reg loss: 0.0184055997\n",
      "====> Epoch: 447 Average eval loss: 0.0001779553\n",
      "====> Epoch: 448 Average train loss: 0.0003498397\n",
      "====> Epoch: 448 Average L0 reg loss: 0.0184005267\n",
      "====> Epoch: 448 Average eval loss: 0.0001324353\n",
      "====> Epoch: 449 Average train loss: 0.0053224579\n",
      "====> Epoch: 449 Average L0 reg loss: 0.0183959989\n",
      "====> Epoch: 449 Average eval loss: 0.0001176936\n",
      "====> Epoch: 450 Average train loss: 0.0003266790\n",
      "====> Epoch: 450 Average L0 reg loss: 0.0183919309\n",
      "====> Epoch: 450 Average eval loss: 0.0001428905\n",
      "====> Epoch: 451 Average train loss: 0.0067679667\n",
      "====> Epoch: 451 Average L0 reg loss: 0.0183883692\n",
      "====> Epoch: 451 Average eval loss: 0.0002802008\n",
      "====> Epoch: 452 Average train loss: 0.0005695480\n",
      "====> Epoch: 452 Average L0 reg loss: 0.0183853608\n",
      "====> Epoch: 452 Average eval loss: 0.0001969259\n",
      "====> Epoch: 453 Average train loss: 0.0004637321\n",
      "====> Epoch: 453 Average L0 reg loss: 0.0183828627\n",
      "====> Epoch: 453 Average eval loss: 0.0001152821\n",
      "====> Epoch: 454 Average train loss: 0.0046307596\n",
      "====> Epoch: 454 Average L0 reg loss: 0.0183807647\n",
      "====> Epoch: 454 Average eval loss: 0.0001928899\n",
      "====> Epoch: 455 Average train loss: 0.0012355211\n",
      "====> Epoch: 455 Average L0 reg loss: 0.0183790024\n",
      "====> Epoch: 455 Average eval loss: 0.0001142225\n",
      "====> Epoch: 456 Average train loss: 0.0005207889\n",
      "====> Epoch: 456 Average L0 reg loss: 0.0183775296\n",
      "====> Epoch: 456 Average eval loss: 0.0015068566\n",
      "====> Epoch: 457 Average train loss: 0.0004854700\n",
      "====> Epoch: 457 Average L0 reg loss: 0.0183763092\n",
      "====> Epoch: 457 Average eval loss: 0.0001328515\n",
      "====> Epoch: 458 Average train loss: 0.0004009460\n",
      "====> Epoch: 458 Average L0 reg loss: 0.0183752463\n",
      "====> Epoch: 458 Average eval loss: 0.0001041075\n",
      "====> Epoch: 459 Average train loss: 0.0005164439\n",
      "====> Epoch: 459 Average L0 reg loss: 0.0183743154\n",
      "====> Epoch: 459 Average eval loss: 0.0001106125\n",
      "====> Epoch: 460 Average train loss: 0.0060732308\n",
      "====> Epoch: 460 Average L0 reg loss: 0.0183735127\n",
      "====> Epoch: 460 Average eval loss: 0.0001227794\n",
      "====> Epoch: 461 Average train loss: 0.0005987402\n",
      "====> Epoch: 461 Average L0 reg loss: 0.0183728392\n",
      "====> Epoch: 461 Average eval loss: 0.0001093278\n",
      "====> Epoch: 462 Average train loss: 0.0005362953\n",
      "====> Epoch: 462 Average L0 reg loss: 0.0183722695\n",
      "====> Epoch: 462 Average eval loss: 0.0001699986\n",
      "====> Epoch: 463 Average train loss: 0.0005347132\n",
      "====> Epoch: 463 Average L0 reg loss: 0.0183717795\n",
      "====> Epoch: 463 Average eval loss: 0.0001334709\n",
      "====> Epoch: 464 Average train loss: 0.0005197867\n",
      "====> Epoch: 464 Average L0 reg loss: 0.0183713591\n",
      "====> Epoch: 464 Average eval loss: 0.0001029128\n",
      "====> Epoch: 465 Average train loss: 0.0081048587\n",
      "====> Epoch: 465 Average L0 reg loss: 0.0183710004\n",
      "====> Epoch: 465 Average eval loss: 0.0081345551\n",
      "====> Epoch: 466 Average train loss: 0.0006033094\n",
      "====> Epoch: 466 Average L0 reg loss: 0.0183706862\n",
      "====> Epoch: 466 Average eval loss: 0.0001054998\n",
      "====> Epoch: 467 Average train loss: 0.0008730145\n",
      "====> Epoch: 467 Average L0 reg loss: 0.0183704072\n",
      "====> Epoch: 467 Average eval loss: 0.0001402586\n",
      "====> Epoch: 468 Average train loss: 0.0003609612\n",
      "====> Epoch: 468 Average L0 reg loss: 0.0183701615\n",
      "====> Epoch: 468 Average eval loss: 0.0001245720\n",
      "====> Epoch: 469 Average train loss: 0.0023176035\n",
      "====> Epoch: 469 Average L0 reg loss: 0.0183699418\n",
      "====> Epoch: 469 Average eval loss: 0.0001104947\n",
      "====> Epoch: 470 Average train loss: 0.0057484368\n",
      "====> Epoch: 470 Average L0 reg loss: 0.0183697509\n",
      "====> Epoch: 470 Average eval loss: 0.0001036069\n",
      "====> Epoch: 471 Average train loss: 0.0004580389\n",
      "====> Epoch: 471 Average L0 reg loss: 0.0183695936\n",
      "====> Epoch: 471 Average eval loss: 0.0001080028\n",
      "====> Epoch: 472 Average train loss: 0.0005038503\n",
      "====> Epoch: 472 Average L0 reg loss: 0.0183694514\n",
      "====> Epoch: 472 Average eval loss: 0.0003096359\n",
      "====> Epoch: 473 Average train loss: 0.0007784567\n",
      "====> Epoch: 473 Average L0 reg loss: 0.0183693354\n",
      "====> Epoch: 473 Average eval loss: 0.0001150085\n",
      "====> Epoch: 474 Average train loss: 0.0005376151\n",
      "====> Epoch: 474 Average L0 reg loss: 0.0183692450\n",
      "====> Epoch: 474 Average eval loss: 0.0001841736\n",
      "====> Epoch: 475 Average train loss: 0.0005566482\n",
      "====> Epoch: 475 Average L0 reg loss: 0.0183691644\n",
      "====> Epoch: 475 Average eval loss: 0.0001177997\n",
      "====> Epoch: 476 Average train loss: 0.0017946695\n",
      "====> Epoch: 476 Average L0 reg loss: 0.0183690925\n",
      "====> Epoch: 476 Average eval loss: 0.0002456479\n",
      "====> Epoch: 477 Average train loss: 0.0005451055\n",
      "====> Epoch: 477 Average L0 reg loss: 0.0183690264\n",
      "====> Epoch: 477 Average eval loss: 0.0001701062\n",
      "====> Epoch: 478 Average train loss: 0.0029232889\n",
      "====> Epoch: 478 Average L0 reg loss: 0.0183689706\n",
      "====> Epoch: 478 Average eval loss: 0.0002317653\n",
      "====> Epoch: 479 Average train loss: 0.0065839849\n",
      "====> Epoch: 479 Average L0 reg loss: 0.0183689271\n",
      "====> Epoch: 479 Average eval loss: 0.0001680615\n",
      "====> Epoch: 480 Average train loss: 0.0006279148\n",
      "====> Epoch: 480 Average L0 reg loss: 0.0183688942\n",
      "====> Epoch: 480 Average eval loss: 0.0001401475\n",
      "====> Epoch: 481 Average train loss: 0.0010372367\n",
      "====> Epoch: 481 Average L0 reg loss: 0.0183688775\n",
      "====> Epoch: 481 Average eval loss: 0.0001318975\n",
      "====> Epoch: 482 Average train loss: 0.0008571272\n",
      "====> Epoch: 482 Average L0 reg loss: 0.0183688684\n",
      "====> Epoch: 482 Average eval loss: 0.0001024191\n",
      "====> Epoch: 483 Average train loss: 0.0004479463\n",
      "====> Epoch: 483 Average L0 reg loss: 0.0183688599\n",
      "====> Epoch: 483 Average eval loss: 0.0001001136\n",
      "====> Epoch: 484 Average train loss: 0.0002922393\n",
      "====> Epoch: 484 Average L0 reg loss: 0.0183688514\n",
      "====> Epoch: 484 Average eval loss: 0.0001067784\n",
      "====> Epoch: 485 Average train loss: 0.0019817938\n",
      "====> Epoch: 485 Average L0 reg loss: 0.0183688434\n",
      "====> Epoch: 485 Average eval loss: 0.0000996423\n",
      "====> Epoch: 486 Average train loss: 0.0005203854\n",
      "====> Epoch: 486 Average L0 reg loss: 0.0183688342\n",
      "====> Epoch: 486 Average eval loss: 0.0000993889\n",
      "====> Epoch: 487 Average train loss: 0.0005209378\n",
      "====> Epoch: 487 Average L0 reg loss: 0.0183688237\n",
      "====> Epoch: 487 Average eval loss: 0.0001074489\n",
      "====> Epoch: 488 Average train loss: 0.0004190741\n",
      "====> Epoch: 488 Average L0 reg loss: 0.0183688120\n",
      "====> Epoch: 488 Average eval loss: 0.0001013257\n",
      "====> Epoch: 489 Average train loss: 0.0002881230\n",
      "====> Epoch: 489 Average L0 reg loss: 0.0183687992\n",
      "====> Epoch: 489 Average eval loss: 0.0001006482\n",
      "====> Epoch: 490 Average train loss: 0.0003705225\n",
      "====> Epoch: 490 Average L0 reg loss: 0.0183687884\n",
      "====> Epoch: 490 Average eval loss: 0.0000988391\n",
      "====> Epoch: 491 Average train loss: 0.0004778510\n",
      "====> Epoch: 491 Average L0 reg loss: 0.0183687785\n",
      "====> Epoch: 491 Average eval loss: 0.0001015438\n",
      "====> Epoch: 492 Average train loss: 0.0003858533\n",
      "====> Epoch: 492 Average L0 reg loss: 0.0183687699\n",
      "====> Epoch: 492 Average eval loss: 0.0001147370\n",
      "====> Epoch: 493 Average train loss: 0.0004588020\n",
      "====> Epoch: 493 Average L0 reg loss: 0.0183687627\n",
      "====> Epoch: 493 Average eval loss: 0.0001042461\n",
      "====> Epoch: 494 Average train loss: 0.0004669289\n",
      "====> Epoch: 494 Average L0 reg loss: 0.0183687562\n",
      "====> Epoch: 494 Average eval loss: 0.0000983645\n",
      "====> Epoch: 495 Average train loss: 0.0003539238\n",
      "====> Epoch: 495 Average L0 reg loss: 0.0183687482\n",
      "====> Epoch: 495 Average eval loss: 0.0001088007\n",
      "====> Epoch: 496 Average train loss: 0.0004997898\n",
      "====> Epoch: 496 Average L0 reg loss: 0.0183687432\n",
      "====> Epoch: 496 Average eval loss: 0.0001043754\n",
      "====> Epoch: 497 Average train loss: 0.0005615693\n",
      "====> Epoch: 497 Average L0 reg loss: 0.0183687374\n",
      "====> Epoch: 497 Average eval loss: 0.0000980216\n",
      "====> Epoch: 498 Average train loss: 0.0004406834\n",
      "====> Epoch: 498 Average L0 reg loss: 0.0183687343\n",
      "====> Epoch: 498 Average eval loss: 0.0003275902\n",
      "====> Epoch: 499 Average train loss: 0.0004950803\n",
      "====> Epoch: 499 Average L0 reg loss: 0.0183687325\n",
      "====> Epoch: 499 Average eval loss: 0.0001278322\n",
      "Best testing error sparse FCNN is 9.802155545912683e-05 and it was found at epoch 497\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train and evaluate the l0-sindy model with polynomial library functions.",
   "id": "e73746a15dbf55df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lib_type = 'polynomial'\n",
    "# degree of the polynomial features\n",
    "degree = 3\n",
    "\n",
    "reg_coefficient = 0.00005\n",
    "\n",
    "l0sindy_model = L0SINDy_dynamics(input_dim=obs_dim+act_dim, output_dim=obs_dim, degree=degree, lambda_coeff=reg_coefficient, lib_type=lib_type)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    l0sindy_model = l0sindy_model.cuda()\n",
    "\n",
    "optimizer_fcnn = torch.optim.Adam([\n",
    "    {'params': l0sindy_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_poly_l0sindy = train_eval_dynamics_model(l0sindy_model, optimizer_fcnn, training_buffer, testing_buffer, batch_size, num_epochs, l0=True)\n",
    "print(\"Best testing error polynomial L0 SINDy is {} and it was found at epoch {}\".format(metrics_poly_l0sindy[2], metrics_poly_l0sindy[3]))\n",
    "\n",
    "# print the close-form equation of the model. Great for interpretability!!!\n",
    "l0sindy_model.print_equations()"
   ],
   "id": "94717fb5ef5019b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train and evaluate the l0-sindy model with Fourier library functions.",
   "id": "8696848d193619ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lib_type = 'fourier'\n",
    "\n",
    "frequency = 1\n",
    "\n",
    "reg_coefficient = 0.005\n",
    "\n",
    "l0sindy_model = L0SINDy_dynamics(input_dim=obs_dim+act_dim, output_dim=obs_dim, degree=degree, lambda_coeff=reg_coefficient, lib_type=lib_type)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    l0sindy_model = l0sindy_model.cuda()\n",
    "\n",
    "optimizer_fcnn = torch.optim.Adam([\n",
    "    {'params': l0sindy_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_fourier_l0sindy = train_eval_dynamics_model(l0sindy_model, optimizer_fcnn, training_buffer, testing_buffer, batch_size, num_epochs, l0=True)\n",
    "print(\"Best testing error Fourier L0 SINDy is {} and it was found at epoch {}\".format(metrics_fourier_l0sindy[2], metrics_fourier_l0sindy[3]))\n",
    "\n",
    "# print the close-form equation of the model. Great for interpretability!!!\n",
    "l0sindy_model.print_equations()"
   ],
   "id": "a9c155f7c83771f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train and evaluate the l0-sindy model with polynomial and Fourier library functions.\n",
   "id": "9e9687b74745c23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lib_type = 'polyfourier'\n",
    "\n",
    "degree = 3\n",
    "frequency = 1\n",
    "\n",
    "reg_coefficient = 0.0005\n",
    "\n",
    "l0sindy_model = L0SINDy_dynamics(input_dim=obs_dim+act_dim, output_dim=obs_dim, degree=degree, lambda_coeff=reg_coefficient, lib_type=lib_type)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    l0sindy_model = l0sindy_model.cuda()\n",
    "\n",
    "optimizer_fcnn = torch.optim.Adam([\n",
    "    {'params': l0sindy_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_polyfourier_l0sindy = train_eval_dynamics_model(l0sindy_model, optimizer_fcnn, training_buffer, testing_buffer, batch_size, num_epochs, l0=True)\n",
    "print(\"Best testing error polynomial+Fourier L0 SINDy is {} and it was found at epoch {}\".format(metrics_polyfourier_l0sindy[2], metrics_polyfourier_l0sindy[3]))\n",
    "\n",
    "# print the close-form equation of the model. Great for interpretability!!!\n",
    "l0sindy_model.print_equations()"
   ],
   "id": "5e687d4585064d27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "536d79390732b2d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Eventually, we plot the mean-squared error between the predictions and the ground-truth over the training and test set.",
   "id": "90695781b7ab4e92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:10:21.625565Z",
     "start_time": "2024-07-16T11:10:21.071903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# creating the plots\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Training and Evaluation Metrics')\n",
    "\n",
    "data_train = {'NN': metrics_fcnn[0], 'SparseNN': metrics_sparsefcnn[0], 'L0SINDy (poly)': metrics_poly_l0sindy[0], 'L0SINDy (fourier)': metrics_fourier_l0sindy[0], 'L0SINDy (poly + fourier)': metrics_polyfourier_l0sindy[0]}\n",
    "methods_train = list(data_train.keys())\n",
    "values_train = list(data_train.values())\n",
    "\n",
    "# creating the bar plot\n",
    "ax1.bar(methods_train, values_train, color='maroon', width=0.4)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "data_eval = {'NN': metrics_fcnn[2], 'SparseNN': metrics_sparsefcnn[2], 'L0SINDy (poly)': metrics_poly_l0sindy[2], 'L0SINDy (fourier)': metrics_fourier_l0sindy[2], 'L0SINDy (poly + fourier)': metrics_polyfourier_l0sindy[2]}\n",
    "methods_eval = list(data_eval.keys())\n",
    "values_eval = list(data_eval.values())\n",
    "\n",
    "ax2.bar(methods_eval, values_eval, color='blue', width=0.4)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "save_dir = \"figures\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "fig.savefig('figures/LearningDynamics.png', dpi=300)"
   ],
   "id": "5b52985fbff205cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
