{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T15:19:12.791045Z",
     "start_time": "2024-05-22T15:19:12.175358Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "from replay_buffer import ReplayBuffer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T15:19:25.097621Z",
     "start_time": "2024-05-22T15:19:15.129320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "render = False\n",
    "if render:\n",
    "    env = gym.make('Pendulum-v1', g=9.81, render_mode=\"human\")\n",
    "else:\n",
    "    env = gym.make('Pendulum-v1', g=9.81)\n",
    "max_episodes = 1000\n",
    "max_steps = 200\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "buf_dim = int(max_episodes*max_steps)\n",
    "\n",
    "# create training set\n",
    "seed = 1\n",
    "training_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=buf_dim)\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    observation, info = env.reset(seed=seed)\n",
    "    for steps in range(max_steps+1):\n",
    "        action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        training_buffer.store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "print(\"Finished creating the training set\")"
   ],
   "id": "f30d49f053f6de6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/Documents/Sparsifying-Parametric-Models-with-L0/venv/lib/python3.10/site-packages/gymnasium/envs/classic_control/pendulum.py:173: UserWarning: \u001B[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\u001B[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating the training set\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T15:19:30.754163Z",
     "start_time": "2024-05-22T15:19:29.617472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create test set\n",
    "max_episodes_test = 50\n",
    "buf_dim = int(max_episodes*max_steps)\n",
    "\n",
    "seed = 7\n",
    "testing_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=buf_dim)\n",
    "\n",
    "for episode in range(max_episodes_test):\n",
    "    observation, info = env.reset(seed=seed)\n",
    "    for steps in range(max_steps + 1):\n",
    "        action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        testing_buffer.store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "        if done:\n",
    "            done = False\n",
    "            break\n",
    "\n",
    "print(\"Finished creating the test set\")"
   ],
   "id": "5c45b1ee713bfd84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating the test set\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T15:19:33.807554Z",
     "start_time": "2024-05-22T15:19:33.303207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# learning the dynamics of the pendulum\n",
    "from models import FCNN, SparseFCNN, L0SINDy_dynamics\n",
    "from trainer import train_eval_dynamics_model\n",
    "import torch\n",
    "\n",
    "h_dim = 64\n",
    "lr = 3e-4\n",
    "batch_size = 256\n",
    "num_epochs = 100"
   ],
   "id": "5b9d333e476e4824",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T15:20:41.712164Z",
     "start_time": "2024-05-22T15:19:36.263938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fcnn_model = FCNN(input_dim=obs_dim+act_dim, output_dim=obs_dim, h_dim=h_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    fcnn_model = fcnn_model.cuda()\n",
    "\n",
    "optimizer_fcnn = torch.optim.Adam([\n",
    "    {'params': fcnn_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_fcnn = train_eval_dynamics_model(fcnn_model, optimizer_fcnn, training_buffer, testing_buffer, batch_size, num_epochs)\n",
    "print(\"Best testing error FCNN is {} and it was found at epoch {}\".format(metrics_fcnn[2], metrics_fcnn[3]))\n"
   ],
   "id": "2cac37c5b681ba1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average train loss: 0.3316175924\n",
      "====> Epoch: 0 Average eval loss: 0.0185671188\n",
      "====> Epoch: 1 Average train loss: 0.0097222322\n",
      "====> Epoch: 1 Average eval loss: 0.0097943805\n",
      "====> Epoch: 2 Average train loss: 0.0054903799\n",
      "====> Epoch: 2 Average eval loss: 0.0054765902\n",
      "====> Epoch: 3 Average train loss: 0.0029297431\n",
      "====> Epoch: 3 Average eval loss: 0.0025566402\n",
      "====> Epoch: 4 Average train loss: 0.0013553764\n",
      "====> Epoch: 4 Average eval loss: 0.0010617595\n",
      "====> Epoch: 5 Average train loss: 0.0007652189\n",
      "====> Epoch: 5 Average eval loss: 0.0006517269\n",
      "====> Epoch: 6 Average train loss: 0.0005810741\n",
      "====> Epoch: 6 Average eval loss: 0.0005035270\n",
      "====> Epoch: 7 Average train loss: 0.0004844149\n",
      "====> Epoch: 7 Average eval loss: 0.0004416810\n",
      "====> Epoch: 8 Average train loss: 0.0004577814\n",
      "====> Epoch: 8 Average eval loss: 0.0003779757\n",
      "====> Epoch: 9 Average train loss: 0.0004195820\n",
      "====> Epoch: 9 Average eval loss: 0.0003569473\n",
      "====> Epoch: 10 Average train loss: 0.0004034427\n",
      "====> Epoch: 10 Average eval loss: 0.0003297643\n",
      "====> Epoch: 11 Average train loss: 0.0003904482\n",
      "====> Epoch: 11 Average eval loss: 0.0002924039\n",
      "====> Epoch: 12 Average train loss: 0.0003754924\n",
      "====> Epoch: 12 Average eval loss: 0.0003007390\n",
      "====> Epoch: 13 Average train loss: 0.0003751736\n",
      "====> Epoch: 13 Average eval loss: 0.0002973441\n",
      "====> Epoch: 14 Average train loss: 0.0003567257\n",
      "====> Epoch: 14 Average eval loss: 0.0003261182\n",
      "====> Epoch: 15 Average train loss: 0.0003483043\n",
      "====> Epoch: 15 Average eval loss: 0.0002829936\n",
      "====> Epoch: 16 Average train loss: 0.0003428939\n",
      "====> Epoch: 16 Average eval loss: 0.0002992266\n",
      "====> Epoch: 17 Average train loss: 0.0003437759\n",
      "====> Epoch: 17 Average eval loss: 0.0003895842\n",
      "====> Epoch: 18 Average train loss: 0.0003372481\n",
      "====> Epoch: 18 Average eval loss: 0.0002560428\n",
      "====> Epoch: 19 Average train loss: 0.0003175661\n",
      "====> Epoch: 19 Average eval loss: 0.0002424136\n",
      "====> Epoch: 20 Average train loss: 0.0003229123\n",
      "====> Epoch: 20 Average eval loss: 0.0004141814\n",
      "====> Epoch: 21 Average train loss: 0.0003202446\n",
      "====> Epoch: 21 Average eval loss: 0.0002430838\n",
      "====> Epoch: 22 Average train loss: 0.0003197833\n",
      "====> Epoch: 22 Average eval loss: 0.0002393449\n",
      "====> Epoch: 23 Average train loss: 0.0003158963\n",
      "====> Epoch: 23 Average eval loss: 0.0002739175\n",
      "====> Epoch: 24 Average train loss: 0.0003061022\n",
      "====> Epoch: 24 Average eval loss: 0.0002266947\n",
      "====> Epoch: 25 Average train loss: 0.0003041380\n",
      "====> Epoch: 25 Average eval loss: 0.0002249455\n",
      "====> Epoch: 26 Average train loss: 0.0002972484\n",
      "====> Epoch: 26 Average eval loss: 0.0002430429\n",
      "====> Epoch: 27 Average train loss: 0.0003103176\n",
      "====> Epoch: 27 Average eval loss: 0.0002536956\n",
      "====> Epoch: 28 Average train loss: 0.0003059609\n",
      "====> Epoch: 28 Average eval loss: 0.0002213465\n",
      "====> Epoch: 29 Average train loss: 0.0003028835\n",
      "====> Epoch: 29 Average eval loss: 0.0002157724\n",
      "====> Epoch: 30 Average train loss: 0.0002726535\n",
      "====> Epoch: 30 Average eval loss: 0.0003287275\n",
      "====> Epoch: 31 Average train loss: 0.0003007624\n",
      "====> Epoch: 31 Average eval loss: 0.0002193757\n",
      "====> Epoch: 32 Average train loss: 0.0002801447\n",
      "====> Epoch: 32 Average eval loss: 0.0002200078\n",
      "====> Epoch: 33 Average train loss: 0.0002808962\n",
      "====> Epoch: 33 Average eval loss: 0.0002535521\n",
      "====> Epoch: 34 Average train loss: 0.0002845518\n",
      "====> Epoch: 34 Average eval loss: 0.0002044306\n",
      "====> Epoch: 35 Average train loss: 0.0002809651\n",
      "====> Epoch: 35 Average eval loss: 0.0002666427\n",
      "====> Epoch: 36 Average train loss: 0.0002693608\n",
      "====> Epoch: 36 Average eval loss: 0.0001952549\n",
      "====> Epoch: 37 Average train loss: 0.0002681461\n",
      "====> Epoch: 37 Average eval loss: 0.0002088036\n",
      "====> Epoch: 38 Average train loss: 0.0002653313\n",
      "====> Epoch: 38 Average eval loss: 0.0002749547\n",
      "====> Epoch: 39 Average train loss: 0.0002712232\n",
      "====> Epoch: 39 Average eval loss: 0.0002341057\n",
      "====> Epoch: 40 Average train loss: 0.0002692228\n",
      "====> Epoch: 40 Average eval loss: 0.0002160265\n",
      "====> Epoch: 41 Average train loss: 0.0002583132\n",
      "====> Epoch: 41 Average eval loss: 0.0002271286\n",
      "====> Epoch: 42 Average train loss: 0.0002571060\n",
      "====> Epoch: 42 Average eval loss: 0.0001887848\n",
      "====> Epoch: 43 Average train loss: 0.0002631693\n",
      "====> Epoch: 43 Average eval loss: 0.0001782846\n",
      "====> Epoch: 44 Average train loss: 0.0002440729\n",
      "====> Epoch: 44 Average eval loss: 0.0001935818\n",
      "====> Epoch: 45 Average train loss: 0.0002401690\n",
      "====> Epoch: 45 Average eval loss: 0.0002574269\n",
      "====> Epoch: 46 Average train loss: 0.0002510372\n",
      "====> Epoch: 46 Average eval loss: 0.0002427193\n",
      "====> Epoch: 47 Average train loss: 0.0002488315\n",
      "====> Epoch: 47 Average eval loss: 0.0001843983\n",
      "====> Epoch: 48 Average train loss: 0.0002360439\n",
      "====> Epoch: 48 Average eval loss: 0.0001733130\n",
      "====> Epoch: 49 Average train loss: 0.0002309632\n",
      "====> Epoch: 49 Average eval loss: 0.0001667993\n",
      "====> Epoch: 50 Average train loss: 0.0002230331\n",
      "====> Epoch: 50 Average eval loss: 0.0001694876\n",
      "====> Epoch: 51 Average train loss: 0.0002268967\n",
      "====> Epoch: 51 Average eval loss: 0.0001606822\n",
      "====> Epoch: 52 Average train loss: 0.0002181176\n",
      "====> Epoch: 52 Average eval loss: 0.0001735011\n",
      "====> Epoch: 53 Average train loss: 0.0002219889\n",
      "====> Epoch: 53 Average eval loss: 0.0001782249\n",
      "====> Epoch: 54 Average train loss: 0.0002348999\n",
      "====> Epoch: 54 Average eval loss: 0.0001716058\n",
      "====> Epoch: 55 Average train loss: 0.0002131720\n",
      "====> Epoch: 55 Average eval loss: 0.0001616343\n",
      "====> Epoch: 56 Average train loss: 0.0002177710\n",
      "====> Epoch: 56 Average eval loss: 0.0001683472\n",
      "====> Epoch: 57 Average train loss: 0.0002164872\n",
      "====> Epoch: 57 Average eval loss: 0.0001946534\n",
      "====> Epoch: 58 Average train loss: 0.0002058937\n",
      "====> Epoch: 58 Average eval loss: 0.0001505600\n",
      "====> Epoch: 59 Average train loss: 0.0002120338\n",
      "====> Epoch: 59 Average eval loss: 0.0001502726\n",
      "====> Epoch: 60 Average train loss: 0.0002010547\n",
      "====> Epoch: 60 Average eval loss: 0.0002064982\n",
      "====> Epoch: 61 Average train loss: 0.0002023293\n",
      "====> Epoch: 61 Average eval loss: 0.0001538169\n",
      "====> Epoch: 62 Average train loss: 0.0002012677\n",
      "====> Epoch: 62 Average eval loss: 0.0001381281\n",
      "====> Epoch: 63 Average train loss: 0.0002015441\n",
      "====> Epoch: 63 Average eval loss: 0.0001495728\n",
      "====> Epoch: 64 Average train loss: 0.0001952146\n",
      "====> Epoch: 64 Average eval loss: 0.0001365537\n",
      "====> Epoch: 65 Average train loss: 0.0001919308\n",
      "====> Epoch: 65 Average eval loss: 0.0001336027\n",
      "====> Epoch: 66 Average train loss: 0.0001933852\n",
      "====> Epoch: 66 Average eval loss: 0.0001413119\n",
      "====> Epoch: 67 Average train loss: 0.0001866176\n",
      "====> Epoch: 67 Average eval loss: 0.0001588981\n",
      "====> Epoch: 68 Average train loss: 0.0001827543\n",
      "====> Epoch: 68 Average eval loss: 0.0001583164\n",
      "====> Epoch: 69 Average train loss: 0.0001728009\n",
      "====> Epoch: 69 Average eval loss: 0.0001647792\n",
      "====> Epoch: 70 Average train loss: 0.0001872279\n",
      "====> Epoch: 70 Average eval loss: 0.0001494106\n",
      "====> Epoch: 71 Average train loss: 0.0001776269\n",
      "====> Epoch: 71 Average eval loss: 0.0001378233\n",
      "====> Epoch: 72 Average train loss: 0.0001790312\n",
      "====> Epoch: 72 Average eval loss: 0.0001207809\n",
      "====> Epoch: 73 Average train loss: 0.0001741761\n",
      "====> Epoch: 73 Average eval loss: 0.0001209421\n",
      "====> Epoch: 74 Average train loss: 0.0001792674\n",
      "====> Epoch: 74 Average eval loss: 0.0001205707\n",
      "====> Epoch: 75 Average train loss: 0.0001669158\n",
      "====> Epoch: 75 Average eval loss: 0.0001265694\n",
      "====> Epoch: 76 Average train loss: 0.0001696029\n",
      "====> Epoch: 76 Average eval loss: 0.0001392604\n",
      "====> Epoch: 77 Average train loss: 0.0001707626\n",
      "====> Epoch: 77 Average eval loss: 0.0001188161\n",
      "====> Epoch: 78 Average train loss: 0.0001615463\n",
      "====> Epoch: 78 Average eval loss: 0.0001531463\n",
      "====> Epoch: 79 Average train loss: 0.0001626939\n",
      "====> Epoch: 79 Average eval loss: 0.0001309849\n",
      "====> Epoch: 80 Average train loss: 0.0001594575\n",
      "====> Epoch: 80 Average eval loss: 0.0001158900\n",
      "====> Epoch: 81 Average train loss: 0.0001655783\n",
      "====> Epoch: 81 Average eval loss: 0.0001128804\n",
      "====> Epoch: 82 Average train loss: 0.0001593488\n",
      "====> Epoch: 82 Average eval loss: 0.0001324356\n",
      "====> Epoch: 83 Average train loss: 0.0001577621\n",
      "====> Epoch: 83 Average eval loss: 0.0001064271\n",
      "====> Epoch: 84 Average train loss: 0.0001543117\n",
      "====> Epoch: 84 Average eval loss: 0.0001227511\n",
      "====> Epoch: 85 Average train loss: 0.0001528946\n",
      "====> Epoch: 85 Average eval loss: 0.0001367874\n",
      "====> Epoch: 86 Average train loss: 0.0001501108\n",
      "====> Epoch: 86 Average eval loss: 0.0001342354\n",
      "====> Epoch: 87 Average train loss: 0.0001497774\n",
      "====> Epoch: 87 Average eval loss: 0.0001158606\n",
      "====> Epoch: 88 Average train loss: 0.0001457585\n",
      "====> Epoch: 88 Average eval loss: 0.0001536478\n",
      "====> Epoch: 89 Average train loss: 0.0001430697\n",
      "====> Epoch: 89 Average eval loss: 0.0001046277\n",
      "====> Epoch: 90 Average train loss: 0.0001369513\n",
      "====> Epoch: 90 Average eval loss: 0.0001096255\n",
      "====> Epoch: 91 Average train loss: 0.0001441038\n",
      "====> Epoch: 91 Average eval loss: 0.0001575533\n",
      "====> Epoch: 92 Average train loss: 0.0001380708\n",
      "====> Epoch: 92 Average eval loss: 0.0000973915\n",
      "====> Epoch: 93 Average train loss: 0.0001324747\n",
      "====> Epoch: 93 Average eval loss: 0.0000913739\n",
      "====> Epoch: 94 Average train loss: 0.0001310240\n",
      "====> Epoch: 94 Average eval loss: 0.0001054334\n",
      "====> Epoch: 95 Average train loss: 0.0001355910\n",
      "====> Epoch: 95 Average eval loss: 0.0000944843\n",
      "====> Epoch: 96 Average train loss: 0.0001310134\n",
      "====> Epoch: 96 Average eval loss: 0.0000896784\n",
      "====> Epoch: 97 Average train loss: 0.0001348655\n",
      "====> Epoch: 97 Average eval loss: 0.0000868493\n",
      "====> Epoch: 98 Average train loss: 0.0001295940\n",
      "====> Epoch: 98 Average eval loss: 0.0001099550\n",
      "====> Epoch: 99 Average train loss: 0.0001303253\n",
      "====> Epoch: 99 Average eval loss: 0.0001205560\n",
      "Best testing error FCNN is 8.684931526659057e-05 and it was found at epoch 97\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T15:24:11.414987Z",
     "start_time": "2024-05-22T15:21:00.298031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reg_coefficient = 0.00001\n",
    "sparsefcnn_model = SparseFCNN(input_dim=obs_dim+act_dim, output_dim=obs_dim, h_dim=h_dim, lambda_coeff=reg_coefficient)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    sparsefcnn_model = sparsefcnn_model.cuda()\n",
    "\n",
    "optimizer_sparsefcnn = torch.optim.Adam([\n",
    "    {'params': sparsefcnn_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_sparsefcnn = train_eval_dynamics_model(sparsefcnn_model, optimizer_sparsefcnn, training_buffer, testing_buffer,\n",
    "                                               batch_size, num_epochs, l0=True)\n",
    "print(\"Best testing error sparse FCNN is {} and it was found at epoch {}\".format(metrics_sparsefcnn[2], metrics_sparsefcnn[3]))\n"
   ],
   "id": "5f6094f05663b753",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/Documents/Sparsifying-Parametric-Models-with-L0/l0_layer.py:51: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.weights, mode='fan_out')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0Dense(4 -> 64, droprate_init=0.5, lamba=1e-05, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False)\n",
      "L0Dense(64 -> 64, droprate_init=0.5, lamba=1e-05, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False)\n",
      "L0Dense(64 -> 3, droprate_init=0.5, lamba=1e-05, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False)\n",
      "====> Epoch: 0 Average train loss: 3.7578335948\n",
      "====> Epoch: 0 Average L0 reg loss: 0.0378251801\n",
      "====> Epoch: 0 Average eval loss: 0.5391274691\n",
      "====> Epoch: 1 Average train loss: 2.6602392671\n",
      "====> Epoch: 1 Average L0 reg loss: 0.0379013693\n",
      "====> Epoch: 1 Average eval loss: 0.4006974101\n",
      "====> Epoch: 2 Average train loss: 2.1774580526\n",
      "====> Epoch: 2 Average L0 reg loss: 0.0379821038\n",
      "====> Epoch: 2 Average eval loss: 0.2398997098\n",
      "====> Epoch: 3 Average train loss: 1.8725506838\n",
      "====> Epoch: 3 Average L0 reg loss: 0.0380774180\n",
      "====> Epoch: 3 Average eval loss: 0.2353447229\n",
      "====> Epoch: 4 Average train loss: 1.7574790091\n",
      "====> Epoch: 4 Average L0 reg loss: 0.0381775061\n",
      "====> Epoch: 4 Average eval loss: 0.1752458662\n",
      "====> Epoch: 5 Average train loss: 1.8061228309\n",
      "====> Epoch: 5 Average L0 reg loss: 0.0382895999\n",
      "====> Epoch: 5 Average eval loss: 0.1769764870\n",
      "====> Epoch: 6 Average train loss: 1.6247636957\n",
      "====> Epoch: 6 Average L0 reg loss: 0.0383880927\n",
      "====> Epoch: 6 Average eval loss: 0.1772630513\n",
      "====> Epoch: 7 Average train loss: 1.3705347013\n",
      "====> Epoch: 7 Average L0 reg loss: 0.0384811197\n",
      "====> Epoch: 7 Average eval loss: 0.1792065948\n",
      "====> Epoch: 8 Average train loss: 1.3123081292\n",
      "====> Epoch: 8 Average L0 reg loss: 0.0385758453\n",
      "====> Epoch: 8 Average eval loss: 0.1948110759\n",
      "====> Epoch: 9 Average train loss: 1.3364047044\n",
      "====> Epoch: 9 Average L0 reg loss: 0.0386690744\n",
      "====> Epoch: 9 Average eval loss: 0.2124550790\n",
      "====> Epoch: 10 Average train loss: 1.2330331907\n",
      "====> Epoch: 10 Average L0 reg loss: 0.0387531349\n",
      "====> Epoch: 10 Average eval loss: 0.2021421641\n",
      "====> Epoch: 11 Average train loss: 1.1898288346\n",
      "====> Epoch: 11 Average L0 reg loss: 0.0388247554\n",
      "====> Epoch: 11 Average eval loss: 0.1968063563\n",
      "====> Epoch: 12 Average train loss: 1.1938993290\n",
      "====> Epoch: 12 Average L0 reg loss: 0.0388987449\n",
      "====> Epoch: 12 Average eval loss: 0.1848588586\n",
      "====> Epoch: 13 Average train loss: 1.0493506733\n",
      "====> Epoch: 13 Average L0 reg loss: 0.0389745236\n",
      "====> Epoch: 13 Average eval loss: 0.1946130246\n",
      "====> Epoch: 14 Average train loss: 0.9794802882\n",
      "====> Epoch: 14 Average L0 reg loss: 0.0390461103\n",
      "====> Epoch: 14 Average eval loss: 0.1999823898\n",
      "====> Epoch: 15 Average train loss: 0.9397374754\n",
      "====> Epoch: 15 Average L0 reg loss: 0.0391275499\n",
      "====> Epoch: 15 Average eval loss: 0.2006176412\n",
      "====> Epoch: 16 Average train loss: 0.9113598190\n",
      "====> Epoch: 16 Average L0 reg loss: 0.0391992681\n",
      "====> Epoch: 16 Average eval loss: 0.1771073192\n",
      "====> Epoch: 17 Average train loss: 0.9256633455\n",
      "====> Epoch: 17 Average L0 reg loss: 0.0392660485\n",
      "====> Epoch: 17 Average eval loss: 0.1837877482\n",
      "====> Epoch: 18 Average train loss: 0.8411844847\n",
      "====> Epoch: 18 Average L0 reg loss: 0.0393293701\n",
      "====> Epoch: 18 Average eval loss: 0.2359913141\n",
      "====> Epoch: 19 Average train loss: 0.7317772372\n",
      "====> Epoch: 19 Average L0 reg loss: 0.0394007906\n",
      "====> Epoch: 19 Average eval loss: 0.2472817302\n",
      "====> Epoch: 20 Average train loss: 0.7538970685\n",
      "====> Epoch: 20 Average L0 reg loss: 0.0394710595\n",
      "====> Epoch: 20 Average eval loss: 0.2474983782\n",
      "====> Epoch: 21 Average train loss: 0.6862173597\n",
      "====> Epoch: 21 Average L0 reg loss: 0.0395401794\n",
      "====> Epoch: 21 Average eval loss: 0.2359870821\n",
      "====> Epoch: 22 Average train loss: 0.7350725905\n",
      "====> Epoch: 22 Average L0 reg loss: 0.0396091768\n",
      "====> Epoch: 22 Average eval loss: 0.2403116822\n",
      "====> Epoch: 23 Average train loss: 0.6582872034\n",
      "====> Epoch: 23 Average L0 reg loss: 0.0396713415\n",
      "====> Epoch: 23 Average eval loss: 0.2776840627\n",
      "====> Epoch: 24 Average train loss: 0.6108161531\n",
      "====> Epoch: 24 Average L0 reg loss: 0.0397369642\n",
      "====> Epoch: 24 Average eval loss: 0.1717352718\n",
      "====> Epoch: 25 Average train loss: 0.6790540455\n",
      "====> Epoch: 25 Average L0 reg loss: 0.0398029233\n",
      "====> Epoch: 25 Average eval loss: 0.1861692965\n",
      "====> Epoch: 26 Average train loss: 0.5347084089\n",
      "====> Epoch: 26 Average L0 reg loss: 0.0398718633\n",
      "====> Epoch: 26 Average eval loss: 0.2357290387\n",
      "====> Epoch: 27 Average train loss: 0.5604412606\n",
      "====> Epoch: 27 Average L0 reg loss: 0.0399439201\n",
      "====> Epoch: 27 Average eval loss: 0.3292405009\n",
      "====> Epoch: 28 Average train loss: 0.5371941529\n",
      "====> Epoch: 28 Average L0 reg loss: 0.0400119058\n",
      "====> Epoch: 28 Average eval loss: 0.2054468840\n",
      "====> Epoch: 29 Average train loss: 0.4769546795\n",
      "====> Epoch: 29 Average L0 reg loss: 0.0400796411\n",
      "====> Epoch: 29 Average eval loss: 0.2824620903\n",
      "====> Epoch: 30 Average train loss: 0.5048229466\n",
      "====> Epoch: 30 Average L0 reg loss: 0.0401503341\n",
      "====> Epoch: 30 Average eval loss: 0.2470609695\n",
      "====> Epoch: 31 Average train loss: 0.5135549841\n",
      "====> Epoch: 31 Average L0 reg loss: 0.0402168897\n",
      "====> Epoch: 31 Average eval loss: 0.3321717381\n",
      "====> Epoch: 32 Average train loss: 0.4517362172\n",
      "====> Epoch: 32 Average L0 reg loss: 0.0402868485\n",
      "====> Epoch: 32 Average eval loss: 0.2426317483\n",
      "====> Epoch: 33 Average train loss: 0.5092388778\n",
      "====> Epoch: 33 Average L0 reg loss: 0.0403510183\n",
      "====> Epoch: 33 Average eval loss: 0.2153625637\n",
      "====> Epoch: 34 Average train loss: 0.4845348881\n",
      "====> Epoch: 34 Average L0 reg loss: 0.0404124424\n",
      "====> Epoch: 34 Average eval loss: 0.2819811702\n",
      "====> Epoch: 35 Average train loss: 0.4037808319\n",
      "====> Epoch: 35 Average L0 reg loss: 0.0404743261\n",
      "====> Epoch: 35 Average eval loss: 0.2094262689\n",
      "====> Epoch: 36 Average train loss: 0.4241884945\n",
      "====> Epoch: 36 Average L0 reg loss: 0.0405355278\n",
      "====> Epoch: 36 Average eval loss: 0.2013942897\n",
      "====> Epoch: 37 Average train loss: 0.3600931601\n",
      "====> Epoch: 37 Average L0 reg loss: 0.0405943826\n",
      "====> Epoch: 37 Average eval loss: 0.1677877009\n",
      "====> Epoch: 38 Average train loss: 0.3875517403\n",
      "====> Epoch: 38 Average L0 reg loss: 0.0406626732\n",
      "====> Epoch: 38 Average eval loss: 0.2854318917\n",
      "====> Epoch: 39 Average train loss: 0.3319706310\n",
      "====> Epoch: 39 Average L0 reg loss: 0.0407238362\n",
      "====> Epoch: 39 Average eval loss: 0.2725019753\n",
      "====> Epoch: 40 Average train loss: 0.3930139836\n",
      "====> Epoch: 40 Average L0 reg loss: 0.0407837835\n",
      "====> Epoch: 40 Average eval loss: 0.2130578309\n",
      "====> Epoch: 41 Average train loss: 0.3183667029\n",
      "====> Epoch: 41 Average L0 reg loss: 0.0408454743\n",
      "====> Epoch: 41 Average eval loss: 0.2351811230\n",
      "====> Epoch: 42 Average train loss: 0.3709865133\n",
      "====> Epoch: 42 Average L0 reg loss: 0.0409087204\n",
      "====> Epoch: 42 Average eval loss: 0.2504411936\n",
      "====> Epoch: 43 Average train loss: 0.2881148420\n",
      "====> Epoch: 43 Average L0 reg loss: 0.0409674667\n",
      "====> Epoch: 43 Average eval loss: 0.2646821737\n",
      "====> Epoch: 44 Average train loss: 0.2473726117\n",
      "====> Epoch: 44 Average L0 reg loss: 0.0410286233\n",
      "====> Epoch: 44 Average eval loss: 0.2549695373\n",
      "====> Epoch: 45 Average train loss: 0.2705000135\n",
      "====> Epoch: 45 Average L0 reg loss: 0.0410868541\n",
      "====> Epoch: 45 Average eval loss: 0.2437660247\n",
      "====> Epoch: 46 Average train loss: 0.2773114740\n",
      "====> Epoch: 46 Average L0 reg loss: 0.0411395457\n",
      "====> Epoch: 46 Average eval loss: 0.3397167027\n",
      "====> Epoch: 47 Average train loss: 0.2520162721\n",
      "====> Epoch: 47 Average L0 reg loss: 0.0411928366\n",
      "====> Epoch: 47 Average eval loss: 0.2336324602\n",
      "====> Epoch: 48 Average train loss: 0.2376556141\n",
      "====> Epoch: 48 Average L0 reg loss: 0.0412484358\n",
      "====> Epoch: 48 Average eval loss: 0.2110135555\n",
      "====> Epoch: 49 Average train loss: 0.2944454255\n",
      "====> Epoch: 49 Average L0 reg loss: 0.0413025999\n",
      "====> Epoch: 49 Average eval loss: 0.1970300972\n",
      "====> Epoch: 50 Average train loss: 0.2612836906\n",
      "====> Epoch: 50 Average L0 reg loss: 0.0413525346\n",
      "====> Epoch: 50 Average eval loss: 0.3737017512\n",
      "====> Epoch: 51 Average train loss: 0.2914754736\n",
      "====> Epoch: 51 Average L0 reg loss: 0.0414022997\n",
      "====> Epoch: 51 Average eval loss: 0.2993729115\n",
      "====> Epoch: 52 Average train loss: 0.2455238123\n",
      "====> Epoch: 52 Average L0 reg loss: 0.0414493433\n",
      "====> Epoch: 52 Average eval loss: 0.1795032620\n",
      "====> Epoch: 53 Average train loss: 0.2406108484\n",
      "====> Epoch: 53 Average L0 reg loss: 0.0414978767\n",
      "====> Epoch: 53 Average eval loss: 0.3391368389\n",
      "====> Epoch: 54 Average train loss: 0.2042257672\n",
      "====> Epoch: 54 Average L0 reg loss: 0.0415496286\n",
      "====> Epoch: 54 Average eval loss: 0.1784196794\n",
      "====> Epoch: 55 Average train loss: 0.1959111485\n",
      "====> Epoch: 55 Average L0 reg loss: 0.0416000429\n",
      "====> Epoch: 55 Average eval loss: 0.2743140757\n",
      "====> Epoch: 56 Average train loss: 0.2130021352\n",
      "====> Epoch: 56 Average L0 reg loss: 0.0416466362\n",
      "====> Epoch: 56 Average eval loss: 0.2745462060\n",
      "====> Epoch: 57 Average train loss: 0.2753891489\n",
      "====> Epoch: 57 Average L0 reg loss: 0.0416917513\n",
      "====> Epoch: 57 Average eval loss: 0.3552906215\n",
      "====> Epoch: 58 Average train loss: 0.1742571610\n",
      "====> Epoch: 58 Average L0 reg loss: 0.0417382733\n",
      "====> Epoch: 58 Average eval loss: 0.2134560198\n",
      "====> Epoch: 59 Average train loss: 0.1979780485\n",
      "====> Epoch: 59 Average L0 reg loss: 0.0417815488\n",
      "====> Epoch: 59 Average eval loss: 0.2408832312\n",
      "====> Epoch: 60 Average train loss: 0.1854636961\n",
      "====> Epoch: 60 Average L0 reg loss: 0.0418208956\n",
      "====> Epoch: 60 Average eval loss: 0.1883034855\n",
      "====> Epoch: 61 Average train loss: 0.1846766509\n",
      "====> Epoch: 61 Average L0 reg loss: 0.0418611065\n",
      "====> Epoch: 61 Average eval loss: 0.1261415333\n",
      "====> Epoch: 62 Average train loss: 0.1525801230\n",
      "====> Epoch: 62 Average L0 reg loss: 0.0419027340\n",
      "====> Epoch: 62 Average eval loss: 0.3245695233\n",
      "====> Epoch: 63 Average train loss: 0.1590818474\n",
      "====> Epoch: 63 Average L0 reg loss: 0.0419389214\n",
      "====> Epoch: 63 Average eval loss: 0.1650358438\n",
      "====> Epoch: 64 Average train loss: 0.1456676524\n",
      "====> Epoch: 64 Average L0 reg loss: 0.0419758232\n",
      "====> Epoch: 64 Average eval loss: 0.3338975608\n",
      "====> Epoch: 65 Average train loss: 0.1769250869\n",
      "====> Epoch: 65 Average L0 reg loss: 0.0420158078\n",
      "====> Epoch: 65 Average eval loss: 0.2389573008\n",
      "====> Epoch: 66 Average train loss: 0.1636269300\n",
      "====> Epoch: 66 Average L0 reg loss: 0.0420570508\n",
      "====> Epoch: 66 Average eval loss: 0.2443234771\n",
      "====> Epoch: 67 Average train loss: 0.1633225795\n",
      "====> Epoch: 67 Average L0 reg loss: 0.0420908327\n",
      "====> Epoch: 67 Average eval loss: 0.2324855030\n",
      "====> Epoch: 68 Average train loss: 0.1823862547\n",
      "====> Epoch: 68 Average L0 reg loss: 0.0421273688\n",
      "====> Epoch: 68 Average eval loss: 0.2229262143\n",
      "====> Epoch: 69 Average train loss: 0.1479422787\n",
      "====> Epoch: 69 Average L0 reg loss: 0.0421640826\n",
      "====> Epoch: 69 Average eval loss: 0.2746835947\n",
      "====> Epoch: 70 Average train loss: 0.1321191290\n",
      "====> Epoch: 70 Average L0 reg loss: 0.0421990040\n",
      "====> Epoch: 70 Average eval loss: 0.1657512784\n",
      "====> Epoch: 71 Average train loss: 0.1893625939\n",
      "====> Epoch: 71 Average L0 reg loss: 0.0422367764\n",
      "====> Epoch: 71 Average eval loss: 0.2221067250\n",
      "====> Epoch: 72 Average train loss: 0.1209575433\n",
      "====> Epoch: 72 Average L0 reg loss: 0.0422704558\n",
      "====> Epoch: 72 Average eval loss: 0.2977307141\n",
      "====> Epoch: 73 Average train loss: 0.1450565648\n",
      "====> Epoch: 73 Average L0 reg loss: 0.0423049150\n",
      "====> Epoch: 73 Average eval loss: 0.1421739906\n",
      "====> Epoch: 74 Average train loss: 0.1660295031\n",
      "====> Epoch: 74 Average L0 reg loss: 0.0423368379\n",
      "====> Epoch: 74 Average eval loss: 0.2877367735\n",
      "====> Epoch: 75 Average train loss: 0.1107257317\n",
      "====> Epoch: 75 Average L0 reg loss: 0.0423650638\n",
      "====> Epoch: 75 Average eval loss: 0.2211667299\n",
      "====> Epoch: 76 Average train loss: 0.1433462194\n",
      "====> Epoch: 76 Average L0 reg loss: 0.0423922417\n",
      "====> Epoch: 76 Average eval loss: 0.2629197836\n",
      "====> Epoch: 77 Average train loss: 0.1150849941\n",
      "====> Epoch: 77 Average L0 reg loss: 0.0424257188\n",
      "====> Epoch: 77 Average eval loss: 0.2447238266\n",
      "====> Epoch: 78 Average train loss: 0.1284522217\n",
      "====> Epoch: 78 Average L0 reg loss: 0.0424560113\n",
      "====> Epoch: 78 Average eval loss: 0.1860914528\n",
      "====> Epoch: 79 Average train loss: 0.1108572440\n",
      "====> Epoch: 79 Average L0 reg loss: 0.0424865063\n",
      "====> Epoch: 79 Average eval loss: 0.2218090892\n",
      "====> Epoch: 80 Average train loss: 0.1015257748\n",
      "====> Epoch: 80 Average L0 reg loss: 0.0425137210\n",
      "====> Epoch: 80 Average eval loss: 0.1623791456\n",
      "====> Epoch: 81 Average train loss: 0.1055781659\n",
      "====> Epoch: 81 Average L0 reg loss: 0.0425384447\n",
      "====> Epoch: 81 Average eval loss: 0.2707992196\n",
      "====> Epoch: 82 Average train loss: 0.1206485332\n",
      "====> Epoch: 82 Average L0 reg loss: 0.0425664583\n",
      "====> Epoch: 82 Average eval loss: 0.1896823943\n",
      "====> Epoch: 83 Average train loss: 0.0877086211\n",
      "====> Epoch: 83 Average L0 reg loss: 0.0425902990\n",
      "====> Epoch: 83 Average eval loss: 0.1870955378\n",
      "====> Epoch: 84 Average train loss: 0.1195294736\n",
      "====> Epoch: 84 Average L0 reg loss: 0.0426123105\n",
      "====> Epoch: 84 Average eval loss: 0.1088521183\n",
      "====> Epoch: 85 Average train loss: 0.1225099392\n",
      "====> Epoch: 85 Average L0 reg loss: 0.0426360423\n",
      "====> Epoch: 85 Average eval loss: 0.1822498441\n",
      "====> Epoch: 86 Average train loss: 0.0935660688\n",
      "====> Epoch: 86 Average L0 reg loss: 0.0426620966\n",
      "====> Epoch: 86 Average eval loss: 0.1381647736\n",
      "====> Epoch: 87 Average train loss: 0.0816240274\n",
      "====> Epoch: 87 Average L0 reg loss: 0.0426882561\n",
      "====> Epoch: 87 Average eval loss: 0.1721939743\n",
      "====> Epoch: 88 Average train loss: 0.0742177049\n",
      "====> Epoch: 88 Average L0 reg loss: 0.0427120273\n",
      "====> Epoch: 88 Average eval loss: 0.1147597805\n",
      "====> Epoch: 89 Average train loss: 0.0949639928\n",
      "====> Epoch: 89 Average L0 reg loss: 0.0427343532\n",
      "====> Epoch: 89 Average eval loss: 0.2267017215\n",
      "====> Epoch: 90 Average train loss: 0.0717673446\n",
      "====> Epoch: 90 Average L0 reg loss: 0.0427557023\n",
      "====> Epoch: 90 Average eval loss: 0.1278585196\n",
      "====> Epoch: 91 Average train loss: 0.1081041337\n",
      "====> Epoch: 91 Average L0 reg loss: 0.0427789146\n",
      "====> Epoch: 91 Average eval loss: 0.1945843250\n",
      "====> Epoch: 92 Average train loss: 0.0878276386\n",
      "====> Epoch: 92 Average L0 reg loss: 0.0428004170\n",
      "====> Epoch: 92 Average eval loss: 0.2512162328\n",
      "====> Epoch: 93 Average train loss: 0.0701863052\n",
      "====> Epoch: 93 Average L0 reg loss: 0.0428189405\n",
      "====> Epoch: 93 Average eval loss: 0.1721190065\n",
      "====> Epoch: 94 Average train loss: 0.0766932128\n",
      "====> Epoch: 94 Average L0 reg loss: 0.0428373488\n",
      "====> Epoch: 94 Average eval loss: 0.1300888807\n",
      "====> Epoch: 95 Average train loss: 0.0464500531\n",
      "====> Epoch: 95 Average L0 reg loss: 0.0428545744\n",
      "====> Epoch: 95 Average eval loss: 0.1934438050\n",
      "====> Epoch: 96 Average train loss: 0.0670279090\n",
      "====> Epoch: 96 Average L0 reg loss: 0.0428707267\n",
      "====> Epoch: 96 Average eval loss: 0.1322821230\n",
      "====> Epoch: 97 Average train loss: 0.0588191970\n",
      "====> Epoch: 97 Average L0 reg loss: 0.0428870238\n",
      "====> Epoch: 97 Average eval loss: 0.1224805787\n",
      "====> Epoch: 98 Average train loss: 0.0735758425\n",
      "====> Epoch: 98 Average L0 reg loss: 0.0429029072\n",
      "====> Epoch: 98 Average eval loss: 0.1560976654\n",
      "====> Epoch: 99 Average train loss: 0.0734565953\n",
      "====> Epoch: 99 Average L0 reg loss: 0.0429203923\n",
      "====> Epoch: 99 Average eval loss: 0.1247472540\n",
      "Best testing error sparse FCNN is 0.10885211825370789 and it was found at epoch 84\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T15:26:38.196244Z",
     "start_time": "2024-05-22T15:24:35.314586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "degree = 3\n",
    "reg_coefficient = 0.01\n",
    "l0sindy_model = L0SINDy_dynamics(input_dim=obs_dim+act_dim, output_dim=obs_dim, degree=degree, lambda_coeff=reg_coefficient)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    l0sindy_model = l0sindy_model.cuda()\n",
    "\n",
    "optimizer_fcnn = torch.optim.Adam([\n",
    "    {'params': l0sindy_model.parameters()},\n",
    "], lr=lr, weight_decay=0.0)\n",
    "\n",
    "metrics_l0sindy = train_eval_dynamics_model(l0sindy_model, optimizer_fcnn, training_buffer, testing_buffer, batch_size, num_epochs, l0=True)\n",
    "print(\"Best testing error L0 SINDy is {} and it was found at epoch {}\".format(metrics_l0sindy[2], metrics_l0sindy[3]))\n"
   ],
   "id": "94717fb5ef5019b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy polynomial of order  3\n",
      "with 35 coefficients\n",
      "['1' 'x0' 'x1' 'x2' 'x3' 'x0^2' 'x0 x1' 'x0 x2' 'x0 x3' 'x1^2' 'x1 x2'\n",
      " 'x1 x3' 'x2^2' 'x2 x3' 'x3^2' 'x0^3' 'x0^2 x1' 'x0^2 x2' 'x0^2 x3'\n",
      " 'x0 x1^2' 'x0 x1 x2' 'x0 x1 x3' 'x0 x2^2' 'x0 x2 x3' 'x0 x3^2' 'x1^3'\n",
      " 'x1^2 x2' 'x1^2 x3' 'x1 x2^2' 'x1 x2 x3' 'x1 x3^2' 'x2^3' 'x2^2 x3'\n",
      " 'x2 x3^2' 'x3^3']\n",
      "L0Dense(35 -> 1, droprate_init=0.5, lamba=1.0, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False, bias=False)\n",
      "L0Dense(35 -> 1, droprate_init=0.5, lamba=1.0, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False, bias=False)\n",
      "L0Dense(35 -> 1, droprate_init=0.5, lamba=1.0, temperature=0.6666666666666666, weight_decay=0.0, local_rep=False, bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/Documents/Sparsifying-Parametric-Models-with-L0/l0_layer.py:51: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.weights, mode='fan_out')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average train loss: 1115.9830772581\n",
      "====> Epoch: 0 Average eval loss: 300.5393676758\n",
      "====> Epoch: 1 Average train loss: 372.8024495255\n",
      "====> Epoch: 1 Average eval loss: 81.5217590332\n",
      "====> Epoch: 2 Average train loss: 110.9633265128\n",
      "====> Epoch: 2 Average eval loss: 20.7075805664\n",
      "====> Epoch: 3 Average train loss: 31.8853292001\n",
      "====> Epoch: 3 Average eval loss: 5.0292463303\n",
      "====> Epoch: 4 Average train loss: 9.6706419132\n",
      "====> Epoch: 4 Average eval loss: 1.8378915787\n",
      "====> Epoch: 5 Average train loss: 3.9099170293\n",
      "====> Epoch: 5 Average eval loss: 1.3069804907\n",
      "====> Epoch: 6 Average train loss: 2.4505540819\n",
      "====> Epoch: 6 Average eval loss: 0.8210397959\n",
      "====> Epoch: 7 Average train loss: 1.6225682306\n",
      "====> Epoch: 7 Average eval loss: 0.4886419177\n",
      "====> Epoch: 8 Average train loss: 1.1212964719\n",
      "====> Epoch: 8 Average eval loss: 0.2619820237\n",
      "====> Epoch: 9 Average train loss: 0.8504627450\n",
      "====> Epoch: 9 Average eval loss: 0.1330213249\n",
      "====> Epoch: 10 Average train loss: 0.7063112145\n",
      "====> Epoch: 10 Average eval loss: 0.0878993273\n",
      "====> Epoch: 11 Average train loss: 0.6415001334\n",
      "====> Epoch: 11 Average eval loss: 0.0581704415\n",
      "====> Epoch: 12 Average train loss: 0.5596503997\n",
      "====> Epoch: 12 Average eval loss: 0.0512767732\n",
      "====> Epoch: 13 Average train loss: 0.5477542731\n",
      "====> Epoch: 13 Average eval loss: 0.0353776440\n",
      "====> Epoch: 14 Average train loss: 0.4764630915\n",
      "====> Epoch: 14 Average eval loss: 0.0628211498\n",
      "====> Epoch: 15 Average train loss: 0.4265374199\n",
      "====> Epoch: 15 Average eval loss: 0.0508040339\n",
      "====> Epoch: 16 Average train loss: 0.4540619812\n",
      "====> Epoch: 16 Average eval loss: 0.0432580560\n",
      "====> Epoch: 17 Average train loss: 0.4094071870\n",
      "====> Epoch: 17 Average eval loss: 0.0494223163\n",
      "====> Epoch: 18 Average train loss: 0.3683644449\n",
      "====> Epoch: 18 Average eval loss: 0.0304433331\n",
      "====> Epoch: 19 Average train loss: 0.3640736482\n",
      "====> Epoch: 19 Average eval loss: 0.0318922400\n",
      "====> Epoch: 20 Average train loss: 0.3493182591\n",
      "====> Epoch: 20 Average eval loss: 0.0399171002\n",
      "====> Epoch: 21 Average train loss: 0.3036461596\n",
      "====> Epoch: 21 Average eval loss: 0.0576613061\n",
      "====> Epoch: 22 Average train loss: 0.2900643779\n",
      "====> Epoch: 22 Average eval loss: 0.0860877633\n",
      "====> Epoch: 23 Average train loss: 0.2738414762\n",
      "====> Epoch: 23 Average eval loss: 0.0290670991\n",
      "====> Epoch: 24 Average train loss: 0.2605741769\n",
      "====> Epoch: 24 Average eval loss: 0.0295430496\n",
      "====> Epoch: 25 Average train loss: 0.2356560679\n",
      "====> Epoch: 25 Average eval loss: 0.0757746696\n",
      "====> Epoch: 26 Average train loss: 0.2291758990\n",
      "====> Epoch: 26 Average eval loss: 0.0258388910\n",
      "====> Epoch: 27 Average train loss: 0.2288300935\n",
      "====> Epoch: 27 Average eval loss: 0.0444789790\n",
      "====> Epoch: 28 Average train loss: 0.2127088982\n",
      "====> Epoch: 28 Average eval loss: 0.0376941487\n",
      "====> Epoch: 29 Average train loss: 0.1632713665\n",
      "====> Epoch: 29 Average eval loss: 0.0361673906\n",
      "====> Epoch: 30 Average train loss: 0.1926094291\n",
      "====> Epoch: 30 Average eval loss: 0.0350545645\n",
      "====> Epoch: 31 Average train loss: 0.1788035582\n",
      "====> Epoch: 31 Average eval loss: 0.0425931811\n",
      "====> Epoch: 32 Average train loss: 0.1639921304\n",
      "====> Epoch: 32 Average eval loss: 0.0153122349\n",
      "====> Epoch: 33 Average train loss: 0.1597858869\n",
      "====> Epoch: 33 Average eval loss: 0.0204883367\n",
      "====> Epoch: 34 Average train loss: 0.1515735691\n",
      "====> Epoch: 34 Average eval loss: 0.0174573697\n",
      "====> Epoch: 35 Average train loss: 0.1478446149\n",
      "====> Epoch: 35 Average eval loss: 0.0278344695\n",
      "====> Epoch: 36 Average train loss: 0.1224348737\n",
      "====> Epoch: 36 Average eval loss: 0.0238539316\n",
      "====> Epoch: 37 Average train loss: 0.1095558636\n",
      "====> Epoch: 37 Average eval loss: 0.0246898197\n",
      "====> Epoch: 38 Average train loss: 0.1281907649\n",
      "====> Epoch: 38 Average eval loss: 0.0370123163\n",
      "====> Epoch: 39 Average train loss: 0.1173524189\n",
      "====> Epoch: 39 Average eval loss: 0.0375277251\n",
      "====> Epoch: 40 Average train loss: 0.1011585990\n",
      "====> Epoch: 40 Average eval loss: 0.0218314361\n",
      "====> Epoch: 41 Average train loss: 0.1018910450\n",
      "====> Epoch: 41 Average eval loss: 0.0098001277\n",
      "====> Epoch: 42 Average train loss: 0.0978691809\n",
      "====> Epoch: 42 Average eval loss: 0.0252689086\n",
      "====> Epoch: 43 Average train loss: 0.0918897322\n",
      "====> Epoch: 43 Average eval loss: 0.0029270146\n",
      "====> Epoch: 44 Average train loss: 0.0790961544\n",
      "====> Epoch: 44 Average eval loss: 0.0085186614\n",
      "====> Epoch: 45 Average train loss: 0.0814380712\n",
      "====> Epoch: 45 Average eval loss: 0.0132552674\n",
      "====> Epoch: 46 Average train loss: 0.0841575388\n",
      "====> Epoch: 46 Average eval loss: 0.0088871010\n",
      "====> Epoch: 47 Average train loss: 0.0715657625\n",
      "====> Epoch: 47 Average eval loss: 0.0060007130\n",
      "====> Epoch: 48 Average train loss: 0.0695304512\n",
      "====> Epoch: 48 Average eval loss: 0.0094341356\n",
      "====> Epoch: 49 Average train loss: 0.0790711383\n",
      "====> Epoch: 49 Average eval loss: 0.0163733494\n",
      "====> Epoch: 50 Average train loss: 0.0736607652\n",
      "====> Epoch: 50 Average eval loss: 0.0116927875\n",
      "====> Epoch: 51 Average train loss: 0.0576549827\n",
      "====> Epoch: 51 Average eval loss: 0.0027054036\n",
      "====> Epoch: 52 Average train loss: 0.0624213609\n",
      "====> Epoch: 52 Average eval loss: 0.0029093255\n",
      "====> Epoch: 53 Average train loss: 0.0543539162\n",
      "====> Epoch: 53 Average eval loss: 0.0032394924\n",
      "====> Epoch: 54 Average train loss: 0.0606500517\n",
      "====> Epoch: 54 Average eval loss: 0.0187497139\n",
      "====> Epoch: 55 Average train loss: 0.0544191286\n",
      "====> Epoch: 55 Average eval loss: 0.0078398399\n",
      "====> Epoch: 56 Average train loss: 0.0599390504\n",
      "====> Epoch: 56 Average eval loss: 0.0024091629\n",
      "====> Epoch: 57 Average train loss: 0.0404293726\n",
      "====> Epoch: 57 Average eval loss: 0.0013065748\n",
      "====> Epoch: 58 Average train loss: 0.0463598579\n",
      "====> Epoch: 58 Average eval loss: 0.0072452221\n",
      "====> Epoch: 59 Average train loss: 0.0395874310\n",
      "====> Epoch: 59 Average eval loss: 0.0084178736\n",
      "====> Epoch: 60 Average train loss: 0.0482171961\n",
      "====> Epoch: 60 Average eval loss: 0.0006077228\n",
      "====> Epoch: 61 Average train loss: 0.0448725205\n",
      "====> Epoch: 61 Average eval loss: 0.0011044992\n",
      "====> Epoch: 62 Average train loss: 0.0452503245\n",
      "====> Epoch: 62 Average eval loss: 0.0026587083\n",
      "====> Epoch: 63 Average train loss: 0.0381496733\n",
      "====> Epoch: 63 Average eval loss: 0.0007598914\n",
      "====> Epoch: 64 Average train loss: 0.0440753923\n",
      "====> Epoch: 64 Average eval loss: 0.0045746597\n",
      "====> Epoch: 65 Average train loss: 0.0320144740\n",
      "====> Epoch: 65 Average eval loss: 0.0012628830\n",
      "====> Epoch: 66 Average train loss: 0.0323354288\n",
      "====> Epoch: 66 Average eval loss: 0.0059529571\n",
      "====> Epoch: 67 Average train loss: 0.0425619785\n",
      "====> Epoch: 67 Average eval loss: 0.0008829534\n",
      "====> Epoch: 68 Average train loss: 0.0286787368\n",
      "====> Epoch: 68 Average eval loss: 0.0105592310\n",
      "====> Epoch: 69 Average train loss: 0.0245088782\n",
      "====> Epoch: 69 Average eval loss: 0.0009599790\n",
      "====> Epoch: 70 Average train loss: 0.0242969846\n",
      "====> Epoch: 70 Average eval loss: 0.0009204498\n",
      "====> Epoch: 71 Average train loss: 0.0339950245\n",
      "====> Epoch: 71 Average eval loss: 0.0008706417\n",
      "====> Epoch: 72 Average train loss: 0.0258938259\n",
      "====> Epoch: 72 Average eval loss: 0.0014840884\n",
      "====> Epoch: 73 Average train loss: 0.0341399732\n",
      "====> Epoch: 73 Average eval loss: 0.0041644103\n",
      "====> Epoch: 74 Average train loss: 0.0249633400\n",
      "====> Epoch: 74 Average eval loss: 0.0030654653\n",
      "====> Epoch: 75 Average train loss: 0.0259306718\n",
      "====> Epoch: 75 Average eval loss: 0.0013291265\n",
      "====> Epoch: 76 Average train loss: 0.0306406710\n",
      "====> Epoch: 76 Average eval loss: 0.0006187560\n",
      "====> Epoch: 77 Average train loss: 0.0239513697\n",
      "====> Epoch: 77 Average eval loss: 0.0004956279\n",
      "====> Epoch: 78 Average train loss: 0.0281689798\n",
      "====> Epoch: 78 Average eval loss: 0.0020715178\n",
      "====> Epoch: 79 Average train loss: 0.0296825462\n",
      "====> Epoch: 79 Average eval loss: 0.0009026568\n",
      "====> Epoch: 80 Average train loss: 0.0205308314\n",
      "====> Epoch: 80 Average eval loss: 0.0019347100\n",
      "====> Epoch: 81 Average train loss: 0.0243213463\n",
      "====> Epoch: 81 Average eval loss: 0.0006989785\n",
      "====> Epoch: 82 Average train loss: 0.0266560733\n",
      "====> Epoch: 82 Average eval loss: 0.0005346852\n",
      "====> Epoch: 83 Average train loss: 0.0288058318\n",
      "====> Epoch: 83 Average eval loss: 0.0006341679\n",
      "====> Epoch: 84 Average train loss: 0.0143794690\n",
      "====> Epoch: 84 Average eval loss: 0.0005374790\n",
      "====> Epoch: 85 Average train loss: 0.0160672899\n",
      "====> Epoch: 85 Average eval loss: 0.0014829664\n",
      "====> Epoch: 86 Average train loss: 0.0121808142\n",
      "====> Epoch: 86 Average eval loss: 0.0007825547\n",
      "====> Epoch: 87 Average train loss: 0.0139936893\n",
      "====> Epoch: 87 Average eval loss: 0.0004401300\n",
      "====> Epoch: 88 Average train loss: 0.0233209296\n",
      "====> Epoch: 88 Average eval loss: 0.0009555684\n",
      "====> Epoch: 89 Average train loss: 0.0176320112\n",
      "====> Epoch: 89 Average eval loss: 0.0004731998\n",
      "====> Epoch: 90 Average train loss: 0.0150280906\n",
      "====> Epoch: 90 Average eval loss: 0.0005625707\n",
      "====> Epoch: 91 Average train loss: 0.0170653930\n",
      "====> Epoch: 91 Average eval loss: 0.0004101898\n",
      "====> Epoch: 92 Average train loss: 0.0180954504\n",
      "====> Epoch: 92 Average eval loss: 0.0007691816\n",
      "====> Epoch: 93 Average train loss: 0.0202003379\n",
      "====> Epoch: 93 Average eval loss: 0.0049832081\n",
      "====> Epoch: 94 Average train loss: 0.0173920540\n",
      "====> Epoch: 94 Average eval loss: 0.0004549320\n",
      "====> Epoch: 95 Average train loss: 0.0159401791\n",
      "====> Epoch: 95 Average eval loss: 0.0005968865\n",
      "====> Epoch: 96 Average train loss: 0.0140433737\n",
      "====> Epoch: 96 Average eval loss: 0.0005381951\n",
      "====> Epoch: 97 Average train loss: 0.0079763144\n",
      "====> Epoch: 97 Average eval loss: 0.0005965648\n",
      "====> Epoch: 98 Average train loss: 0.0090354047\n",
      "====> Epoch: 98 Average eval loss: 0.0006110210\n",
      "====> Epoch: 99 Average train loss: 0.0151226677\n",
      "====> Epoch: 99 Average eval loss: 0.0004141138\n",
      "Best testing error L0 SINDy is 0.0004101897939108312 and it was found at epoch 91\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T15:27:16.786817Z",
     "start_time": "2024-05-22T15:27:16.609784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# creating the plots\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Training and Evaluation Metrics')\n",
    "\n",
    "data_train = {'FCNN': metrics_fcnn[0], 'SparseFCNN': metrics_sparsefcnn[0], 'L0SINDy': metrics_l0sindy[0]}\n",
    "methods_train = list(data_train.keys())\n",
    "values_train = list(data_train.values())\n",
    "\n",
    "# creating the bar plot\n",
    "ax1.bar(methods_train, values_train, color='maroon', width=0.4)\n",
    "\n",
    "data_eval = {'FCNN': metrics_fcnn[2], 'SparseFCNN': metrics_sparsefcnn[2], 'L0SINDy': metrics_l0sindy[2]}\n",
    "methods_eval = list(data_eval.keys())\n",
    "values_eval = list(data_eval.values())\n",
    "\n",
    "ax2.bar(methods_eval, values_eval, color='blue', width=0.4)\n",
    "\n",
    "save_dir = \"figures\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "fig.savefig('figures/LearningDynamics.png', dpi=300)"
   ],
   "id": "5b52985fbff205cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
